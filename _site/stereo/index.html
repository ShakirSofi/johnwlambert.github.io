<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Stereo and Disparity</title>
  <meta name="description" content="disparity maps, cost volume, MC-CNN">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://johnwlambert.github.io/stereo/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>Stereo and Disparity</h1>
    <p class="meta">Dec 27, 2018</p>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>
<ul>
  <li><a href="#stereo-overview">Stereo Vision Overview</a></li>
  <li><a href="#correspondence">The Correspondence Problem</a></li>
  <li><a href="#stereo-geometry">Stereo Geometry</a></li>
  <li><a href="#disparity">Disparity</a></li>
  <li><a href="#classical-matching">Classical Matching Techniques</a></li>
  <li><a href="#first-algorithm">A First Stereo Algorithm</a></li>
  <li><a href="#disparity-map">Disparity Map</a></li>
  <li><a href="#cost-volume">Cost Volumes</a></li>
  <li><a href="#smoothing">Smoothing</a></li>
  <li><a href="#when-smoothing">When is smoothing a good idea?</a></li>
  <li><a href="#stereo-challenges">Challenging Scenarios for Stereo</a></li>
  <li><a href="#mc-cnn">MC-CNN</a></li>
</ul>

<p><a name="stereo-overview"></a></p>
<h2 id="stereo-vision-overview">Stereo Vision Overview</h2>

<p>“Stereo matching” is the task of estimating a 3D model of a scene from two or more images. The task requires finding matching pixels in the two images and converting the 2D positions of these matches into 3D depths [1]. Humans also use stereo vision, with a baseline (distance between our eyes) of 60 mm.</p>

<p>The basic idea is the following: a camera takes picture of the same scene, in two different positions, and we wish to recover the depth at each pixel. Depth is the missing ingredient, and the goal of computer vision here is to recover the missing depth information of the scene when and where the image was acquired. Depth is crucial for navigating in the world. It is the key to unlocking the images and using them for 3d reasoning. It allows us to understand shape of world, and what is in the image.</p>

<p>Keep in mind: the camera could be moving. We call 2d shifts “parallax”. If we fix the cameras’ position relative to each other, we can calibrate just once and operate under a known relationship. However, in other cases, we may need to calibrate them constantly, if cameras are constantly moving with respect to one another.</p>

<p><a name="correspondence"></a></p>
<h2 id="the-correspondence-problem">The Correspondence Problem</h2>

<p>Think of two cameras collecting images at the same time. There are differences in the images. Notably, there will be a per pixel shift in the scene as a consequence of the camera’s new, different position in 3d space. However, if a large part of the scene is seen in both images, two points will correspond to the same structure in the real world.</p>

<p>The correspondence problem is defined as finding a match across these two images to determine, <em>What is the part of the scene on the right that matches that location?</em> Previously, the community used only search techniques and optimization to solve this problem. Today, deep learning is used. The matching process is the key computation in stereo.</p>

<p><a name="stereo-geometry"></a></p>
<h2 id="stereo-geometry">Stereo Geometry</h2>

<p>Consider the geometry of a standard, “narrow-baseline” stereo rig. If we send out two rays from a 3D scene point to the two camera centers, a triangle is constructed in the process. Our goal is to “reverse” this 3d to 2d projection.</p>

<p>The x-coordinate is the only difference, one is translated by the baseline $B$. shift in 1-dimension, by one number. 2 points will lie on the same row in both images. we know where to look.</p>

<script type="math/tex; mode=display">Z=f\frac{B}{d}</script>

<p>there are some scale factors (which we get from calibration), simple reciprocal/inverse relationship</p>

<p>now, knowing geometric relationships, how to build stereo systems. Practical details: classical stereo, 80s/90s.</p>

<p>Consider a simple model of stereo vision: we have two cameras whose optic axes are parallel. Each camera points down the <script type="math/tex">Z</script>-axis. A figure is shown below:</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/stereo_coordinate_systems_rehg_dellaert.jpg" width="90%" />
  <div class="figcaption">
    Two cameras with optical centers O_L and O_R are separated by a baseline B. The z-axis extends towards the world, away from the camera.
  </div>
</div>

<p>Now, consider just the plane spanned by the <script type="math/tex">x</script>- and <script type="math/tex">z</script>-axes, with a constant <script type="math/tex">y</script> value:</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/stereo_vision_setup.jpg" width="50%" />

  <div class="figcaption">
    Two cameras L and R are separated by a baseline b. Here the Y-axis is perpendicular to the page. f is our (horizontal) focal length.
  </div>
</div>

<p>In this figure, the world point <script type="math/tex">P=(x,z)</script> is projected into the left image as <script type="math/tex">p_l</script> and into the right image as <script type="math/tex">p_r</script>.</p>

<p>By defining right triangles we can find two similar triangles: with vertices at <script type="math/tex">(0,0)-(0,z)-(x,z)</script> and <script type="math/tex">(0,0)-(0,f)-(f,x_l)</script>. Since they share the same angle <script type="math/tex">\theta</script>, then <script type="math/tex">\mbox{tan}(\theta)= \frac{\mbox{opposite}}{\mbox{adjacent}}</script> for both, meaning:</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x}{x_l}</script>

<p>We notice another pair of similar triangles
<script type="math/tex">(b,0)-(b,z)-(x,z)</script> and <script type="math/tex">(b,0)-(b,f)-(b+x_r,f)</script>, which by the same logic gives us</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x-b}{x_r}</script>

<p>We’ll derive a closed form expression for depth in terms of disparity. We already know that</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x}{x_l}</script>

<p>Multiply both sides by <script type="math/tex">f</script>, and we get an expression for our depth from the observer:</p>

<script type="math/tex; mode=display">z = f(\frac{x}{x_l})</script>

<p>We now want to find an expression for <script type="math/tex">\frac{x}{x_l}</script> in terms of <script type="math/tex">x_l-x_r</script>, the <em>disparity</em> between the two images.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{x}{x_l} &= \frac{x-b}{x_r} & \text{By similar triangles} \\
x x_r &= x_l (x-b) & \text{Multiply diagonals of the fraction} \\
x x_r &= x_lx - x_l b & \text{Distribute terms} \\
x_r &= \frac{x_lx}{x} - b(\frac{x_l}{x}) & \text{Divide all terms by } x \\
x_r &= x_l - b(\frac{x_l}{x}) & \text{Simplify} \\
b\frac{x_l}{x} &= x_l - x_r & \text{Rearrange terms to opposite sides} \\
b (\frac{x_l}{x}) (\frac{x}{x_l}) &= (x_l - x_r) (\frac{x}{x_l}) & \text{Multiply both sides by fraction inverse} \\
b &= (x_l - x_r) (\frac{x}{x_l}) & \text{Simplify} \\
\frac{b}{x_l - x_r} &= \frac{x}{x_l} & \text{Divide both sides by } (x_l - x_r) \\
\end{align} %]]></script>

<p>We can now plug this back in</p>

<script type="math/tex; mode=display">z = f(\frac{x}{x_l}) = f(\frac{b}{x_l - x_r})</script>

<p><a name="disparity"></a></p>
<h2 id="disparity">Disparity</h2>

<p>What is our takeaway? The amount of horizontal distance between the object in Image L and image R (<em>the disparity</em> <script type="math/tex">d</script>) is inversely proportional to the distance <script type="math/tex">z</script> from the observer. This makes perfect sense. Far away objects (large distance from the observer) will move very little between the left and right image. Very closeby objects (small distance from the observer) will move quite a bit more. The focal length <script type="math/tex">f</script> and the baseline <script type="math/tex">b</script> between the cameras are just constant scaling factors.</p>

<p>We made two large assumptions:</p>

<ol>
  <li>We know the focal length <script type="math/tex">f</script> and the baseline <script type="math/tex">b</script>. This requires prior knowledge or camera calibration.</li>
  <li>We need to find point correspondences, e.g. find the corresponding <script type="math/tex">(x_r,y_r)</script> for
each <script type="math/tex">(x_l,y_l)</script>.</li>
</ol>

<p><a name="classical-matching"></a></p>
<h2 id="classical-matching-techniques">Classical Matching Techniques</h2>

<p>To solve the correspondence problem, we must be able to decide if two chunks of pixels (e.g. patches) are the same. How can we do so?</p>

<p>One classical approach is to convert the search problem into just optimizing a function. We start somewhere, and then using Newton’s Method or Gradient Descent, we can find the best match. We will place a box at every single pixel location. There will be a lot of noise in each image, so we’ll want to put a lot of measurements together to overcome that noise.</p>

<p>If stereo images are rectified, then all scanlines are epipolar lines. You might ask, if these epipolar lines are parallel, how do they converge? The answer is that they converge at infinity.</p>

<p>We’ll literally apply this formula row by row.</p>

<p>To convert matching to a search or optimization problem, we’ll need a vector representation and then a matching cost. Consider that an image can be thought of as a vector, if we stack each row on top of each other. Thus, a vector represents the entire image; in vector space, each point in that space is an entire image. Alternatively, each window of pixels (patch) could be a point, in which we could compare them as vectors. Inner product, or normalized correlation are measures of similarity. If we have found a good match, then the angle between the vectors will be zero, and we can use the cosine of this angle to measure this.</p>

<p>Two distance measures are most common for testing patch similarity, and can serve as “matching costs”:</p>

<p><strong>Sum of Squared-Differences (SSD)</strong> The SSD measure is sum of squared difference of pixel values in two patches. This matching cost is measured at a proposed disparity. If <script type="math/tex">A,B</script> are patches to compare, separated by disparity <script type="math/tex">d</script>, then SSD is defined as:</p>

<script type="math/tex; mode=display">SSD(A, B) = \sum_{i,j}(A_{ij} - B_{ij})^2</script>

<p>In this case <script type="math/tex">A</script> could be a tensor of any shape/dimensions, and <script type="math/tex">B</script> must be a tensor of the same shape as <script type="math/tex">A</script></p>

<p><strong>Sum of absolute differences (SAD)</strong> Tests if two patches are similar by the <a href="https://en.wikipedia.org/wiki/Sum_of_absolute_differences">SAD</a> distance measure. <script type="math/tex">A,B</script> are defined identically as above:</p>

<script type="math/tex; mode=display">SAD(A, B) = \sum_{i,j}\lvert A_{ij}-B_{ij}\lvert</script>

<p>In general, absolute differences are more robust to large noise/outliers than squared differences, since outliers have less of an effect.</p>

<h2 id="search-line-scan-line">Search Line (Scan Line)</h2>

<p>A correspondence will lie upon an epipolar line. Unfortunately, just because we know the Fundamental matrix betwen two images, we cannot know the exact pixel location of a match without searching for it along a 1d line (range of possible locations the pixel might appear at in the other image)</p>

<p>In our discussion below, we will assume that we have rectified the images, such that we can search on just a horizontal scanline.</p>

<p><a name="first-algorithm"></a></p>
<h2 id="a-first-stereo-algorithm">A First Stereo Algorithm</h2>

<div class="fig figcenter fighighlight">
  <img src="/assets/simple_stereo_algorithm_fig.jpg" width="100%" />

  <div class="figcaption">
  	Stereo image pair with horizontal scanline.

  </div>
</div>

<p>Given a left image, right image, similarity function, patch size, maximum search value, this algorithm will output a “disparity map”.</p>

<ol>
  <li>Pick a patch in the left image (red block), P1.</li>
  <li>Place the patch in the same (x,y) coordinates in the right image (red block). As this is binocular stereo, we will need to search for P1 on the left side starting from this position. Make sure you understand this point well before proceeding further.</li>
  <li>Slide the block of candidates to the left (indicated by the different pink blocks). The search area is restricted by the parameter max_search_bound in the code. The candidates will overlap.</li>
  <li>We will pick the candidate patch with the minimum similarity error (green block). The horizontal shift from the red block to the green block in this image is the disparity value for the centre of P1 in the left image.</li>
</ol>

<p><a name="disparity-map"></a></p>
<h2 id="disparity-map">Disparity Map</h2>

<p>Calculate the disparity value at each pixel by searching a small patch around a pixel from the left image in the right image</p>

<p>Note:</p>
<ol>
  <li>We follow the convention of search
input in left image and search target in right image</li>
  <li>
    <p>While searching for disparity value for a patch, it may happen that there
are multiple disparity values with the minimum value of the similarity
measure. In that case we need to pick the smallest disparity value.
Please check the numpy’s argmin and pytorch’s argmin carefully.
Example:
– diparity_val – | – similarity error –
– 0               | 5 
– 1               | 4
– 2               | 7
– 3               | 4
– 4               | 12</p>

    <p>In this case we need the output to be 1 and not 3.</p>
  </li>
  <li>The max_search_bound is defined from the patch center.</li>
</ol>

<p>Args:</p>
<ul>
  <li>left_img: image from the left stereo camera. Torch tensor of shape (H,W,C).
          C will be &gt;= 1.</li>
  <li>right_img: image from the right stereo camera. Torch tensor of shape (H,W,C)</li>
  <li>block_size: the size of the block to be used for searching between
            left and right image</li>
  <li>sim_measure_function: a function to measure similarity measure between
                      two tensors of the same shape; returns the error value</li>
  <li>max_search_bound: the maximum horizontal distance (in terms of pixels) 
                  to use for searching
  Returns:</li>
  <li>disparity_map: The map of disparity values at each pixel.</li>
</ul>

<p><a name="cost-volume"></a></p>
<h2 id="cost-volumes">Cost Volumes</h2>

<p>Instead of taking the argmin of the similarity error profile, one will often store the tensor of error profile at each pixel location along the third dimension.</p>

<p>Calculate the cost volume. Each pixel will have D=max_disparity cost values
  associated with it. Basically for each pixel, we compute the cost of
  different disparities and put them all into a tensor.</p>

<p>Note:</p>
<ol>
  <li>It is important for this project to follow the convention of search
input in left image and search target in right image</li>
  <li>If the shifted patch in the right image will go out of bounds, it is
good to set the default cost for that pixel and disparity to be something
high(we recommend 255), so that when we consider costs, valid disparities will have a lower
cost.</li>
</ol>

<p>Args:</p>
<ul>
  <li>left_img: image from the left stereo camera. Torch tensor of shape (H,W,C).
          C will be 1 or 3.</li>
  <li>right_img: image from the right stereo camera. Torch tensor of shape (H,W,C)</li>
  <li>max_disparity:  represents the number of disparity values we will consider.
            0 to max_disparity-1</li>
  <li>sim_measure_function: a function to measure similarity measure between
            two tensors of the same shape; returns the error value</li>
  <li>block_size: the size of the block to be used for searching between
            left and right image
  Returns:</li>
  <li>cost_volume: The cost volume tensor of shape (H,W,D). H,W are image
          dimensions, and D is max_disparity. cost_volume[x,y,d] 
          represents the similarity or cost between a patch around left[x,y]<br />
          and a patch shifted by disparity d in the right image.</li>
</ul>

<p>Appendix B.5 and a recent survey paper on MRF inference (Szeliski, Zabih, Scharstein et al. 2008)</p>

<h2 id="error-profile">Error Profile</h2>

<p>ll disparity map, we will analyse the similarity error between patches. You will have to find out different patches in the image which exhibit a close-to-convex error profile, and a highly non-convex profile.</p>

<p><a name="smoothing"></a></p>
<h2 id="smoothing">Smoothing</h2>

<p>One issue with the results from is that they aren’t very smooth. Pixels next to each other on the same surface can have vastly different disparities, making the results look very noisy and patchy in some areas. Intuitively, pixels next to each other should have a smooth transition in disparity(unless at an object boundary or occlusion). In this section, we try to improve our results. One way of doing this is through the use of a smoothing constraint. The smoothing method we use is called Semi-Global Matching(SGM) or Semi-Global Block Matching. Before, we picked the disparity for a pixel based on the minimum matching cost of the block using some metric(SSD or SAD). The basic idea of SGM is to penalize pixels with a disparity that’s very different than their neighbors by adding a penalty term on top of the matching cost term.</p>

<p><a name="when-smoothing"></a></p>
<h2 id="when-is-smoothing-a-good-idea">When is smoothing a good idea?</h2>

<p>Of course smoothing should help in noisy areas of an image. It will only help to alleviate particular types of noise, however. It is not a definitive solution to obtaining better depth maps.</p>

<p>Smoothing performs best on background/uniform areas: disparity values here are already similar to each other, so smoothing doesn’t have to make any drastic changes. Pixels near each other are actually supposed to have similar disparity values, so the technique makes sense in these regions. Smoothing can also help in areas of occlusion, where SAD may not be able to find any suitable match. Smoothing will curb the problems by pushing the dispairty of a pixel to be similar to that of its neighbors.</p>

<p>Smoothing performs poorly in areas of fine detail, with narrow objects. It also performs poorly in areas with many edges and depth discontinuities. Smoothing algorithms penalize large disparity differences in closeby regions (which can actually occur in practice). Smoothing penalizes sharp disparity changes (corresponding to depth discontinuities).</p>

<p><a name="stereo-challenges"></a></p>
<h2 id="challenges">Challenges</h2>

<p>SSD or SAD is only the beginning. SSD/SAD suffers from… large “blobs” of disparities, can have areas of large error in textureless regions. Can fail in shadows</p>

<p>Too small a window might not be able to distinguish unique features of an image, but too large a window would mean many patches would likely have many more things in common, leading to less helpful matches.</p>

<p>Note that the problem is far from solved with these approaches, as many complexities remain. Images can be problematic, and can contain areas where it is quite hard or impossible to obtain matches (e.g. under occlusion).</p>

<p>Consider the challenge of texture-less image regions, such as a blank wall. Here there is an issue of aggregation: one cannot look at a single white patch of pixels and tell. One must integrate info at other levels, for example by looking at the edge of wall.</p>

<p>Violations of the brightness constancy assumption (e.g. specular reflections) present another problem. These occur when light is reflected off of a mirror or glass picture frame, for example. Camera calibration errors would also cause problems.</p>

<p>What tool should we reach for to solve all of the problems in stereo?</p>

<p><a name="mc-cnn"></a></p>
<h2 id="mc-cnn">MC-CNN</h2>

<p>In stereo, as in almost all other computer vision tasks, convnets are the answer. However, getting deep learning into stereo took a while; for example, it took longer than recognizing cats. Our desire is for a single architecture, which will allow less propagation of error. Thus, every decision you make about how to optimize, will be end-to-end optimzation, leaving us with the best chance to drive the error down with data.</p>

<p><em>Computing the Stereo Matching Cost with a Convolutional Neural Network</em> (MC-CNN) [9] by Zbontar and LeCun in 2015 introduced the idea of training a network to learn how to classify 2 patches as a positive vs. a negative match.</p>

<p>You can see how MC-CNN compares to classical approaches on the standard benchmark for stereo, which is the Middlebury Stereo Dataset and <a href="http://vision.middlebury.edu/stereo/eval3/">Leaderboard</a>. At times, MC-CNN can show fine structures and succeed in shadows, when SAD and SSD cannot.</p>

<h2 id="references">References</h2>
<p>[1] Richard Szeliski. Computer Vision: Algorithms and Applications.</p>

<p>[2] James Hays. <a href="https://www.cc.gatech.edu/~hays/compvision/lectures/09.pdf">PDF</a>.</p>

<p>[3] Rajesh Rao. Lecture 16: Stereo and 3D Vision, University of Washington. <a href="https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf">PDF</a>.</p>

<p>[5] Yuri Boykov, Olga Veksler, Ramin Zabih. Fast Approximate Energy Minimization via Graph Cuts. ICCV 1999. <a href="http://www.cs.cornell.edu/rdz/Papers/BVZ-iccv99.pdf">PDF</a></p>

<p>[6] Zhou. Unsupervised Learning of Depth. <a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html">PDF</a>.</p>

<p>[7] Heiko Hirschmuller. Stereo Processing by Semi-Global Matching
and Mutual Information. <a href="https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf">PDF</a>.</p>

<p>[8] Heiko Hirschmuller. Semi-Global Matching – Motivation, Development, and Applications. <a href="https://elib.dlr.de/73119/1/180Hirschmueller.pdf">PDF</a></p>

<p>[9] Computing the Stereo Matching Cost with a Convolutional Neural Network. Jure Zbontar and Yann LeCun. <a href="https://arxiv.org/pdf/1409.4326.pdf">PDF</a>.</p>

<p>Additional Resources:
 http://people.scs.carleton.ca/~c_shu/Courses/comp4900d/notes/simple_stereo.pdf
 http://www.cse.usf.edu/~r1k/MachineVisionBook/MachineVision.files/MachineVision_Chapter11.pdf
 https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf
 http://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture6_affine_SFM_notes.pdf
 https://www.cc.gatech.edu/~hays/compvision/lectures/09.pdf
 MRF inference (Szeliski, Zabih, Scharstein et al. 2008)</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
