<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Stereo and Disparity</title>
  <meta name="description" content="focal length, similar triangles, depth">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://johnwlambert.github.io/stereo/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>Stereo and Disparity</h1>
    <p class="meta">Dec 27, 2018</p>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>
<ul>
  <li><a href="#sfmpipeline">Stereo Matching</a></li>
  <li><a href="#costfunctions">Disparity</a></li>
  <li><a href="#bundleadjustment">Bundle Adjustment</a></li>
</ul>

<p><a name="sfmpipeline"></a></p>

<h2 id="stereo-vision-and-stereo-matching">Stereo Vision and Stereo Matching</h2>

<p>“Stereo matching” is the task of estimating a 3D model of a scene from two or more images. The task requires finding matching pixels in the two images and converting the 2D positions of these matches into 3D depths [1]</p>

<p>Humans have stereo vision with a baseline of 60 mm.</p>

<h2 id="disparity">Disparity</h2>

<p>Consider a simple model of stereo vision: we have two cameras whose optic axes are parallel. Each camera points down the <script type="math/tex">Z</script>-axis. A figure is shown below:</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/stereo_coordinate_systems_rehg_dellaert.jpg" width="90%" />
  <div class="figcaption">
    Two cameras with optical centers O_L and O_R are separated by a baseline B. The z-axis extends towards the world, away from the camera.
  </div>
</div>

<p>Now, consider just the plane spanned by the <script type="math/tex">x</script>- and <script type="math/tex">z</script>-axes, with a constant <script type="math/tex">y</script> value:</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/stereo_vision_setup.jpg" width="50%" />

  <div class="figcaption">
    Two cameras L and R are separated by a baseline b. Here the Y-axis is perpendicular to the page. f is our (horizontal) focal length.
  </div>
</div>

<p>In this figure, the world point <script type="math/tex">P=(x,z)</script> is projected into the left image as <script type="math/tex">p_l</script> and into the right image as <script type="math/tex">p_r</script>.</p>

<p>By defining right triangles we can find two similar triangles: with vertices at <script type="math/tex">(0,0)-(0,z)-(x,z)</script> and <script type="math/tex">(0,0)-(0,f)-(f,x_l)</script>. Since they share the same angle <script type="math/tex">\theta</script>, then <script type="math/tex">\mbox{tan}(\theta)= \frac{\mbox{opposite}}{\mbox{adjacent}}</script> for both, meaning:</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x}{x_l}</script>

<p>We notice another pair of similar triangles
<script type="math/tex">(b,0)-(b,z)-(x,z)</script> and <script type="math/tex">(b,0)-(b,f)-(b+x_r,f)</script>, which by the same logic gives us</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x-b}{x_r}</script>

<p>We’ll derive a closed form expression for depth in terms of disparity. We already know that</p>

<script type="math/tex; mode=display">\frac{z}{f} = \frac{x}{x_l}</script>

<p>Multiply both sides by <script type="math/tex">f</script>, and we get an expression for our depth from the observer:</p>

<script type="math/tex; mode=display">z = f(\frac{x}{x_l})</script>

<p>We now want to find an expression for <script type="math/tex">\frac{x}{x_l}</script> in terms of <script type="math/tex">x_l-x_r</script>, the <em>disparity</em> between the two images.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{x}{x_l} &= \frac{x-b}{x_r} & \text{By similar triangles} \\
x x_r &= x_l (x-b) & \text{Multiply diagonals of the fraction} \\
x x_r &= x_lx - x_l b & \text{Distribute terms} \\
x_r &= \frac{x_lx}{x} - b(\frac{x_l}{x}) & \text{Divide all terms by } x \\
x_r &= x_l - b(\frac{x_l}{x}) & \text{Simplify} \\
b\frac{x_l}{x} &= x_l - x_r & \text{Rearrange terms to opposite sides} \\
b (\frac{x_l}{x}) (\frac{x}{x_l}) &= (x_l - x_r) (\frac{x}{x_l}) & \text{Multiply both sides by fraction inverse} \\
b &= (x_l - x_r) (\frac{x}{x_l}) & \text{Simplify} \\
\frac{b}{x_l - x_r} &= \frac{x}{x_l} & \text{Divide both sides by } (x_l - x_r) \\
\end{align} %]]></script>

<p>We can now plug this back in</p>

<script type="math/tex; mode=display">z = f(\frac{x}{x_l}) = f(\frac{b}{x_l - x_r})</script>

<p>What is our takeaway? The amount of horizontal distance between the object in Image L and image R (<em>the disparity</em> <script type="math/tex">d</script>) is inversely proportional to the distance <script type="math/tex">z</script> from the observer. This makes perfect sense. Far away objects (large distance from the observer) will move very little between the left and right image. Very closeby objects (small distance from the observer) will move quite a bit more. The focal length <script type="math/tex">f</script> and the baseline <script type="math/tex">b</script> between the cameras are just constant scaling factors.</p>

<p>We made two large assumptions:</p>

<ol>
  <li>We know the focal length <script type="math/tex">f</script> and the baseline <script type="math/tex">b</script>. This requires prior knowledge or camera calibration.</li>
  <li>We need to find point correspondences, e.g. find the corresponding <script type="math/tex">(x_r,y_r)</script> for
each <script type="math/tex">(x_l,y_l)</script>.</li>
</ol>

<h2 id="the-epipolar-line-plane-and-constraint">The Epipolar Line, Plane, and Constraint</h2>

<p>Unfortunately, just because we know</p>

<p>how to compute for a given pixel in one image the range of possible locations the pixel might appear at in the other image, i.e., its epipolar lin</p>

<h2 id="sum-of-squared-differences">Sum of Squared-Differences</h2>

<p>The matching cost is the squared difference of intensity values at a given disparity.</p>

<h2 id="ssd-or-sad-is-only-the-beginning">SSD or SAD is only the beginning</h2>

<p>SSD/SAD suffers from… large “blobs” of disparities, can have areas of large error in textureless regions. fail in shadows</p>

<p>MC-CNN can… show fine structures. succeed in shadows.</p>

<p>Too small a window might not be able to distinguish unique features of an image, but too large a window would mean many patches would likely have many more things in common, leading to less helpful matches.</p>

<h2 id="cost-volumes">Cost Volumes</h2>

<p>Appendix B.5 and a recent survey paper on MRF inference (Szeliski, Zabih, Scharstein et al. 2008)</p>

<h2 id="simple-example-on-kitti">Simple Example on KITTI</h2>

<script type="math/tex; mode=display">E = R \mbox{ } [t]_{\times}</script>

<h2 id="when-is-smoothing-a-good-idea">When is smoothing a good idea?</h2>

<p>Of course smoothing should help in noisy areas of an image. It will only help to alleviate particular types of noise, however. It is not a definitive solution to obtaining better depth maps.</p>

<p>Smoothing performs best on background/uniform areas: disparity values here are already similar to each other, so smoothing doesn’t have to make any drastic changes. Pixels near each other are actually supposed to have similar disparity values, so the technique makes sense in these regions. Smoothing can also help in areas of occlusion, where SAD may not be able to find any suitable match. Smoothing will curb the problems by pushing the dispairty of a pixel to be similar to that of its neighbors.</p>

<p>Smoothing performs poorly in areas of fine detail, with narrow objects. It also performs poorly in areas with many edges and depth discontinuities. Smoothing algorithms penalize large disparity differences in closeby regions (which can actually occur in practice). Smoothing penalizes sharp disparity changes (corresponding to depth discontinuities).</p>

<h2 id="object-detection-in-stereo">Object Detection in Stereo</h2>

<p>Chen <em>et al.</em>
3DOP (NIPS 2015) Urtasun [4]</p>

<h2 id="unsupervised-learning-of-depth">Unsupervised Learning of Depth</h2>

<p>From 2017 [5]</p>

<p>Let <script type="math/tex">p_t</script> denote homogeneous coordinates in the target view. <script type="math/tex">K</script> denotes the camera intrinsic matrix.</p>

<script type="math/tex; mode=display">p_s \sim K \hat{T}_{t \rightarrow s} \hat{D}_t (p_t) K^{-1} p_t</script>

<h2 id="references">References</h2>

<p>Computing the Stereo Matching Cost with a Convolutional Neural Network. Jure Zbontar  and Yann LeCun. <a href="https://arxiv.org/pdf/1409.4326.pdf">PDF</a>.</p>

<p>[1] Richard Szeliski.</p>

<p>[2] James Hays. <a href="https://www.cc.gatech.edu/~hays/compvision/lectures/09.pdf">PDF</a>.</p>

<p>[3] Rajesh Rao. Lecture 16: Stereo and 3D Vision, University of Washington. <a href="https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf">PDF</a>.</p>

<p>[4] X. Chen, K. Kundu, Y. Zhu, A. Berneshawi, H. Ma, S. Fidler, R. Urtasun. <em>3D Object Proposals for Accurate Object Class Detection.</em>  Advances in Neural Information Processing Systems 28 (NIPS 2015). <a href="https://papers.nips.cc/paper/5644-3d-object-proposals-for-accurate-object-class-detection">PDF</a>.</p>

<p>[5]</p>

<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html">PDF</a>.</p>

<p>STEREO http://people.scs.carleton.ca/~c_shu/Courses/comp4900d/notes/simple_stereo.pdf
DEPTH http://www.cse.usf.edu/~r1k/MachineVisionBook/MachineVision.files/MachineVision_Chapter11.pdf
STEREO https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf
SFM http://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture6_affine_SFM_notes.pdf
JAMES MULTI-VIEW https://www.cc.gatech.edu/~hays/compvision/lectures/09.pdf</p>

<p>2d shifts=parallax</p>

<p>Basic idea: camera takes picture of the same scene, in two different positions, you can recover the depth</p>

<p>depth is the mixing ingredient</p>

<p>vision: recover the missing depth information when the iamge was acquired from the scene</p>

<p>the camera could be moving</p>

<p>think of two cameras collecting images at the same time. there are diffeerences in the images. per pixel shift in the scene as consequence of cameras different position in 3d space</p>

<p>understanding that two points correspond to the same structure in the real world</p>

<p>fix their position relative to each other, just calibrate once, known relationship</p>

<p>or may need to calibrate them constantly, if constantly moving</p>

<p>What is the part of the scene on the right that matches that location? Correspondence problem (finding match across)</p>

<p>Used to use search techniques and optimization. now use deep learning.</p>

<p>send out two rays, just calibrate the camera bit (3d to 2d projection), then reverse it</p>

<p>triangle constructed in this process</p>

<p>Key computation: the matching.</p>

<p>Depth is crucial for navigating in the world, the key to unlocking the images and using them for 3d reasoning</p>

<p>understand shape of world, and what is in the image.</p>

<p>The x-coordinate is the only difference, one is translated by the baseline $B$. shift in 1-dimension, by one number. 2 points will lie on the same row in both images. we know where to look.</p>

<script type="math/tex; mode=display">Z=f\frac{B}{d}</script>

<p>there are some scale factors (which we get from calibration), simple reciprocal/inverse relationship</p>

<p>now, knowing geometric relationships, how to build stereo systems. Practical details: classical stereo, 80s/90s.</p>

<p>How do we decide two chunks of pixels are the same?</p>

<p>Turn the search problem into just optimizing a function.</p>

<p>Start somewhere, newton’s method or gradient descent, find best match</p>

<p>You put a box at every single pixel location.</p>

<p>Lot of noise in these image, want to put a lot of measurments together to overcome that noise.</p>

<p>All scanlines are epipolar lines.</p>

<p>These lines are paralle, how do they converge? they converge at infinity.</p>

<p>literally apply this formula row by row</p>

<p>image normalization : image can be though of as a vector, each row, stack them. vector represents the entire image. vector space, each point in that space is an entire image. or each window of pixels could be a point.</p>

<p>windows – compare them as vectors</p>

<p>inner product of two vectors, normalized correlation?</p>

<p>good match – vectors are the same. angle between the vectors, cosine of angle between them.</p>

<p>Lot of complexities: images can be problematic, can contian ares quite hard to get matches (occluded).</p>

<p>Or what about blank wall, textureless regions. Issue of aggregation. Can’t look at a single white patch of pixels and tell. integrate info at other levels. look at edge of wall.</p>

<p>Dynamic programming, used in this case. Use a single match as constraint to find other matches. get ordering constraint.</p>

<p>sometimes blue patch is not going to happen! happens all the time. have to allow that sometime there will not be a match, and just move on to the next guy.</p>

<p>real-time stereo in late 80s from stereo matching with dynamic programming.</p>

<p>Turns out there are quite a few problems.</p>

<p>illusion? thin nail illusion, thin object close to the camera?</p>

<p>slanted plane: scrunched, will have multiple matches in one location.</p>

<p>one scanline at a time, wasnt the winning approach.</p>

<p>matching the scannline independently isn’t good – may see weird artifacts when you look at a vertical column, 3d boundaries in scene may not make straight lines. no consistently through your scanlines.</p>

<p>Scanline is bad as representation of the world. segemntation based stereo – first segment the iamge into patches that are similar in their appearance, then work patch by patch and do the matching.</p>

<p>you can also get view interpolation, synthesize new views in between the two views.</p>

<p>Another challenge in stereo: reflections in mirror, bright objects like picture frame. What tool should we reach for to solve all of the problems in stereo?</p>

<p>It is always the answer… Getting deep learning into stereo took a while. took longer than recognizing cats.</p>

<p>One architetecture – less propagation of error. fits into one model. every decision you make about how to optimize, end-to-end optimzation. best chance to drive the error down with data.</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
