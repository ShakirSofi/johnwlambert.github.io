<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Gauss-Newton Optimization in 10 Minutes</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Notes on Machine Learning and Optimization.">
    <link rel="canonical" href="http://johnwlambert.github.io/2018/03/31/gaussnewton/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="John Lambert blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
    <a href="/feed.xml">
      <img src="/assets/mom_melissa_huntington.png" width="400">
    </a>
    </div>


    <a class="site-title" href="/">John Lambert blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Gauss-Newton Optimization in 10 Minutes</h1>
    <p class="meta">Mar 31, 2018</p>
  </header>

  <article class="post-content">
  <h2 id="unconstrained-optimization">Unconstrained Optimization</h2>

<h3 id="the-gauss-newton-method">The Gauss-Newton Method</h3>

<p>Suppose our residual is no longer affine, but rather nonlinear. We want to minimize \(\lVert r(x) \rVert^2\). Generally speaking, we cannot solve this problem, but rather can use good heuristics to find local minima.</p>

<ul>
  <li>
    <p>Start from initial guess for your solution</p>
  </li>
  <li>
    <p>Repeat:</p>
  </li>
  <li>(1) Linearize \(r(x)\) around current guess \(x^{(k)}\). This can be accomplished by using a Taylor Series and Calculus (Standard Gauss-Newton), or one can use a least-squares fit to the line.</li>
  <li>(2) Solve least squares for linearized objective, get \(x^{(k+1)}\).</li>
</ul>

<p>The linearized residual \(r(x)\) will resemble:</p>

<script type="math/tex; mode=display">r(x) \approx r(x^{(k)}) + Dr(x^{(k)}) (x-x^{(k)})</script>

<p>where \(Dr\) is the Jacobian, meaning \( (Dr)_{ij} = \frac{\partial r_i}{\partial x_j}\)</p>

<p>Distributing the rightmost product, we obtain</p>

<script type="math/tex; mode=display">r(x) \approx Dr(x^{(k)})x - \bigg(Dr(x^{(k)}) (x^{(k)}) - r(x^{(k)}) \bigg)</script>

<p>With a single variable \(x\), we can re-write the above equation as</p>

<script type="math/tex; mode=display">r(x) \approx A^{(k)}x - b^{(k)}</script>

<h3 id="levenberg-marquardt-algorithm-trust-region-gauss-newton-method">Levenberg-Marquardt Algorithm (Trust-Region Gauss-Newton Method)</h3>

<p>In Levenberg-Marquardt, we have add a term to the objective function to emphasize that we should not move so far from \( \theta^{(k)} \) that we cannot trust the affine approximation. We often refer to this concept as remaining within a “trust region” (<a href="https://arxiv.org/abs/1502.05477">TRPO</a> is named after the same concept). Thus, we wish 
<script type="math/tex">|| \theta - \theta^{(k)} ||^2</script>
 to be small. Our new objective is:</p>

<script type="math/tex; mode=display">||A^{(k)} \theta - b^{(k)}||^2 + \lambda^{(k)} || \theta − \theta^{(k)}||^2</script>

<p>This objective can be written inside a single \(\ell_2\)-norm, instead using two separate \(\ell_2\)-norms:</p>

<script type="math/tex; mode=display">|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \end{bmatrix} ||^2</script>

<h2 id="example">Example</h2>

<p>Suppose we have some input data \(x\) and labels \(y\). Our prediction function could be</p>

<script type="math/tex; mode=display">\hat{f} = x^T\theta_1 + \theta_2</script>

<p>Suppose at inference time we use \(f(x) = \mbox{sign }\bigg(\hat{f}(x)\bigg)\), where \(\mbox{sign }(a) = +1\) for \(a \geq 0\) and −1 for \(a &lt; 0\). At training time, we use its smooth (and differentiable) approximation, the hyperbolic tangent, tanh:</p>

<script type="math/tex; mode=display">\phi(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}</script>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phi</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">(</span><span class="nb">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="p">/(</span><span class="nb">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">));</span>
</code></pre></div></div>

<p>The gradient of \(\mbox{tanh}\): \(\nabla_x \mbox{tanh } = 1-\mbox{tanh }(x)^2\). We call this \(\phi^{\prime}\) in code:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phiprime</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">);</span>
</code></pre></div></div>

<p>Suppose our objective function is the MSE loss, with a regularization term:</p>

<script type="math/tex; mode=display">J = \sum\limits_{i=1}^{N} \Bigg(y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)\Bigg)^2 + \mu ||\theta_1||^2</script>

<p>The residual for a single training example \(i\) is \(r_i\) is</p>

<script type="math/tex; mode=display">y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)</script>

<p>For a vector of training examples \(\mathbf{X}\) and labels \(\mathbf{Y}\), our nonlinear residual function is:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">)</span> <span class="n">y</span><span class="o">-</span><span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="o">'*</span><span class="n">t1</span><span class="o">+</span><span class="n">t2</span><span class="p">);</span>
</code></pre></div></div>

<p>To linearize the residual, we compute its Jacobian \(Dr(\theta_1,\theta_2)\) via matrix calculus:</p>

<script type="math/tex; mode=display">\frac{\partial r_i}{\partial \theta_1} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)x_i^T</script>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jacobian_0_entr</span> <span class="o">=</span> <span class="o">-</span><span class="n">phiprime</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="s1">'*theta(1:400)+theta(end))* X(:,i)'</span>
</code></pre></div></div>

<p><script type="math/tex">\frac{\partial r_i}{\partial \theta_2} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)</script></p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jacobian_1_entr</span> <span class="o">=</span> <span class="o">-</span><span class="n">phiprime</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="o">'*</span><span class="n">theta</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">400</span><span class="p">)</span><span class="o">+</span><span class="n">theta</span><span class="p">(</span><span class="k">end</span><span class="p">))</span>
</code></pre></div></div>
<p>The the full Jacobian evaluated at a certain point \(X_i\) is just these stacked individual entries:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Dr</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">labels_train</span><span class="p">),</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="k">for</span> <span class="nb">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">length</span><span class="p">(</span><span class="n">labels_train</span><span class="p">)</span>
	<span class="n">Dr</span><span class="p">(</span><span class="nb">i</span><span class="p">,:)</span> <span class="o">=</span> <span class="p">[</span><span class="n">jacobian_0_entr</span><span class="p">,</span> <span class="n">jacobian_1_entr</span><span class="p">];</span>
<span class="k">end</span>
</code></pre></div></div>
<p>Let \(\theta= \begin{bmatrix} \theta_1^T &amp; \theta_2 \end{bmatrix}^T \in \mathbb{R}^{401}\). The linearized residual follows the exact form outlined in the Gauss-Newton section above:</p>

<script type="math/tex; mode=display">r(\theta) \approx A^{(k)}\theta - b^{(k)}</script>

<p>where</p>

<script type="math/tex; mode=display">b^{(k)} = A^{(k)} \theta^{(k)} - r\bigg(\theta^{(k)}\bigg)</script>

<p>In code, this term is computed as:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A_k_temp</span> <span class="o">=</span> <span class="n">Dr</span><span class="p">;</span> <span class="c1">% computed above</span>
<span class="n">b_k_temp</span> <span class="o">=</span> <span class="n">Dr</span><span class="o">*</span><span class="n">theta</span> <span class="o">-</span> <span class="n">r</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span><span class="n">theta</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">400</span><span class="p">),</span><span class="n">theta</span><span class="p">(</span><span class="k">end</span><span class="p">));</span>
</code></pre></div></div>

<p>We solve a least-squares problem in every iteration, with a 3-part objective function (penalizing the residual, large step sizes, and also large \(\theta_1\)-norm weights):</p>

<script type="math/tex; mode=display">% <![CDATA[
|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I_{401} \\ \begin{bmatrix} \sqrt{\mu} I_{400 } & 0\end{bmatrix} \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \\ 0 \end{bmatrix} ||^2. %]]></script>

<p>We represent the left term by</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A_k</span> <span class="o">=</span> <span class="p">[</span><span class="n">A_k_temp</span><span class="p">;</span> <span class="nb">sqrt</span><span class="p">(</span><span class="n">lambda</span><span class="p">(</span><span class="n">itr</span><span class="p">))</span><span class="o">*</span><span class="nb">eye</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span> <span class="nb">sqrt</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="nb">eye</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)]];</span>
</code></pre></div></div>
<p>and the right term by</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b_k = [b_k_temp; sqrt(lambda(itr))*theta; zeros(length(theta)-1,1)];
</code></pre></div></div>

<p>We solve for the next iterate of \(\theta\) with the pseudo-inverse:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta_temp</span> <span class="o">=</span> <span class="n">A_k</span><span class="p">\</span><span class="n">b_k</span><span class="p">;</span>
</code></pre></div></div>

<p>The full algorithm might resemble:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="c1">%regularization coefficient</span>
<span class="n">lambda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">%initial lambda for Levenberg-Marquardt</span>

<span class="n">theta</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="mi">401</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="c1">%initial value for theta (last entry is theta_2)</span>

<span class="k">for</span> <span class="n">itr</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="mi">15</span>
	<span class="c1">%calculate Jacobian at the current iteration (see code above)</span>

	<span class="c1">%linearize the objective function (see code above)</span>

	<span class="c1">% stopping condition ...</span>
<span class="k">end</span>

</code></pre></div></div>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
<!--  
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'karpathyblog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
   -->
  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">John Lambert blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>John Lambert blog</li>
        <!-- <li><a href="mailto:"></a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/johnwlambert">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">johnwlambert</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/none">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">none</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Notes on Machine Learning and Optimization.</p>
    </div>

  </div>

</footer>


    </body>
</html>