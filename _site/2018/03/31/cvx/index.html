<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Convex Optimization</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Notes on Machine Learning and Optimization.">
    <link rel="canonical" href="http://localhost:4000/2018/03/31/cvx/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="John Lambert blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
    <a href="/feed.xml">
      <img src="/assets/mom_melissa_huntington.png" width="400">
    </a>
    </div>

    <a class="site-title" href="/">John Lambert blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          <a class="page-link" href="/neuralnets/">Hacker's guide to Neural Networks</a>
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Convex Optimization</h1>
    <p class="meta">Mar 31, 2018</p>
  </header>

  <article class="post-content">
  <!-- 
<svg width="800" height="200">
	<rect width="800" height="200" style="fill:rgb(98,51,20)" />
	<rect width="20" height="50" x="20" y="100" style="fill:rgb(189,106,53)" />
	<rect width="20" height="50" x="760" y="30" style="fill:rgb(77,175,75)" />
	<rect width="10" height="10" x="400" y="60" style="fill:rgb(225,229,224)" />
</svg>
 -->

<h2 id="convexity">Convexity</h2>

<h3 id="first-order-condition">First-Order Condition</h3>

<p>If \(f\) is convex and differentiable, then</p>

<script type="math/tex; mode=display">f(x) + \nabla f(x)^T (y-x) \leq f(y)</script>

<p>That is to say, a tangent line to \(f\) is a <strong>global underestimator</strong> of the function.</p>

<h3 id="second-order-condition">Second-Order Condition</h3>

<p>Assuming \(f\) is twice differentiable, that is, its Hessian or second derivative \(\nabla^2f\) exists at each point in the domain of \(f\), then \(f\) is convex **if and only if ** the domain of \(f\) is convex and its Hessian is positive semidefinite.</p>

<p>Formally, we state, for all \(x \in \mbox{dom}(f)\),</p>

<p><script type="math/tex">\nabla^2 f(x) \succeq 0</script>
This condition can be interpreted geometrically as the requirement that the graph of the function have positive (upward) curvature at \(x\).</p>

<h3 id="known-convex-and-concave-functions">Known Convex and Concave Functions</h3>

<p>Convex</p>
<ul>
  <li>Linear.</li>
  <li>Affine. \(f(x) = Ax+b\), where \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\). This is the sum of a linear function and a constant.</li>
  <li>Exponential. \(e^{ax}\) is convex on \(\mathbb{R}\), for any \(a \in \mathbb{R}\).</li>
  <li>Powers. \(x^a\) is convex on \(R_{++}\) when \(a \geq 1\) or \(a \leq 0\).</li>
  <li>Powers of absolute value. \(|x|^p\), for \(p\geq 1\), is convex on \(\mathbb{R}\).</li>
  <li>Negative Entropy. \(x \mbox{log}(x)\) is convex on \(\mathbb{R}_{++}\).</li>
  <li>Norms. Every norm on \(\mathbb{R}^n\) is convex.</li>
  <li>Max function. \(f(x) = \mbox{max} { x_1, \dots, x_n } \) is convex on \(\mathbb{R}^n\).</li>
  <li>Quadratic-over-linear function.</li>
  <li>Log-sum-exp. \(f(x) = \mbox{log }(e^{x_1}+\cdots+e^{x_n})\) is convex on \(\mathbb{R}^n\).</li>
</ul>

<p>Concave</p>
<ul>
  <li>Logarithm. \(\mbox{log}(x)\) concave on \(\mathbb{R}_{++}\).</li>
  <li>Powers. \(x^a\) is concave for \(0 \leq a \leq 1\).</li>
  <li>
    <p>Geometric mean. \(f(x) = (\prod\limits_{i=1}^n x_i)^{1/n}\) is concave on \(\mathbb{R}_{++}^n\).</p>
  </li>
  <li>Log-determinant. \(f(X) = \mbox{log } \mbox{det } X\) is concave on \(S_{++}^n\)</li>
</ul>

<h2 id="constrained-optimization-problems">Constrained Optimization Problems</h2>

<p>These problems take on a very general form, that we’ll revisit over and over again. In math, that form is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} & f_0(x) & \\
\mbox{subject to} & f_i(x) \leq 0, & i=1,\dots,m \\
& h_i(x) = 0, & i=1,\dots,p
\end{array}
\end{aligned} %]]></script>

<p>In prose, the problem is to find an \(x\) that minimizes \(f_0(x)\) among all \(x\) that satisfy the conditions \( f_i(x) \leq 0\) for \(i=1,\dots,m \) and \( h_i(x) = 0\) for \( i=1,\dots,p\).</p>

<p>The inequalities \(f_i(x) \leq 0\) are called inequality constraints, and the equations \(h_i(x) = 0\) are called the equality constraints.</p>

<h3 id="the-epigraph-form-of-the-above-standard-problem-is-the-problem">The Epigraph form of the above standard problem is the problem</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} & t & \\
\mbox{subject to} & f_0(x) - t \leq 0 & \\
& f_i(x) \leq 0, & i=1,\dots,m \\
& h_i(x) = 0, & i=1,\dots,p
\end{array}
\end{aligned} %]]></script>

<p>Geometrically, from [1]:</p>

<p><img src="/assets/epigraph_problem.png" width="50%" /></p>

<h2 id="the-lagrangian">The Lagrangian</h2>

<p>The basic idea in Lagrangian duality is to take the constraints in the standard problem into account by <strong>augmenting the objective function with a weighted sum of the constraint functions</strong>. The Lagrangian associated with the standard problem is:</p>

<script type="math/tex; mode=display">L(x, \lambda, \nu) = f_0(x) + \sum\limits_{i=1}^{m} \lambda_i f_i(x) + \sum\limits_{i=1}^p \nu_i h_i(x)</script>

<p>We call \(\lambda_i\) as the Lagrange multiplier associated with the \(i\)’th <strong>inequality</strong> constraint \(f_i(x) \leq 0\).  We refer to \(\nu_i\) as the Lagrange multiplier associated with the \(i\)’th <strong>equality</strong> constraint \(h_i(x) = 0\).</p>

<h3 id="the-lagrange-dual-function">The Lagrange Dual Function</h3>

<script type="math/tex; mode=display">g(\lambda, \nu) = \underset{x \in \mathcal{D}}{\mbox{inf }} L(x,\lambda,\nu)</script>

<p>In detail, the dual function is:</p>

<script type="math/tex; mode=display">g(\lambda, \nu) = \underset{x \in \mathcal{D}}{\mbox{inf }}\Bigg( f_0(x) + \sum\limits_{i=1}^{m} \lambda_i f_i(x) + \sum\limits_{i=1}^p \nu_i h_i(x) \Bigg)</script>

<p>This is the pointwise infimum of a family of affine functions of \( (\lambda, \nu)\), so the dual function is <strong>concave</strong>, even when the standard optimization problem is not convex.</p>

<h3 id="the-lagrange-dual-problem">The Lagrange Dual Problem</h3>

<p>For each pair \( (\lambda, \nu)\), the Lagrange dual function gives us a lower bound on the optimal value \(p^{*}\) of the standard optimization problem. It is a <strong>lower bound</strong> that depends on some parameters \( (\lambda,\nu)\). But the question of interest for us is, what is the <strong>best</strong> lower bound that can be obtained from the Lagrange dual function. This leads to the following optimization problem:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\begin{array}{ll}
\mbox{maximize} & g(\lambda, \nu) \\
\mbox{subject to} & \lambda \succeq 0
\end{array}
\end{aligned} %]]></script>

<p>We refer to this problem as the <strong>Lagrange dual problem</strong> associated with the standard optimization problem.</p>

<h3 id="weak-duality">Weak Duality</h3>

<p>Let us define \( d^* \) as the optimal value of the Lagrange dual problem. This is <strong>the best lower bound</strong> on \( p^* \) that can be obtained from the Lagrange dual function.</p>

<p>Even if the original problem is not convex, we can always say \(d^* \leq p^*\). We call this property weak duality.</p>

<h3 id="slaters-constraint-qualification">Slater’s Constraint Qualification</h3>

<p>Slater’s condition is a qualification on the problem constraints. It states that there exists an \(x \in \mbox{relint}(\mathcal{D})\) such that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{lll}
f_i(x) < 0, & i=1,\dots,m, & Ax=b.
\end{array} %]]></script>

<p>Why is that important? Well, because if the problem is convex, and <strong>if Slater’s condition</strong> holds, then <strong>strong duality</strong> holds.</p>

<h2 id="kkt-conditions">KKT Conditions</h2>

<h2 id="newtons-method">Newton’s Method</h2>

<p>Instead of minimizing the first-order Taylor approximation of a function \(f\), we may want to minimize the second-order Taylor approximation, \(\hat{f}\). That approximation at a point \(x\) is given by:</p>

<script type="math/tex; mode=display">\hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v</script>

<p>If the function \(f\) is quadratic, then minimizing \(\hat{f}(x+v)\) will give us an exact minimizer of \(f\). If the function \(f\) is nearly quadratic, intuition suggests that minimizing \(\hat{f}(x+v)\) should be a very good estimate of the minimizer of \(f\), i.e., \(x^*\).</p>

<p>Setting \(\nabla_v \hat{f}(x+v)=0\), we see:</p>

<h2 id="interior-point-methods">Interior Point Methods</h2>

<p>Suppose we have the following inequality constrained optimization problem:</p>

<p>We can approximate the problem as an <strong>equality constrained problem</strong>. This is highly desirable because we know that Newton’s method can be applied to equality constrained problems. We can make the inequality constraints implicit in the objective:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} & f_0(x) + \sum\limits_{i=1}^m I_{-}\bigg(f_i(x)\bigg) & \\
\mbox{subject to} & Ax = b
\end{array}
\end{aligned} %]]></script>

<p>The indicator function expresses our displeasure with respect to the satisfaction of the inequality constraints. If the indicator function is violated, its value becomes negative infinity:</p>

<script type="math/tex; mode=display">% <![CDATA[
I_{-}(u) = \begin{cases} 0 & u\leq 0 \\ \infty & u > 0 \end{cases} %]]></script>

<p>Otherwise, we ignore its presence in the objective function.  Although we were able to remove the inequality constraints, the objective function is, in general, <strong>not differentiable</strong>, so Newton’s method cannot be applied.</p>

<h3 id="the-log-barrier">The Log-Barrier</h3>

<p>However, we could approximate the indicator function \(I_{-}\) with the <strong>log-barrier</strong> function:</p>

<script type="math/tex; mode=display">\hat{I}_{-}(u) = -\bigg(\frac{1}{t}\bigg) \mbox{log }(-u)</script>

<p>Here, \(t\) is a parameter that sets the accuracy of the approximation. As \(t\) increases, the approximation becomes more accurate.</p>

<p>A figure shows the quality of the approximation [1]:</p>

<p><img src="/assets/log_barrier_approximation.png" width="70%" /></p>

<p>We now have a new problem:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} & f_0(x) + \sum\limits_{i=1}^m -\bigg(\frac{1}{t}\bigg) \mbox{log }\bigg(-f_i(x)\bigg) & \\
\mbox{subject to} & Ax = b
\end{array}
\end{aligned} %]]></script>

<p>This objective function is convex, and as long as an <strong>appropriate closedness condition</strong> holds, Newton’s method can be used to solve it.</p>

<h3 id="references">References:</h3>
<ol>
  <li>Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press, New York, NY, USA.</li>
</ol>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
<!--  
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'karpathyblog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
   -->
  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">John Lambert blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>John Lambert blog</li>
        <!-- <li><a href="mailto:"></a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/johnwlambert">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">johnwlambert</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/none">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">none</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Notes on Machine Learning and Optimization.</p>
    </div>

  </div>

</footer>


    </body>
</html>