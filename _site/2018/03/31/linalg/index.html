<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Linear Algebra Without the Agonizing Pain</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Notes on Machine Learning and Optimization.">
    <link rel="canonical" href="http://localhost:4000/2018/03/31/linalg/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="John Lambert blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
    <a href="/feed.xml">
      <img src="/assets/mom_melissa_huntington.png" width="400">
    </a>
    </div>

    <a class="site-title" href="/">John Lambert blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          <a class="page-link" href="/neuralnets/">Hacker's guide to Neural Networks</a>
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Linear Algebra Without the Agonizing Pain</h1>
    <p class="meta">Mar 31, 2018</p>
  </header>

  <article class="post-content">
  <!-- 
<svg width="800" height="200">
	<rect width="800" height="200" style="fill:rgb(98,51,20)" />
	<rect width="20" height="50" x="20" y="100" style="fill:rgb(189,106,53)" />
	<rect width="20" height="50" x="760" y="30" style="fill:rgb(77,175,75)" />
	<rect width="10" height="10" x="400" y="60" style="fill:rgb(225,229,224)" />
</svg>
 -->
<h2 id="linear-algebra-definitions">Linear Algebra Definitions</h2>
<p>Before we do anything interesting with machine learning or optimization, we’ll need to review some absolutely <strong>essential</strong> linear algebra concepts.</p>
<h3 id="matrix-rank">Matrix Rank</h3>

<h3 id="vector-space">Vector Space</h3>

<h3 id="null-space-of-a-matrix">Null Space of a Matrix</h3>

<p>Given \(A \in \mathbb{R}^{m \times n}\), the <strong>null space</strong> of \(A\) is the set of vectors which are sent to the zero vector:</p>

<script type="math/tex; mode=display">\mathcal{N}(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}</script>

<p>Multiplication by \(A\) can be seen as a function which sends a vector \(x \in \mathbb{R}^n\) to a vector \(Ax \in \mathbb{R}^m\).</p>

<p>Of course, \(\mathcal{N}(A)\) always contains the zero vector, i.e. \({0} \in \mathcal{N}(A)\). But the question is, does it contain any other vectors? If the columns of \(A\) are linearly independent, then we can always say \(\mathcal{N}(A) = {0} \).</p>

<h3 id="column-space-range-of-a-matrix">Column Space (Range) of a Matrix</h3>

<p>Given an \(m \times n\) matrix \(A\), we would like to know for which vectors \(b \in \mathbb{R}^m\) the system \(Ax = b\) has a solution. Let’s define the columns of \(A\) as:</p>

<script type="math/tex; mode=display">% <![CDATA[
A = \begin{bmatrix} | & | & & | \\ v_1 & v_2 & \cdots & v_n \\ | & | & & | \end{bmatrix} %]]></script>

<p>The column space of \(A\) is</p>

<script type="math/tex; mode=display">C(A) = \mbox{span}(v_1, v_2, \dots, v_n)</script>

<script type="math/tex; mode=display">C(A) = \{ Ax \mid x \in \mathbb{R}^n \}</script>

<p>The system \(Ax = b\) has a solution <strong>if and only if</strong> \(b \in C(A)\), equivalent to stating \(b\) is in the range of \(A\): \(b \in R(A)\).</p>

<h3 id="rank-nullity-theorem">Rank-Nullity Theorem</h3>
<p>Let \(A\) be any matrix such that \(A \in \mathbb{R}^{m \times n}\).</p>

<script type="math/tex; mode=display">\mbox{rank}(A) + \mbox{nullity}(A) = n</script>

<script type="math/tex; mode=display">\mbox{dim}\bigg(C(A)\bigg) + \mbox{dim}\bigg(N(A)\bigg) = n</script>

<h3 id="orthogonal-complement">Orthogonal Complement</h3>

<h3 id="matrix-calculus">Matrix Calculus</h3>

<p>Two identities are essential: gradients of matrix-vector products and of quadratic forms.</p>
<ul>
  <li>\( \nabla_x (Ax) = A^T\)</li>
  <li>\(\nabla_x (x^TAx) = Ax + A^Tx\)</li>
</ul>

<p>When \(A\) is symmetric, which is often the case, \(A = A^T\) and thus \(\nabla_x (x^TAx) = 2Ax \)</p>

<p>John Duchi explains exactly why identies are true in <a href="https://web.stanford.edu/~jduchi/projects/matrix_prop.pdf">[1]</a>.</p>

<h3 id="gram-schmidt">Gram Schmidt</h3>

<h2 id="solving-systems-of-equations">Solving Systems of Equations</h2>

<h3 id="overdetermined-systems">Overdetermined Systems</h3>
<p>Here, matrix \(A\) is a skinny, full-rank matrix. We cannot solve such a system, so instead we minimize a residual \(r\), i.e. we minimize \(\lVert r \rVert^2 = \lVert Ax-y \rVert^2\).  We find an approximate solution to \(Y=Ax\). Formally, we minimize some objective function \(J\):
<script type="math/tex">% <![CDATA[
\begin{align}
\begin{array}{ll}
\mbox{minimize} & J \\
& \lVert r \rVert^2 \\
 &  \lVert Ax-y \rVert^2 \\
& (Ax-y)^T (Ax-y) \\
& (Ax)^T(Ax) - y^TAx - (Ax)^Ty + y^Ty \\
& x^TA^TAx - y^TAx - x^TA^Ty + y^Ty
\end{array}
\end{align} %]]></script></p>

<p>We can set its gradient to zero, and since the objective is the square of an affine function, it is convex, so we can find its true, global minimum:</p>

<script type="math/tex; mode=display">\nabla_x J = 2(A^TA)x - 2A^Ty = 0</script>

<script type="math/tex; mode=display">2(A^TA)x = 2A^Ty</script>

<script type="math/tex; mode=display">(A^TA)x = A^Ty</script>

<p>Multiply on the left by \((A^TA)^{-1}\), and we recover the least squares solution:</p>

<script type="math/tex; mode=display">x_{ls} = (A^TA)^{-1}A^Ty = A^{\dagger}y</script>

<p>We call \(A^{\dagger}\) a <strong>left-inverse</strong> of \(A\) because \(A^{\dagger}A=I\).</p>

<h3 id="underdetermined-systems">Underdetermined Systems</h3>
<p>Here \(A\) is a fat, full-rank matrix. We can <strong>always</strong> solve such a system, and there will be an infinite # of solutions.</p>

<p>We often choose to find the smallest solution, i.e. the one closest to the origin. We call this a least-norm (\(x_{ln}\) ) solution, because we minimize \(\lVert x \rVert\):</p>

<script type="math/tex; mode=display">x_{ln} = A^T(AA^T)^{-1}y = A^{\dagger}y</script>

<p>We call \(A^{\dagger}\) a right-inverse of \(A\) because \(AA^{\dagger}=I\).</p>

<h2 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h2>

<h3 id="svd-definition">SVD Definition</h3>

<script type="math/tex; mode=display">% <![CDATA[
A=U\Sigma V^T = \begin{bmatrix} u_1 & \dots & u_r \end{bmatrix} \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_r \end{bmatrix} \begin{bmatrix} v_1^T \\ \vdots \\ v_r^T \end{bmatrix} %]]></script>

<p>where \(U\), \(V\) are orthogonal matrices, meaning \(U^TU = I\), \(UU^T=I\).</p>

<p>We call \(V=\begin{bmatrix} v_1, \dots, v_r \end{bmatrix}\) the right/input singular vectors, because this is the first matrix to interact with an input vector \(x\) when we compute \(y=Ax\).</p>

<p>We call \(U=\begin{bmatrix} u_1, \dots, u_r \end{bmatrix}\) the left/output singular vectors, because this is the last matrix that the intermediate results are multiplied before we obtain our result ( \(y=Ax\) ).</p>

<h3 id="computation-of-the-svd">Computation of the SVD</h3>

<p>To find this decomposition for a matrix \(A\), we’ll need to compute the \(V\)’s.</p>

<script type="math/tex; mode=display">A^TA = (V\Sigma U^T) (U \Sigma V^T)</script>

<p>This reduces to \(V \Sigma^2 V^T\). We need to find orthonormal eigenvectors, and the \(V_i\)’s are simply the eigenvectors of \(A^TA\).</p>

<p>Now, we’ll need to compute the \(U\)’s.</p>

<script type="math/tex; mode=display">AA^T = (U \Sigma V^T)(V\Sigma U^T) = U \Sigma^2 U^T</script>

<p>The \(U_i\)’s are the eigenvectors of \(AA^T\).</p>
<h3 id="svd-applications">SVD Applications</h3>

<p>We can use the SVD to compute the general pseudo-inverse of a matrix for \(y=Ax\):</p>

<p>For skinny/full-rank matrices:</p>

<h2 id="extremal-trace-problems">Extremal Trace Problems</h2>

<h2 id="eigenvectors">Eigenvectors</h2>

<h2 id="unconstrained-optimization">Unconstrained Optimization</h2>

<h3 id="the-gauss-newton-method">The Gauss-Newton Method</h3>

<p>Suppose our residual is no longer affine, but rather nonlinear. We want to minimize \(\lVert r(x) \rVert^2\). Generally speaking, we cannot solve this problem, but rather can use good heuristics to find local minima.</p>

<ul>
  <li>
    <p>Start from initial guess for your solution</p>
  </li>
  <li>
    <p>Repeat:</p>
  </li>
  <li>(1) Linearize \(r(x)\) around current guess \(x^{(k)}\). This can be accomplished by using a Taylor Series and Calculus (Standard Gauss-Newton), or one can use a least-squares fit to the line.</li>
  <li>(2) Solve least squares for linearized objective, get \(x^{(k+1)}\).</li>
</ul>

<p>The linearized residual \(r(x)\) will resemble:</p>

<script type="math/tex; mode=display">r(x) \approx r(x^{(k)}) + Dr(x^{(k)}) (x-x^{(k)})</script>

<p>where \(Dr\) is the Jacobian, meaning \( (Dr)_{ij} = \frac{\partial r_i}{\partial x_j}\)</p>

<p>Distributing the rightmost product, we obtain</p>

<script type="math/tex; mode=display">r(x) \approx Dr(x^{(k)})x - \bigg(Dr(x^{(k)}) (x^{(k)}) - r(x^{(k)}) \bigg)</script>

<p>With a single variable \(x\), we can re-write the above equation as</p>

<script type="math/tex; mode=display">r(x) \approx A^{(k)}x - b^{(k)}</script>

<h3 id="levenberg-marquardt-algorithm-trust-region-gauss-newton-method">Levenberg-Marquardt Algorithm (Trust-Region Gauss-Newton Method)</h3>

<p>In Levenberg-Marquardt, we have add a term to the objective function to emphasize that we should not move so far from \( \theta^{(k)} \) that we cannot trust the affine approximation. We often refer to this concept as remaining within a “trust region” (<a href="https://arxiv.org/abs/1502.05477">TRPO</a> is named after the same concept). Thus, we wish 
<script type="math/tex">|| \theta - \theta^{(k)} ||^2</script>
 to be small. Our new objective is:</p>

<script type="math/tex; mode=display">||A^{(k)} \theta - b^{(k)}||^2 + \lambda^{(k)} || \theta − \theta^{(k)}||^2</script>

<p>This objective can be written inside a single \(\ell_2\)-norm, instead using two separate \(\ell_2\)-norms:</p>

<script type="math/tex; mode=display">|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \end{bmatrix} ||^2</script>

<p>Suppose we have some input data \(x\) and labels \(y\). Our prediction function could be</p>

<script type="math/tex; mode=display">\hat{f} = x^T\theta_1 + \theta_2</script>

<p>Suppose at inference time we use \(f(x) = \mbox{sign }\bigg(\hat{f}(x)\bigg)\), where \(\mbox{sign }(a) = +1\) for \(a \geq 0\) and −1 for \(a &lt; 0\). At training time, we use its smooth approximation, the hyperbolic tangent, tanh:</p>

<script type="math/tex; mode=display">\phi(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}</script>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phi</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">(</span><span class="nb">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="p">/(</span><span class="nb">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">));</span>
</code></pre></div></div>

<p>The gradient of \(\mbox{tanh}\): \(\nabla_x \mbox{tanh } = 1-\mbox{tanh }(x)^2\). We call this \(\phi^{\prime}\) in code:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phiprime</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">);</span>
</code></pre></div></div>

<p>Suppose our objective function is the MSE loss, with a regularization term:</p>

<script type="math/tex; mode=display">J = \sum\limits_{i=1}^{N} \Bigg(y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)\Bigg)^2 + \mu ||\theta_1||^2</script>

<p>The residual for a single training example \(i\) is \(r_i\) is</p>

<script type="math/tex; mode=display">y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)</script>

<p>For a vector of training examples \(\mathbf{X}\) and labels \(\mathbf{Y}\), our nonlinear residual function is:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r</span> <span class="o">=</span> <span class="o">@</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">)</span> <span class="n">y</span><span class="o">-</span><span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="o">'*</span><span class="n">t1</span><span class="o">+</span><span class="n">t2</span><span class="p">);</span>
</code></pre></div></div>

<p>To linearize the residual, we compute its Jacobian \(Dr(\theta_1,\theta_2)\) via matrix calculus:</p>

<script type="math/tex; mode=display">\frac{\partial r_i}{\partial \theta_1} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)x_i^T</script>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jacobian_0_entr</span> <span class="o">=</span> <span class="o">-</span><span class="n">phiprime</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="s1">'*theta(1:400)+theta(end))* X(:,i)'</span>
</code></pre></div></div>

<p><script type="math/tex">\frac{\partial r_i}{\partial \theta_2} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)</script></p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jacobian_1_entr</span> <span class="o">=</span> <span class="o">-</span><span class="n">phiprime</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="o">'*</span><span class="n">theta</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">400</span><span class="p">)</span><span class="o">+</span><span class="n">theta</span><span class="p">(</span><span class="k">end</span><span class="p">))</span>
</code></pre></div></div>
<p>The the full Jacobian evaluated at a certain point \(X_i\) is just these stacked individual entries:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Dr</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">labels_train</span><span class="p">),</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="k">for</span> <span class="nb">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">length</span><span class="p">(</span><span class="n">labels_train</span><span class="p">)</span>
	<span class="n">Dr</span><span class="p">(</span><span class="nb">i</span><span class="p">,:)</span> <span class="o">=</span> <span class="p">[</span><span class="n">jacobian_0_entr</span><span class="p">,</span> <span class="n">jacobian_1_entr</span><span class="p">];</span>
<span class="k">end</span>
</code></pre></div></div>
<p>Let \(\theta= \begin{bmatrix} \theta_1^T &amp; \theta_2 \end{bmatrix}^T \in \mathbb{R}^{401}\). The linearized residual follows the exact form outlined in the Gauss-Newton section above:</p>

<script type="math/tex; mode=display">r(\theta) \approx A^{(k)}\theta - b^{(k)}</script>

<p>where</p>

<script type="math/tex; mode=display">b^{(k)} = A^{(k)} \theta^{(k)} - r\bigg(\theta^{(k)}\bigg)</script>

<p>In code, this term is computed as:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A_k_temp</span> <span class="o">=</span> <span class="n">Dr</span><span class="p">;</span> <span class="c1">% computed above</span>
<span class="n">b_k_temp</span> <span class="o">=</span> <span class="n">Dr</span><span class="o">*</span><span class="n">theta</span> <span class="o">-</span> <span class="n">r</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span><span class="n">theta</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">400</span><span class="p">),</span><span class="n">theta</span><span class="p">(</span><span class="k">end</span><span class="p">));</span>
</code></pre></div></div>

<p>We solve a least-squares problem in every iteration, with a 3-part objective function (penalizing the residual, large step sizes, and also large \(\theta_1\)-norm weights):</p>

<script type="math/tex; mode=display">% <![CDATA[
|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I_{401} \\ \begin{bmatrix} \sqrt{\mu} I_{400 } & 0\end{bmatrix} \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \\ 0 \end{bmatrix} ||^2. %]]></script>

<p>We represent the left term by</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A_k</span> <span class="o">=</span> <span class="p">[</span><span class="n">A_k_temp</span><span class="p">;</span> <span class="nb">sqrt</span><span class="p">(</span><span class="n">lambda</span><span class="p">(</span><span class="n">itr</span><span class="p">))</span><span class="o">*</span><span class="nb">eye</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span> <span class="nb">sqrt</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="nb">eye</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)]];</span>
</code></pre></div></div>
<p>and the right term by</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b_k = [b_k_temp; sqrt(lambda(itr))*theta; zeros(length(theta)-1,1)];
</code></pre></div></div>

<p>We solve for the next iterate of \(\theta\) with the pseudo-inverse:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta_temp</span> <span class="o">=</span> <span class="n">A_k</span><span class="p">\</span><span class="n">b_k</span><span class="p">;</span>
</code></pre></div></div>

<p>The full algorithm might resemble:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="c1">%regularization coefficient</span>
<span class="n">lambda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">%initial lambda for Levenberg-Marquardt</span>

<span class="n">theta</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="mi">401</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="c1">%initial value for theta (last entry is theta_2)</span>

<span class="k">for</span> <span class="n">itr</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="mi">15</span>
	<span class="c1">%calculate Jacobian at the current iteration (see code above)</span>

	<span class="c1">%linearize the objective function (see code above)</span>

	<span class="c1">% stopping condition ...</span>
<span class="k">end</span>

</code></pre></div></div>

<p>References:</p>

<ol>
  <li>Duchi, John. <a href="https://web.stanford.edu/~jduchi/projects/matrix_prop.pdf">Properties of the Trace and Matrix Derivatives</a>.</li>
</ol>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
<!--  
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'karpathyblog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
   -->
  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">John Lambert blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>John Lambert blog</li>
        <!-- <li><a href="mailto:"></a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/johnwlambert">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">johnwlambert</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/none">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">none</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Notes on Machine Learning and Optimization.</p>
    </div>

  </div>

</footer>


    </body>
</html>