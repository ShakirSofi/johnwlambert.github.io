<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://johnwlambert.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://johnwlambert.github.io/" rel="alternate" type="text/html" /><updated>2019-01-03T22:40:33-05:00</updated><id>http://johnwlambert.github.io/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">Lie Groups and Rigid Body Kinematics</title><link href="http://johnwlambert.github.io/lie-groups/" rel="alternate" type="text/html" title="Lie Groups and Rigid Body Kinematics" /><published>2018-12-28T06:00:00-05:00</published><updated>2018-12-28T06:00:00-05:00</updated><id>http://johnwlambert.github.io/lie-groups</id><content type="html" xml:base="http://johnwlambert.github.io/lie-groups/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whyliegroups&quot;&gt;Why do we need Lie Groups?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#liegroups&quot;&gt;Lie Groups&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#son&quot;&gt;SO(N)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so2&quot;&gt;SO(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so3&quot;&gt;SO(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se2&quot;&gt;SE(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se3&quot;&gt;SE(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conjugation&quot;&gt;Conjugation in Group Theory&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lie-algebra&quot;&gt;The Lie Algebra&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;whyliegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-lie-groups&quot;&gt;Why do we need Lie Groups?&lt;/h2&gt;

&lt;p&gt;Rigid bodies have a state which consists of position and orientation. When sensors are placed on a rigid body (e.g. a robot), they provide measurements in the body frame. Suppose we wish to take a measurement &lt;script type=&quot;math/tex&quot;&gt;y_b&lt;/script&gt; from the body frame and move it to the world frame, &lt;script type=&quot;math/tex&quot;&gt;y_w&lt;/script&gt;. We can do this via left multiplication with a transformation matrix &lt;script type=&quot;math/tex&quot;&gt;{}^{w}T_{b}&lt;/script&gt;, a member of the matrix Lie groups, that transports the point from one space to another space:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_w = {}^{w}T_{b} y_b&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;liegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lie-groups&quot;&gt;Lie Groups&lt;/h2&gt;

&lt;p&gt;When we are working with pure rotations, we work with Special Orthogonal groups, &lt;script type=&quot;math/tex&quot;&gt;SO(\cdot)&lt;/script&gt;. When we are working with a rotation and a translation together, we work with Special Euclidean groups &lt;script type=&quot;math/tex&quot;&gt;SE(\cdot)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Lie Groups are unique because they are &lt;strong&gt;both a group and a manifold&lt;/strong&gt;. They are continuous manifolds in high-dimensional spaces, and have a group structure. I’ll describe them in more detail below.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;son&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;son&quot;&gt;SO(N)&lt;/h3&gt;

&lt;p&gt;Membership in the Special Orthogonal Group &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt; requires two matrix properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;R^TR = I&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{det}(R) = +1&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives us a very helpful property: &lt;script type=&quot;math/tex&quot;&gt;R^{-1} = R^T&lt;/script&gt;, so the matrix inverse is as simple as taking the transpose.  We will generally work with &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;N=2,3&lt;/script&gt;, meaning the matrices are rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{2 \times 2}&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;These rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; are not commutative.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;so2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so2&quot;&gt;SO(2)&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is a 1D manifold living in a 2D Euclidean space, e.g. moving around a circle.  We will be stuck with singularities if we use 2 numbers to parameterize it, which would mean kinematics break down at certain orientations.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is the space of orthogonal matrices that corresponds to rotations in the plane.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A simple example&lt;/strong&gt;:
Let’s move from the body frame &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; to a target frame &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_t = {}^tR_b(\theta) P_b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
5 \mbox{ cos} (\theta) \\
5 \mbox{ sin} (\theta)
\end{bmatrix} = \begin{bmatrix} cos(\theta) &amp; -sin(\theta) \\ sin(\theta) &amp; cos(\theta) \end{bmatrix} * \begin{bmatrix} 5 \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;As described in [3], another way to think of this is to consider that a robot can be rotated counterclockwise by some angle &lt;script type=&quot;math/tex&quot;&gt;\theta \in [0,2 \pi)&lt;/script&gt; by mapping every &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x, y) \rightarrow (x \mbox{ cos } \theta − y \mbox{ sin } \theta, x \mbox{ sin } \theta + y \mbox{ cos } \theta).&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;so3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so3&quot;&gt;SO(3)&lt;/h3&gt;

&lt;p&gt;There are several well-known parameterizations of &lt;script type=&quot;math/tex&quot;&gt;R \in SO(3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1.) &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt; full rotation matrix, 9 parameters – there must be 6 constraints&lt;/li&gt;
  &lt;li&gt;(2.) Euler angles, e.g. &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt;, so 3 parameters&lt;/li&gt;
  &lt;li&gt;(3.) Angle-Axis parameters &lt;script type=&quot;math/tex&quot;&gt;(\vec{a}, \phi)&lt;/script&gt;, which is 4 parameters and 1 constraint (unit length)&lt;/li&gt;
  &lt;li&gt;(4.) Quaternions (&lt;script type=&quot;math/tex&quot;&gt;q_0,q_1,q_2,q_3)&lt;/script&gt;, 4 parameters and 1 constraint (unit length)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are only 3 degrees of freedom in describing a rotation. But this object doesn’t live in 3D space. It is a 3D manifold, embedded in a 4-D Euclidean space.&lt;/p&gt;

&lt;p&gt;Parameterizations 1,3,4 are overconstrained, meaning they employ more parameters than we strictly need. With overparameterized representations, we have to do extra work to make sure we satisfy the constraints of the representation.&lt;/p&gt;

&lt;p&gt;As it turns out &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; cannot be parameterized by only 3 parameters in a non-singular way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Euler Angles&lt;/strong&gt;
One parameterization of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; is to imagine three successive rotations around different axes. The Euler angles encapsulate yaw-pitch-roll: first, a rotation about the z-axis (yaw, &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;). Then, a rotation about the pitch axis by &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (via right-hand rule), and finally we perform a roll by &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/euler_angles.jpg&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    The Euler angles.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The sequence of successive rotations is encapsulated in &lt;script type=&quot;math/tex&quot;&gt;{}^{w}R_b&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{w}R_b = R_R(\phi) R_P(\theta) R_Y (\psi)&lt;/script&gt;

&lt;p&gt;As outlined in [3], these successive rotations by &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt; are defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{yaw} = R_y (\psi) = \begin{bmatrix}
\mbox{cos} \psi &amp; -\mbox{sin} \psi  &amp; 0 \\
\mbox{sin} \psi &amp; \mbox{cos} \psi &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{pitch} = R_p (\theta) = \begin{bmatrix}
\mbox{cos} \theta &amp; 0 &amp; \mbox{sin} \theta \\
 0 &amp; 1 &amp; 0 \\
 -\mbox{sin} \theta &amp; 0 &amp; \mbox{cos} \theta \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{roll} = R_R (\phi) = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; \mbox{cos} \phi &amp; -\mbox{sin} \phi \\
0 &amp; \mbox{sin} \phi &amp;  \mbox{cos} \phi \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;You have probably noticed that each rotation matrix &lt;script type=&quot;math/tex&quot;&gt;\in \mathbf{R}^{3 \times 3}&lt;/script&gt; above is a simple extension of the 2D rotation matrix from &lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt;. For example, the yaw matrix &lt;script type=&quot;math/tex&quot;&gt;R_{yaw}&lt;/script&gt; performs a 2D rotation with respect to the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; coordinates while leaving the &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; coordinate unchanged [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se2&quot;&gt;SE(2)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;3 \times 3&lt;/script&gt; matrices, moving a point in homogenous coordinates to a new frame. It is important to remember that this represents a rotation followed by a translation (not the other way around).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T = \begin{bmatrix} x_w \\ y_w \\ 1 \end{bmatrix} = \begin{bmatrix}
 R_{2 \times 2}&amp; &amp; t_{2 \times 1}  \\
&amp; \ddots &amp; \vdots  \\
0 &amp; 0 &amp; 1
\end{bmatrix} * \begin{bmatrix} x_b \\ y_b \\ 1 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;By adding an extra dimension to the input points and transformation matrix &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; the translational part of the transformation is absorbed [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se3&quot;&gt;SE(3)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(3)&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;4 \times 4&lt;/script&gt; matrices, the real space resembles:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; R_{3 \times 3} &amp; &amp;  t_{3 \times 1} \\
&amp; &amp; &amp; \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;conjugation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conjugation-in-group-theory&quot;&gt;Conjugation in Group Theory&lt;/h2&gt;

&lt;p&gt;Surprisingly, movement in &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; can always be achieved by moving somewhere, making a rotation there, and then moving back.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SE(2) = (\cdot) SO(2) (\cdot)&lt;/script&gt;

&lt;p&gt;If we move to a point by vector movement &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, essentially we perform: &lt;script type=&quot;math/tex&quot;&gt;B-p&lt;/script&gt;, then go back with &lt;script type=&quot;math/tex&quot;&gt;p + \dots&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
B^{\prime} = \begin{bmatrix}
R &amp; t \\
0 &amp; 1
\end{bmatrix}_B = \begin{bmatrix}
I &amp; p \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
R &amp; 0 \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
I &amp; -p \\
0 &amp; 1
\end{bmatrix}_B %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;lie-algebra&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;connecting-spatial-coordinates-and-spatial-velocities-the-lie-algebra&quot;&gt;Connecting Spatial Coordinates and Spatial Velocities: The Lie Algebra&lt;/h2&gt;

&lt;p&gt;The Lie Algebra maps spatial coordinates to spatial velocity.&lt;/p&gt;

&lt;p&gt;In the case of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt;, the Lie Algebra is the space of skew-symmetric matrices&lt;/p&gt;

&lt;p&gt;Rigid Body Kinematics: we want a differential equation (ODE) that links &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{}^{^w}R_b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;in particular,
&lt;script type=&quot;math/tex&quot;&gt;{}^{^w} \dot{R}_r = f({}^{^w}R_b, \vec{\omega})&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\frac{ \partial }{\partial t}(R^R = I)
 \\
\dot{R}^TR + R^T \dot{R} = 0 \\
\dot{R}^TR = -R^T \dot{R} 
\end{aligned}&lt;/script&gt;

&lt;p&gt;we define &lt;script type=&quot;math/tex&quot;&gt;\Omega = R^T \dot{R}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^T = (R^T \dot{R} )^T = \dot{R}^TR  = -R^T \dot{R}  = -\Omega&lt;/script&gt;

&lt;p&gt;Skew-symmetric! $\Omega^T = -\Omega$&lt;/p&gt;

&lt;p&gt;In fact,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Omega = \begin{bmatrix}
0 &amp; -\omega_z &amp; \omega_y \\
\omega_z &amp; 0 &amp; -\omega_x \\
-\omega_y &amp; \omega_x &amp; 0
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega} = \begin{bmatrix} \omega_x \\ \omega_y \\ \omega_z \end{bmatrix}&lt;/script&gt; is the angular velocity&lt;/p&gt;

&lt;p&gt;Notation! the “hat” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\omega}^{\hat{}} =\Omega&lt;/script&gt;

&lt;p&gt;the “v” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^{v} = \bar{\omega}&lt;/script&gt;

&lt;p&gt;So we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\dot{R} = R \Omega \\
\Omega = \bar{\omega}^{\hat{}}
\end{aligned}&lt;/script&gt;

&lt;p&gt;\item In terms of frames, we have Poisson’s kinematic equation&lt;/p&gt;

&lt;p&gt;\item The intrinsic equation is (where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}_b&lt;/script&gt; in the body frame is $\Omega_b$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^w}\dot{R}_b =  {}^{^w}R_b \Omega_b&lt;/script&gt;

&lt;p&gt;The “strap-down” equation (extrinsic)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^b}\dot{R}_w =  - \Omega_b {}^{^b}R_w&lt;/script&gt;

&lt;p&gt;Take vector on aircraft, put it into the coordinate frame of the world&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{}_{^w}R_b = \begin{bmatrix} | &amp; | &amp; | \\ {}^{^w} \hat{x}_b &amp; {}^{^w}\hat{y}_b &amp; {}^{^w} \hat{z}_b \\ | &amp; | &amp; |  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega \vec{v}_b = \omega \times \vec{v}_b&lt;/script&gt;

&lt;p&gt;Can we use this for filtering? Can we use &lt;script type=&quot;math/tex&quot;&gt;\dot{R} = R \Omega&lt;/script&gt; directly in an EKF, UKF
We’d have to turn it into discrete-time 
Naively, you might do the first order Euler approximation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ R_{t + \delta_t} -R_t}{\delta t} \approx R_t \Omega_t&lt;/script&gt;

&lt;p&gt;This does not work!&lt;/p&gt;

&lt;p&gt;You won’t maintain orthogonality! The defining property of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; was orthogonality, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{t + \delta t} \not\in SO(3)&lt;/script&gt;

&lt;p&gt;and pretty soon&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
R_{t + \delta t}^T R_{t + \delta t} \neq I \\
\mbox{det }(R_{t+\delta t}) \neq +1
\end{aligned}&lt;/script&gt;

&lt;p&gt;The 3x3 matrix will not be recognizable of a rotation&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frank Dellaert. Lecture Presentations of MM 8803: Mobile Manipulation, taught at the Georgia Institute of Technology, Fall 2018.&lt;/p&gt;

&lt;p&gt;[2] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.&lt;/p&gt;

&lt;p&gt;[3] Steven M. LaValle. &lt;em&gt;Planning Algorithms&lt;/em&gt;. Cambridge University Press, 2006, New York, NY, USA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rigid Body Kinematics and Filtering&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kinematics with Euler Angles&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{R} = f(R, \vec{w}), (\dot{\phi}, \dot{\theta}, \dot{\psi}) = f(\phi, \theta, \psi, \vec{\omega})
\end{equation}&lt;/p&gt;

&lt;p&gt;\item&lt;/p&gt;

&lt;p&gt;\begin{equation}
\vec{\omega} = \begin{bmatrix} w_x \ w_y \ w_z \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; -\mbox{sin} \theta \ 0 &amp;amp; \mbox{cos} \phi &amp;amp; \mbox{sin} \phi \mbox{cos} \phi \ 0 &amp;amp; -\mbox{sin} \phi &amp;amp; \mbox{cos} \phi \mbox{cos} \theta \end{bmatrix} \begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;\item We want to invert this matrix and solve for $\begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}$&lt;/p&gt;</content><author><name></name></author><summary type="html">SO(2), SO(3), SE(2), SE(3), Lie algebras</summary></entry><entry><title type="html">Recall and Precision</title><link href="http://johnwlambert.github.io/recall-precision/" rel="alternate" type="text/html" title="Recall and Precision" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/precision-recall</id><content type="html" xml:base="http://johnwlambert.github.io/recall-precision/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#recall&quot;&gt;Recall&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#precision&quot;&gt;Precision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#map&quot;&gt;Mean Average Precision (mAP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-more-finely-grained-measures-of-accuracy&quot;&gt;The Need for More Finely-Grained Measures of Accuracy&lt;/h2&gt;

&lt;p&gt;We’ll suppose that we are performing binary classification: classifying objects into different classes.&lt;/p&gt;

&lt;p&gt;Mean Average Precision involves computing the area under a curve (an integral), and can actually be quite confusing.&lt;/p&gt;

&lt;h3 id=&quot;recall&quot;&gt;Recall&lt;/h3&gt;

&lt;p&gt;Recall measures how many objects you missed. You can have very high recall by classifying everythin as some class (at the expense of precision).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{Recall} = \frac{tp}{tp+fn}&lt;/script&gt;

&lt;h3 id=&quot;precision&quot;&gt;Precision&lt;/h3&gt;

&lt;p&gt;Precision measures your discriminative ability. If you claimed that all of the objects you saw were of a particular class, and you were usually wrong because they belonged to a different class, you would have low precision. Your judgments can’t be considered &lt;em&gt;precise&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{Precision} = \frac{tp}{tp+fp}&lt;/script&gt;

&lt;h2 id=&quot;mean-average-precision&quot;&gt;Mean Average Precision&lt;/h2&gt;

&lt;p&gt;When performing the task of object detection, we would like to be discriminative not just about two classes&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def voc_eval(detpath,
             annopath,
             imagesetfile,
             classname,
             cachedir,
             ovthresh=0.5,
             use_07_metric=False):
    &quot;&quot;&quot;rec, prec, ap = voc_eval(detpath,
                                annopath,
                                imagesetfile,
                                classname,
                                [ovthresh],
                                [use_07_metric])

    Top level function that does the PASCAL VOC evaluation.

    detpath: Path to detections
        detpath.format(classname) should produce the detection results file.
    annopath: Path to annotations
        annopath.format(imagename) should be the xml annotations file.
    imagesetfile: Text file containing the list of images, one image per line.
    classname: Category name (duh)
    cachedir: Directory for caching the annotations
    [ovthresh]: Overlap threshold (default = 0.5)
    [use_07_metric]: Whether to use VOC07's 11 point AP computation
        (default False)
    &quot;&quot;&quot;
    # assumes detections are in detpath.format(classname)
    # assumes annotations are in annopath.format(imagename)
    # assumes imagesetfile is a text file with each line an image name
    # cachedir caches the annotations in a pickle file

    # first load gt
    if not os.path.isdir(cachedir):
        os.mkdir(cachedir)
    cachefile = os.path.join(cachedir, 'annots.pkl')
    # read list of images
    with open(imagesetfile, 'r') as f:
        lines = f.readlines()
    imagenames = [x.strip() for x in lines]

    if not os.path.isfile(cachefile):
        # load annots
        recs = {}
        for i, imagename in enumerate(imagenames):
            recs[imagename] = parse_rec(annopath.format(imagename))
            if i % 100 == 0:
                print 'Reading annotation for {:d}/{:d}'.format(
                    i + 1, len(imagenames))
        # save
        print 'Saving cached annotations to {:s}'.format(cachefile)
        with open(cachefile, 'w') as f:
            cPickle.dump(recs, f)
    else:
        # load
        with open(cachefile, 'r') as f:
            recs = cPickle.load(f)

    # extract gt objects for this class
    class_recs = {}
    npos = 0
    for imagename in imagenames:
        R = [obj for obj in recs[imagename] if obj['name'] == classname]
        bbox = np.array([x['bbox'] for x in R])
        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)
        det = [False] * len(R)
        npos = npos + sum(~difficult)
        class_recs[imagename] = {'bbox': bbox,
                                 'difficult': difficult,
                                 'det': det}

    # read dets
    detfile = detpath.format(classname)
    with open(detfile, 'r') as f:
        lines = f.readlines()

    splitlines = [x.strip().split(' ') for x in lines]
    image_ids = [x[0] for x in splitlines]
    confidence = np.array([float(x[1]) for x in splitlines])
    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])

    # sort by confidence
    sorted_ind = np.argsort(-confidence)
    sorted_scores = np.sort(-confidence)
    BB = BB[sorted_ind, :]
    image_ids = [image_ids[x] for x in sorted_ind]

    # go down dets and mark TPs and FPs
    nd = len(image_ids)
    tp = np.zeros(nd)
    fp = np.zeros(nd)
    for d in range(nd):
        R = class_recs[image_ids[d]]
        bb = BB[d, :].astype(float)
        ovmax = -np.inf
        BBGT = R['bbox'].astype(float)

        if BBGT.size &amp;gt; 0:
            # compute overlaps
            # intersection
            ixmin = np.maximum(BBGT[:, 0], bb[0])
            iymin = np.maximum(BBGT[:, 1], bb[1])
            ixmax = np.minimum(BBGT[:, 2], bb[2])
            iymax = np.minimum(BBGT[:, 3], bb[3])
            iw = np.maximum(ixmax - ixmin + 1., 0.)
            ih = np.maximum(iymax - iymin + 1., 0.)
            inters = iw * ih

            # union
            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +
                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *
                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)

            overlaps = inters / uni
            ovmax = np.max(overlaps)
            jmax = np.argmax(overlaps)

        if ovmax &amp;gt; ovthresh:
            if not R['difficult'][jmax]:
                if not R['det'][jmax]:
                    tp[d] = 1.
                    R['det'][jmax] = 1
                else:
                    fp[d] = 1.
        else:
            fp[d] = 1.

    # compute precision recall
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, use_07_metric)

    return rec, prec, ap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Bharath Hariharan and Ross Girshick. Fast/er R-CNN. &lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py&quot;&gt;https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The Histogram Filter</title><link href="http://johnwlambert.github.io/histogram-filter/" rel="alternate" type="text/html" title="The Histogram Filter" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/histogram-filter</id><content type="html" xml:base="http://johnwlambert.github.io/histogram-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-histogram-filter&quot;&gt;The Histogram Filter&lt;/h2&gt;

&lt;p&gt;An alternative to continuous distributions are piecewise constant approximations, such as histograms. These are &lt;em&gt;nonparametric&lt;/em&gt; filters, since they do not utilize parameters like a mean and covariance &lt;script type=&quot;math/tex&quot;&gt;(\mu, \Sigma)&lt;/script&gt; to define a distribution. Thus, nonparametric filters do not rely on a &lt;em&gt;fixed functional form of the posterior&lt;/em&gt;, like the Gaussian does [1].&lt;/p&gt;

&lt;p&gt;The Histogram Filter is a type of nonparametric filters that discretizes the state space into a finite number of regions. The histogram assigns to each region a single cumulative probability.&lt;/p&gt;

&lt;h2 id=&quot;1-d-histogram-filter&quot;&gt;1-D Histogram Filter&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;for all \(k\) do:
    &lt;ul&gt;
      &lt;li&gt;\( \hat{p}&lt;em&gt;{k,t} = \sum\limits_i p(X_t = x_k \mid u_t, X&lt;/em&gt;{t-1} = x_i) p_{i,t-1} \)&lt;/li&gt;
      &lt;li&gt;\( p_{k,t} = \eta p(z_t \mid X_t = x_k) \hat{p}_{k,t}) \)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-d-histogram-filter-and-grid-localization&quot;&gt;2-D Histogram Filter and Grid Localization&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Sebastian Thrun, Wolfram Burgard, Dieter Fox. &lt;em&gt;Probabilistic Robotics&lt;/em&gt;. The MIT Press, Cambridge, MA, 2005.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Spectral Clustering</title><link href="http://johnwlambert.github.io/spectral-clustering/" rel="alternate" type="text/html" title="Spectral Clustering" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/spectral-clustering</id><content type="html" xml:base="http://johnwlambert.github.io/spectral-clustering/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;spectral-clustering&quot;&gt;Spectral Clustering&lt;/h2&gt;

&lt;p&gt;\subsection{Graph Partitioning}
\begin{itemize}
\item Graph-cut problem: partition graph such that (1) edges between groups have a very low weight, and (2) edges within a group have high weight
\item Could use \textbf{Ratio Cut} (by size of the component) or the \textbf{Normalized Cut} (or by volume of the component, aka the sum of degrees)
\item Penalize using very small components, or very large components
\item These are NP-hard combinatorial problems. But Spectral clustering offers a way to solve the relaxation version of these problems
\item NCut -&amp;gt; use random-walk laplacian
\item RatioCut -&amp;gt; use unnormalized laplacian
\item 2-partition: assign by vector values of Fiedler vectors $\in \mathbbm{R}$, which ones $\in \mathbbm{R}&lt;em&gt;+$, or in $\mathbbm{R}&lt;/em&gt;-$
\item Can apply k-means or standard clustering algorithm on the embedded points (transform graph clustering into a point clustering problem!). Take you into point cloud setting
\item K-means minimizes the distortion measure/energy
\begin{equation}
J = \sum\limits_j \sum\limits_k r_{ji}(y_j - \mu_i)^2 ??
\end{equation}
\item Centroid already minimizes the sum of squared distances
\item Can only reduce energy in every step (locally converge to minimum)
\item Can discover number of clusters that you need – look at gap between eigenvalues, where is there a large gap $|\lambda_k - \lambda_{k-1}|$. 
\item Eigenvalues drop fast, then stabilize
\item Spectral clustering can cluster spirals of points, where k-nearest neighbors in this space would epicly fail
\item Look at data at a very different way… Even though data comes from a Euclidean space, easier to understand in the spectral space
\item edge weight $\mbox{exp} (-diff(pixel_{i}, pixel_j)/t^2 )$
\item Get reasonable, but not perfect, results for 3d segmentation
\item Fast and efficient with decent results
\end{itemize}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. &lt;em&gt;Graph Laplacians, Laplacian Embeddings, and Spectral Clustering&lt;/em&gt;. Lectures of CS233: Geometric and Topological Data Analysis, taught at Stanford University in Spring 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Understanding Multivariate Gaussians and Covariance</title><link href="http://johnwlambert.github.io/learning-point-cloud-features/" rel="alternate" type="text/html" title="Understanding Multivariate Gaussians and Covariance" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/learning-point-cloud-features</id><content type="html" xml:base="http://johnwlambert.github.io/learning-point-cloud-features/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;semantic-segmentation-of-features&quot;&gt;Semantic Segmentation of Features&lt;/h2&gt;

&lt;h2 id=&quot;instance-segmentation-of-features&quot;&gt;Instance Segmentation of Features&lt;/h2&gt;

&lt;p&gt;SPGN…&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;</content><author><name></name></author><summary type="html">PointNet, SPGN, SPLATNet, Dynamic Graph Learning</summary></entry><entry><title type="html">Dimensionality Reduction</title><link href="http://johnwlambert.github.io/dimensionality-reduction/" rel="alternate" type="text/html" title="Dimensionality Reduction" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/dimensionality-reduction</id><content type="html" xml:base="http://johnwlambert.github.io/dimensionality-reduction/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nonlinear-dr-methods&quot;&gt;Nonlinear Dimensionality Reduction Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#isomap&quot;&gt;ISOMAP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lle&quot;&gt;LLE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sne&quot;&gt;SNE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#t-sne&quot;&gt;t-SNE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-dimensionality-reduction&quot;&gt;The Need for Dimensionality Reduction&lt;/h2&gt;

&lt;p&gt;When the dimensionality of data points is high, interpretation or visualization of the data can be quite challenging.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;pca&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;linear-methods&quot;&gt;Linear Methods&lt;/h3&gt;

&lt;h2 id=&quot;principal-components-analysis-pca&quot;&gt;Principal Components Analysis (PCA)&lt;/h2&gt;

&lt;p&gt;PCA is an algorithm for linear dimensionality reduction that can be surprisingly confusing. There are many different ways to explain the algorithm, and it is not new – Karl Pearson (University College, London) introduced the ideas in 1901 and Harold Hostelling (Columbia) developed the terminology and more of the math about principal components in 1933.&lt;/p&gt;

&lt;p&gt;To understand PCA, we must first consider data as points in a Euclidean space.  Suppose the lies in a high-dimensional subspace, but the real space of the data could be low-dimensional. We would like to identify the real, lower-dim subspace where lower-dim structure corresponds to linear subspaces in the high-dim space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PCA has one key hyperparameter: the dimensionality of the lower-dim space.&lt;/strong&gt; How many dimensions do we need to explain most of the variability in the data?&lt;/p&gt;

&lt;p&gt;The key idea is that we will project the points from high-D to low-D, and then when we project them back into high-D, the “reconstruction error” should be minimized.&lt;/p&gt;

&lt;p&gt;\item Pearson (1901) – find a single lower dimensional subspace that captures most of the variation in the data
\item What should be the dimensionality of the lower-dim space?&lt;/p&gt;

&lt;p&gt;Acts as a proxy for the real space&lt;/p&gt;

&lt;p&gt;Minimize errors introudced by projecting the data into this subspace&lt;/p&gt;

&lt;p&gt;Assumption: data has zero mean, ``centered data’’&lt;/p&gt;

&lt;p&gt;Move the origin to the middle of the data, the data will live in the hyperplane. Becomes subspace in new translated coordinate system&lt;/p&gt;

&lt;p&gt;As a consequence, variance becomes just the 2nd moment&lt;/p&gt;

&lt;p&gt;Data points in 2D -&amp;gt; project onto the best fit line (projection onto 1D), then assess how well the data looks now when you lay out the projected points onto the best fit line (this is the reconstruction)
Bring it back onto the best fit line (multiply by vector)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{||} = vv^Tx&lt;/script&gt;

&lt;p&gt;Minimize an error function: error equivalent to maximizing the variance of the projected data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E = \frac{1}{M} \sum\limits_I \sum\limits_M ... = &lt; \|x^{\prime \mu} - x_{||}^{\prime \mu}\|^2 &gt;_{\mu} %]]&gt;&lt;/script&gt;

&lt;p&gt;$I$ dimensions
WHY IS THIS?
Trying to find the directions of maximum variance
Knowing the covariance matrix tells us about the 2nd order moments, but not about the highest-order structure of the data (unless data is normal)&lt;/p&gt;

&lt;p&gt;If the covariance matrix is diagonal, then finding the direction of maximum variance is easy&lt;/p&gt;

&lt;p&gt;Rotate your coordinate system so that the covariance matrix becomes diagonal.&lt;/p&gt;

&lt;p&gt;This becomes an eigenvalue problem: the eigenvectors of the covariance matrix point in the directions of maximal variance?&lt;/p&gt;

&lt;p&gt;Solve all top-k possible subspaces&lt;/p&gt;

&lt;p&gt;Valid for all of them&lt;/p&gt;

&lt;p&gt;Check the error depending on many dimensions we used. A tradeoff
$\mathbf{V}^T \mathbf{V} = I$ because $\mathbf{V}$ is an orthonormal matrix
Projection $x_{||} = \mathbf{V}y = \mathbf{V}\mathbf{V}^Tx$&lt;/p&gt;

&lt;p&gt;Projection operator = $\mathbf{V}\mathbf{V}^T$&lt;/p&gt;

&lt;p&gt;But in general $P$ is of low rank and loses info&lt;/p&gt;

&lt;p&gt;Variance and reconstruction error&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
&lt;x^Tx&gt; - &lt;y_{||}^T y_{||}&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;Want to be able to maximize the variance in the projected subspace… (FIND OUT WHY THAT IS)&lt;/p&gt;

&lt;p&gt;Spectral analysis of the covariance matrix:
if covariance matrix is symmetric, it always has real eigenvalues and orthogonal eigenvectors&lt;/p&gt;

&lt;p&gt;The total variance fo the data is just the sum of the eigenvalues of the covariance matrix, all non-negative&lt;/p&gt;

&lt;p&gt;Have diagonalized the covariance matrix, aligned, which was what we set out to do&lt;/p&gt;

&lt;p&gt;This is an extremal trace problem?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_i \lambda_i \sum\limits_p (v_{ip}^{\prime})^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \mbox{tr }(V^{\prime T} \Lambda V^{\prime})&lt;/script&gt;

&lt;p&gt;To maximize the variance, need to put as mucch weight as possible on the large eigenvalues&lt;/p&gt;

&lt;p&gt;The reconstruction error is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=P+1}^I \lambda_i&lt;/script&gt;

&lt;p&gt;Aligns the axis with your data, so that for any $P$, can find $P$-dimensional subspace&lt;/p&gt;

&lt;h3 id=&quot;eigenvectors-solve-pca&quot;&gt;Eigenvectors Solve PCA&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Main Lesson from PCA&lt;/strong&gt;: For any P, the optimal $P$-dimensional subspace is the one spanned by the first $P$ eigenvectors of the covariance matrix&lt;/p&gt;

&lt;p&gt;The eigenvectors give us the frame that we need – align with it&lt;/p&gt;

&lt;h3 id=&quot;pca-examples&quot;&gt;PCA Examples&lt;/h3&gt;

&lt;p&gt;First principal component does not always have semantic meaning (combination of arts, recreation, transportation, health, and housing explain the ratings of places the most)&lt;/p&gt;

&lt;p&gt;This is simply what the analysis gives.&lt;/p&gt;

&lt;p&gt;The math is beautiful, but semantic meaning is not always clear. Sparse PCA tries to fix this (linear combinations of only a small number of variables)&lt;/p&gt;

&lt;p&gt;Geography of where an individual came from is reflected in their genetic material&lt;/p&gt;

&lt;p&gt;Machinery gives us a way to understand distortions of changes: a recipe encoded as a matrix&lt;/p&gt;

&lt;p&gt;PCA works very well here&lt;/p&gt;

&lt;p&gt;View matrices as points in high-dimensional space&lt;/p&gt;

&lt;p&gt;Recover grid structure for deformed sphere&lt;/p&gt;

&lt;p&gt;Get circular pattern of galloping horse from points&lt;/p&gt;

&lt;h3 id=&quot;svd-trick&quot;&gt;SVD Trick&lt;/h3&gt;

&lt;p&gt;What if we have fewer data points than dimensions &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
M&lt;I %]]&gt;&lt;/script&gt;? Think of images…&lt;/p&gt;

&lt;p&gt;By SVD, transpose your data&lt;/p&gt;

&lt;p&gt;Think of your data as rows, not columns&lt;/p&gt;

&lt;p&gt;Data becomes first pixel across 1000 images&lt;/p&gt;

&lt;p&gt;Covariance matrix &lt;script type=&quot;math/tex&quot;&gt;C_2&lt;/script&gt; of transposed problem&lt;/p&gt;

&lt;p&gt;Rows of transformed X (PCA’d X) are just eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;C_1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Corresponding eigenvalues are just those of &lt;script type=&quot;math/tex&quot;&gt;C_2&lt;/script&gt;, scaled by &lt;script type=&quot;math/tex&quot;&gt;I/M&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;ntroduced by Pearson (1901)
and Hotelling (1933) to
describe the variation in a set
of multivariate data in terms of
a set of uncorrelated variables.
PCA looks for a single lower
dimensional subspace that
captures most of the variation
in the data.
Specifically, we aim to
minimize the error introduced
by projecting the data into this
linear subspace.&lt;/p&gt;

&lt;p&gt;Use spectral analysis of the covariance matrix C of the
data
For any integer p, the error-minimizing p-dimensional
subspace is the one spanned by the first p eigenvectors
of the covariance matrix&lt;/p&gt;

&lt;p&gt;Eigenfaces (PCA on face images)&lt;/p&gt;

&lt;p&gt;M. Turk and A. Pentland, Face Recognition using Eigenfaces, CVPR 1991&lt;/p&gt;

&lt;p&gt;https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf&lt;/p&gt;

&lt;p&gt;http://theory.stanford.edu/~tim/s17/l/l8.pdf&lt;/p&gt;

&lt;h2 id=&quot;multi-dimensional-scaling-mds&quot;&gt;Multi-Dimensional Scaling (MDS)&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;nonlinear-dr-methods&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;non-linear-methods&quot;&gt;Non-Linear Methods&lt;/h2&gt;

&lt;p&gt;Many data sets contain essential nonlinear structures and unfortunately these structures are invisible to linear methods like PCA [1].&lt;/p&gt;

&lt;p&gt;For example, Euclidean distance between points cannot disentangle or capture the structure of manifolds like the “swiss roll.” Instead, we need geodesic distance: distance directly on the manifold.&lt;/p&gt;

&lt;p&gt;Furthermore, PCA cares about large distances. The goal is to maximize variance, and variance comes from things that are far apart. If you care about small distances, PCA is the wrong tool to use.&lt;/p&gt;

&lt;h2 id=&quot;creating-graphs-from-geometric-point-data&quot;&gt;Creating Graphs from Geometric Point Data&lt;/h2&gt;

&lt;p&gt;Rely upon neighborhood relations. A graph can be constructed via (1) k-nearest neighbors or (2) &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-balls.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;isomap&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;isometric-feature-mapping-isomap&quot;&gt;Isometric Feature Mapping (ISOMAP)&lt;/h3&gt;

&lt;p&gt;In the case of the swiss roll, Isometric Feature Mapping (ISOMAP) produces an unrolled, planar version of data that respects distances [5]. This is “isometric” unrolling.&lt;/p&gt;

&lt;p&gt;The ISOMAP algorithm involves three main steps, as outlined by Guibas [1]:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1.) Form a nearest-neighbor graph &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}&lt;/script&gt; on the original data points, weighing the edges by their original distances &lt;script type=&quot;math/tex&quot;&gt;d_x(i,j)&lt;/script&gt;. One can build the NN-graph either (A) with a fixed radius/threshold &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;, or by using a (B) fixed # of neighbors.&lt;/li&gt;
  &lt;li&gt;(2.) Estimate the geodesic distances &lt;script type=&quot;math/tex&quot;&gt;d_{\mathcal{G}}(i,j)&lt;/script&gt; between all pairs of points on the sampled manifold by computing their shortest path distances in the graph &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}&lt;/script&gt;. This can be done with classic graph algorithms for all-pairs shortest-path algorithm (APSP)s: Floyd/Warshall’s algorithm or Dijkstra’s algorithm. Initially, all pairs given distance &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;, except for neighbors, which are connected.&lt;/li&gt;
  &lt;li&gt;(3.) Construct an embedding of the data in &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional Euclidean space that best preserves the inter-point distances &lt;script type=&quot;math/tex&quot;&gt;d_{\mathcal{G}}(i,j)&lt;/script&gt;. This is performed via Multi-Dimensional Scaling (MDS).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ISOMAP actually comes with recovery guarantees that discovered structure equal to actual structure of the manifold, especially as the graph point density increases. MDS and ISOMAP both converge, but ISOMAP gets there more quickly.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;lle&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;locally-linear-embedding-lle&quot;&gt;Locally Linear Embedding (LLE)&lt;/h3&gt;

&lt;p&gt;LLE is a method that learns linear weights that locally reconstruct points in order to map points to a lower dimension [1,4]. The method requires solving two successive optimization problems and requires a connectivity graph. Almost all methods start with the nearest neihgbor graph, since it’s the only thing that we can trust.&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;first step of LLE&lt;/strong&gt;, we find weights that reconstruct each data point from its neighbors:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\underset{w}{\mbox{minimize }} &amp; \| x_i - \sum\limits_{j \in N(i)} w_{ij}x_j \|^2 \\
\mbox{subject to} &amp; \sum\limits_j w_{ij} = 1
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can use linear least squares with Lagrange multipliers to obtain optimal linear combinations.&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;second step of LLE&lt;/strong&gt;, We then &lt;strong&gt;fix these weights&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; while optimizing for &lt;script type=&quot;math/tex&quot;&gt;x_i^{\prime}&lt;/script&gt;. We try to find low-dimensional coordinates:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2
\end{array}&lt;/script&gt;

&lt;p&gt;This is a sparse eigenvalue problem that requires constraints in order to prevent degenerate solutions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1.) The coordinates &lt;script type=&quot;math/tex&quot;&gt;x_i^{\prime}&lt;/script&gt; can be translated by a constant
displacement without affecting the cost. We can remove this degree of freedom by requiring the coordinates to be centered on the origin.&lt;/li&gt;
  &lt;li&gt;(2.) We constrain the embedding vectors to have unit covariance.
The optimization problem becomes:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} &amp; \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2 \\
\mbox{subject to} &amp; \sum\limits_i x_i^{\prime} = 0 \\
&amp; \frac{1}{n} \sum\limits_i x_i^{\prime}x_i^{\prime T} = I
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;These weights &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; capture the local shape. As Roweis and Saul point out, “&lt;em&gt;LLE illustrates a general principle of manifold learning…that overlapping
local neighborhoods – collectively analyzed – can provide information about global
geometry&lt;/em&gt;”” [4].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;sne&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;stochastic-neighbor-embedding-sne&quot;&gt;Stochastic Neighbor Embedding (SNE)&lt;/h3&gt;

&lt;p&gt;The Stochastic Neighbor Embedding (SNE) converts high-dimensional points to low-dimensional points by preserving distances. The method takes a probabilistic point of view: high-dimensional Euclidean point distances are converted into conditional probabilities that represent similarities [2,3].&lt;/p&gt;

&lt;p&gt;Similarity of datapoints in &lt;strong&gt;high himension&lt;/strong&gt;: The conditional probability is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{j \mid i} = \frac{\mbox{exp }\big( - \frac{\|x_i - x_j\|^2}{2 \sigma_i^2}\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \frac{\|x_i - x_k\|^2}{2 \sigma_i^2} \big)}&lt;/script&gt;

&lt;p&gt;Similarity of datapoints in &lt;strong&gt;the low dimension&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{j \mid i} = \frac{\mbox{exp }\big( - \|y_i - y_j\|^2\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \|y_i - y_k\|^2 \big)}&lt;/script&gt;

&lt;p&gt;If similarities between &lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt; are correctly mapped to similarities between &lt;script type=&quot;math/tex&quot;&gt;y_i,y_j&lt;/script&gt; by SNE, then the conditional probabilities should be equal: &lt;script type=&quot;math/tex&quot;&gt;q_{j \mid i} = p_{j \mid i}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;SNE seeks minimize the following cost function using gradient descent, which measures the dissimilarity between the two distributions (Kullback-Leibler divergence):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \sum\limits_i KL(P_i || Q_i) = \sum\limits_i \sum\limits_j p_{j \mid i} \mbox{ log } \frac{p_{j \mid i}}{q_{j \mid i}}&lt;/script&gt;

&lt;p&gt;This is known as asymetric SNE. The gradient turns out to be analytically simple:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial C}{\partial y_i} = 2 \sum\limits_j (p_{j \mid i} - q_{j \mid i} - p_{i \mid j} - q_{i \mid j} )(y_i - y_j)&lt;/script&gt;

&lt;p&gt;However, the Kullback-Leibler divergence is not symmetric, so a formulation with a joint distribution can be made.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;t-sne&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;t-sne&quot;&gt;t-SNE&lt;/h3&gt;

&lt;p&gt;An improvement to SNE is t-Distributed Stochastic Neighbor Embedding (t-SNE). t-SNE employs a Gaussian in the high-dimension, but a t-Student distribution in low-dim. The t-Student distribution has longer tails than a Gaussian and is thus happier to have points far away than a Gaussian. The motivation for doing so is that in low-D, you have have less freedom than you would in the high-dimension to put many things closeby. This is because there is not much space around (crowded easily), so we penalize having points far away less.&lt;/p&gt;

&lt;p&gt;The joint distribution in the low-distribution is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{ij} = \frac{ (1+ \| y_i −y_j \|^2)^{−1} }{ \sum\limits_{k \neq l} (1+ \|y_k −y_l\|^2)^{−1} }&lt;/script&gt;

&lt;h3 id=&quot;mnist-examples&quot;&gt;MNIST examples&lt;/h3&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. &lt;em&gt;Multi-Dimensional Scaling, Non-Linear Dimensionality Reduction&lt;/em&gt;. Class lectures of CS233: Geometric and Topological Data Analysis, taught at Stanford University in 18 April 2018.&lt;/p&gt;

&lt;p&gt;[2] Geoffrey Hinton and Sam Roweis. &lt;em&gt;Stochastic Neighbor Embedding&lt;/em&gt;. Advances in Neural Information Processing Systems (NIPS) 2003, pages 857–864. &lt;a href=&quot;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&quot;&gt;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] L.J.P. van der Maaten and G.E. Hinton. &lt;em&gt;Visualizing High-Dimensional Data Using t-SNE&lt;/em&gt;. Journal of Machine Learning Research 9 (Nov):2579-2605, 2008.&lt;/p&gt;

&lt;p&gt;[4] Sam T. Roweis and Lawrence K. Saul. &lt;em&gt;Nonlinear Dimensionality Reduction by Locally Linear Embedding&lt;/em&gt;. Science Magazine, Vol. 290,  22 Dec. 2000.&lt;/p&gt;

&lt;p&gt;[5] J. B. Tenenbaum, V. de Silva and J. C. Langford. &lt;em&gt;A Global Geometric Framework for Nonlinear Dimensionality Reduction&lt;/em&gt;. Science 290 (5500): 2319-2323, 22 December 2000.&lt;/p&gt;

&lt;p&gt;[6] Laurenz Wiskott. &lt;em&gt;Principal Component Analysis&lt;/em&gt;. 11 March 2004. &lt;a href=&quot;https://pdfs.semanticscholar.org/d657/68e1dad46bbdb5cfb17eb19eb07cc0f5947c.pdf&quot;&gt;Online PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[7] Karl Pearson. &lt;em&gt;On Lines and Planes of Closest Fit to Systems of Points in Space&lt;/em&gt;. 1901. Philosophical Magazine. 2 (11): 559–572.&lt;/p&gt;

&lt;p&gt;[8] H Hotelling. &lt;em&gt;Analysis of a complex of statistical variables into principal components&lt;/em&gt;. 1933. Journal of Educational Psychology, 24, 417–441, and 498–520.
Hotelling, H (1936). “Relations between two sets of variates”. Biometrika. 28 (3/4): 321–377. doi:10.2307/2333955. JSTOR 2333955.&lt;/p&gt;</content><author><name></name></author><summary type="html">PCA, geodesic distances, ISOMAP, LLE, SNE, t-SNE</summary></entry><entry><title type="html">Understanding Multivariate Gaussians and Covariance</title><link href="http://johnwlambert.github.io/gauss-covariance/" rel="alternate" type="text/html" title="Understanding Multivariate Gaussians and Covariance" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/gaussians-and-covariance</id><content type="html" xml:base="http://johnwlambert.github.io/gauss-covariance/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-multivariate-gaussian-random-variable&quot;&gt;What is a multivariate Gaussian random variable?&lt;/h2&gt;

&lt;p&gt;Gaussian R.V.s are parameterized by two quantities:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X \sim p(x)  = \mathcal{N}(\mu_x, \Sigma_x)&lt;/script&gt;

&lt;p&gt;Preceded by a term for normalization&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \Sigma_x) = 
\frac{1}{\sqrt{(2\pi)^n|\Sigma_x|}}\mbox{exp}
\Bigg\{ -\frac{1}{2} (x - \mu_x)^T \Sigma_x^{-1} (x - \mu_x) \Bigg\}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the dimension, i.e. &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbbm{R}^n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And of course in the scalar case, we see&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \sigma_x) =&lt;/script&gt;

&lt;p&gt;Level sets trace out ellipses that are centered at &lt;script type=&quot;math/tex&quot;&gt;\mu_x&lt;/script&gt;, have minor and major axes (in 2D), and ellipsoids in higher dimensions&lt;/p&gt;

&lt;p&gt;Mean:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[X] = \int_x x p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_x = \int_x x \mathcal{N}(\mu_x,\Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Covariance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(X- \mu_x)(X - \mu_x)^T] = \int_x (x-\mu_x)(x-\mu_x)^T p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_x = \int_x (x- \mu_x) (x-\mu_x)^T \mathcal{N}(\mu_x, \Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Gaussian distribution is a second-order distribution, which does not mean that the higher order moments are zero (not true in general)
Convert between a standard normal, and any multivariate Gaussian&lt;/p&gt;

&lt;h2 id=&quot;the-standard-normal-distribution&quot;&gt;The Standard Normal Distribution&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_s \sim \mathcal{N}(0, I)&lt;/script&gt;

&lt;p&gt;unit (identity) covariance, so each axis decouples, compute integral over each axis separately&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to generate any Gaussian R.V. from standard normal&lt;/strong&gt; $$X_s$?$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X = \Sigma_x^{1/2}X_s + \mu_x&lt;/script&gt;

&lt;p&gt;Can obtain by scaling by covariance matrix, and by translating by mean
Very good for simulating
With MATLAB, can generate scalar, unit variance, 0 mean Gaussian R.V. with \texttt{randn}
Call \texttt{randn} $n$ times to populate $X_s$, and them multiply, then add
And we can compute via Cholesky Decomposition (unique if positive definite)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma = \Sigma_x^{1/2}(\Sigma_x^{1/2})^T&lt;/script&gt;

&lt;h2 id=&quot;matrix-square-roots&quot;&gt;Matrix Square Roots&lt;/h2&gt;

&lt;p&gt;\item There are other possible matrix square roots
\item \textbf{How to transform any Gaussian R.V. to the standard normal} $X_s$?
\item Do so via rearrangement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
X_s = \Sigma_x^{-1/2}(X - \mu_x), &amp; X \sim \mathcal{N}(\mu_x, \Sigma_x)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Much easier to integrate over the form on the RHS, not LHS
Comes from method of derived distributions
Derived Distributions: Given &lt;script type=&quot;math/tex&quot;&gt;X \sim p(x)&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Y=f(X)&lt;/script&gt;, find &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt;
Here &lt;script type=&quot;math/tex&quot;&gt;X = X_s&lt;/script&gt;, and function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the linear distribution &lt;script type=&quot;math/tex&quot;&gt;AX + b&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;properties-of-multivariate-gaussians&quot;&gt;Properties of Multivariate Gaussians&lt;/h2&gt;

&lt;h2 id=&quot;how-can-we-understand-a-covariance-matrix&quot;&gt;How can we understand a covariance matrix?&lt;/h2&gt;

&lt;p&gt;Larger covariance means more uncertainty. Isocontours/error ellipses&lt;/p&gt;

&lt;p&gt;We set &lt;script type=&quot;math/tex&quot;&gt;P=0.95&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon = \frac{1-P}{2 \pi |\Sigma|^{1/2}} = \frac{1-0.95}{2 \pi |\Sigma|^{1/2}} = \frac{0.05}{2 \pi |\Sigma|^{1/2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}(\varepsilon) = \Big\{ x \mid p(x) \geq \varepsilon \Big\}&lt;/script&gt;

&lt;h2 id=&quot;covariance&quot;&gt;Covariance&lt;/h2&gt;

&lt;p&gt;Cavariance Matrix How can we determine the direction of maximal variance? The first
we can do is to determine the variances of the individual components. If the data points (or
vectors) are written as x = (x1, x2)T (T indicates transpose), then the variances of the first
and second component can be written as C11 := “x1x1# and C22 := “x2x2# (angle brackets
indicate averaging over all data points). If C11 is large compared to C22, then the direction of
maximal variance is close to (1, 0)T , while if C11 is small, the direction of maximal variance
is close to (0, 1)T . (Notice that variance doesn’t have a polarity, so that one could use the
inverse vector (−1, 0)T instead of (1, 0)T equally well for indicating the direction of maximal
variance.)
But what if C11 is of similar value as C22, like in the example of Figure 1? Then the
co-variance between the two components, C12 := “x1x2#, can give us additional information
(notice that C21 := “x2x1# is equal to C12). A large positive value of C12 indicates a strong
correlation between x1 and x2 and that the data cloud is extended along the (1, 1)T direction.
A negative value would indicate anti-correlation and an extension along the (−1, 1)T
direction. A small value of C12 would indicate no correlation and thus little structure of
the data, i.e. no prominent direction of maximal variance. The variances and covariances
are conveniently arranged in a matrix with components Cij , which is called covariance matrix
(assuming zero mean data). Figure 3 shows several data clouds and the corresponding
covariance matrices.&lt;/p&gt;

&lt;p&gt;0.2 0
0 1&lt;/p&gt;

&lt;p&gt;1 -0.5
-0.5 0.3&lt;/p&gt;

&lt;p&gt;1 0 
0 1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import numpy as np
import pdb
import matplotlib.pyplot as plt
import seaborn as sns


import scipy

sns.set_style({'font.family': 'Times New Roman'})

def plot_gauss_ellipse(mu, cov, color='g', rad=2, ):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Plots a 2D ellipse derived from a 2D Gaussian specified by mu &amp;amp; cov.
	
	USAGE:
		hs = plotGaussEllipses( mus, Cs, [rad] )
	
	Args:
	-	mus: Numpy array of shape (2,), representing mean
	-	Cs: Numpy array of shape (2,2), representing covariance matrix
	-	color: string representing Matplotlib color
	-	rad: [2] Number of std to create the ellipse to
	
	Returns:
	-	None
	
	color choices: ['b', 'g', 'r', 'c', 'm', 'y', 'k']
	&quot;&quot;&quot;
	cRow, ccol, ra, rb, phi = gauss2ellipse( mu, cov, rad)
	plotEllipse( cRow, ccol, ra, rb, phi, color)



def gauss2ellipse(mu, C, rad=2):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Creates an ellipse representing the 2D Gaussian distribution.
	
	Creates an ellipse representing the 2D Gaussian distribution with mean mu
	and covariance matrix C.  Returns 5 parameters that specify the ellipse.
	
	USAGE
	 [cRow, cCol, ra, rb, phi] = gauss2ellipse( mu, C, [rad] )
	
	Args:
	-	mu: 1x2 vector representing the center of the ellipse
	-	C: 2x2 cov matrix
	-	rad: [2] Number of std to create the ellipse to
	
	OUTPUTS
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis length (in pixels) of the ellipse
	-	rb: semi-minor axis length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	
	EXAMPLE
	#  [cRow, cCol, ra, rb, phi] = gauss2ellipse( [5 5], [1 0; .5 2] )
	#  plotEllipse( cRow, cCol, ra, rb, phi );
	&quot;&quot;&quot;
	# error check
	if mu.size != 2 or C.shape != (2,2):
		print('Works only for 2D Gaussians')
		quit()

	# decompose using SVD
	_,D,Rh = np.linalg.svd(C)
	R = Rh.T
	normstd = np.sqrt(D)

	# get angle of rotation (in row/column format)
	phi = np.arccos(R[0,0])

	if R[1,0] &amp;lt; 0:
		phi = 2*np.pi - phi
	phi = np.pi/2 - phi

	# get ellipse radii
	ra = rad * normstd[0]
	rb = rad * normstd[1]

	# center of ellipse
	cRow = mu[0]
	cCol = mu[1]

	return cRow, cCol, ra, rb, phi



def plotEllipse(cRow,cCol,ra,rb,phi,color='b',nPnts=100,lw=1,ls='-'):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Adds an ellipse to the current plot.
	
	USAGE:
	-	h,hc,hl = plotEllipse(cRow,cCol,ra,rb,phi,[color],[nPnts],[lw],[ls])
	
	Args:
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis radius length (in pixels) of the ellipse
	-	rb: semi-minor axis radius length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	-	color: ['b'] color for ellipse
	-	nPnts: [100] number of points used to draw each ellipse
	-	lw: [1] line width
	-	ls: ['-'] line style

	Returns:
	-	h : handle to ellipse
	-	hc: handle to ellipse center
	-	hl: handle to ellipse orient

	EXAMPLE:
		plotEllipse( 3, 2, 1, 5, pi/6, 'g');
	&quot;&quot;&quot;
	# plot ellipse (rotate a scaled circle):
	ts = np.linspace(-np.pi, np.pi, nPnts+1)
	cts = np.cos(ts)
	sts = np.sin(ts)

	x = ra * cts * np.cos(-phi) + rb * sts * np.sin(-phi) + cCol
	y = rb * sts * np.cos(-phi) - ra * cts * np.sin(-phi) + cRow
	h = plt.plot(x,y, color=color, linewidth=lw, linestyle=ls)

	# plot center point and line indicating orientation
	hc = plt.plot(cCol, cRow, 'k+', color=color, linewidth=lw, linestyle=ls)

	x = [cCol, cCol+np.cos(-phi)*ra]
	y = [cRow, cRow-np.sin(-phi)*ra]
	hl = plt.plot(x, y, color=color, linewidth=lw, linestyle=ls)

	return h,hc,hl


def gen_from_distribution():



	Sigma_sqrt = scipy.linalg.sqrtm(Sigma)
	Sigma_inv = np.linalg.inv(Sigma)
	tiled_mu = np.tile(mu,(1000,1)).T
	samples = np.matmul( Sigma_sqrt, np.random.randn(2,1000) ) + tiled_mu
	exterior_samples = np.zeros((1,2))
	interior_samples = np.zeros((1,2))

	for i in range(samples.shape[1]):
			X = samples[:,i]
			f_val = 0.5 * np.matmul( np.matmul( (X - mu).T, Sigma_inv), X - mu )
			if f_val &amp;lt; -np.log(0.05):
				interior_samples = np.vstack([interior_samples, np.reshape(X,(1,2)) ])
			else:
				exterior_samples = np.vstack([exterior_samples, np.reshape(X,(1,2)) ])

	plt.scatter(interior_samples[:,0], interior_samples[:,1], c= 'b')
	plt.scatter(exterior_samples[:,0], exterior_samples[:,1], c= 'b')

def plot_gauss_ellipse_v2():
	&quot;&quot;&quot;
	&quot;&quot;&quot;
	d = 2 # dimension of samples
	p = 0.95 # probability
	num_samples = 1000

	mu = np.array(0,0) # dimension (2,)

	# define covar matrices of dim (2,2)
	cov_mats[0] = np.array([[1,0],
							[0,1]])
	cov_mats[1] = np.array([[2,0],
							[0,2]])
	cov_mats[2] = np.array([[0.25, 0.3]
							[0.3, 1]])
	cov_mats[3] = np.array([[10., 5]
							[5., 5]])

	for covar_mat in cov_mats:


		# generate and plot 1000 samples
		generate_gaussian_samples()
		plt.plot()

		# plot the error ellipse
		r = np.sqrt(ellipse_const)
		n_pts = int((2*np.pi) / 0.01)+1
		theta = np.linspace(0,2*np.pi,n_pts)
		w1 = r * np.cos(theta)
		w2 = r * np.sin(theta)
		w = np.array([w1,w2]).reshape(2,1)

		# transferred back to x coordinates
		x = scipy.linalg.sqrtm(sigma).dot(w) + mu
		plt.plot()


def unit_test1():
	&quot;&quot;&quot;
	&quot;&quot;&quot;

	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2))
	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2)*2.)
	# plt.show()

	fig()

	plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.array([[5,0],[0,3]]) )
	plt.show()




if __name__ == '__main__':
	unit_test1()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">error ellipses, uncertainty</summary></entry><entry><title type="html">Conjugate Gradients</title><link href="http://johnwlambert.github.io/conjugate-gradients/" rel="alternate" type="text/html" title="Conjugate Gradients" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/conjugate-gradients</id><content type="html" xml:base="http://johnwlambert.github.io/conjugate-gradients/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#conjugate-gradients&quot;&gt;Conjugate Gradients&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#krylov-subspaces&quot;&gt;Krylov Subspaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cayley-hamilton-thm&quot;&gt;Cayley-Hamilton Theorem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cg-algo&quot;&gt;The CG Algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#precondition&quot;&gt;Preconditioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;conjugate-gradients&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conjugate-gradients-cg&quot;&gt;Conjugate Gradients (CG)&lt;/h2&gt;

&lt;p&gt;Conjugate Gradients (CG) is a well-studied method introduced in 1952 by Hestenes and Stiefel [4] for solving a system of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; equations, &lt;script type=&quot;math/tex&quot;&gt;Ax=b&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is positive definite. Solving such a system becomes quite challenging when &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is large. CG continues to be used today, especially in state-of-the-art reinforcement learning algorithms like &lt;a href=&quot;/policy-gradients/trunc-natural-grad&quot;&gt;TRPO&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The CG method is interesting because we never give a set of numbers for &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. In fact, we never form or even store the matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. This could be desirable when &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is huge. Instead, CG is simply a method for calculating &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; times a vector [1] that relies upon a deep result, the &lt;em&gt;Cayley-Hamilton Theorem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A common example where CG proves useful is minimization of the following quadratic form, where &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbf{R}^{n \times n}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \frac{1}{2} x^TAx - b^Tx&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; is a convex function when &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is positive definite. The gradient of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla f(x) = Ax - b&lt;/script&gt;

&lt;p&gt;Setting the gradient equal to &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; allows us to find the solution to this system of equations, &lt;script type=&quot;math/tex&quot;&gt;x^{\star} = A^{-1}b&lt;/script&gt;. CG is an appropriate method to do so when inverting &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; directly is not feasible.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;krylov-subspaces&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-krylov-subspace&quot;&gt;The Krylov Subspace&lt;/h2&gt;

&lt;p&gt;CG relies upon an idea named the &lt;em&gt;Krylov subspace&lt;/em&gt;. The Krylov subspace is defined as the span of the vectors generated from successively higher powers of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{K}_k = \mbox{span} \{b, Ab, \dots, A^{k-1}b \}&lt;/script&gt;

&lt;p&gt;The Krylov sequence is a sequence of solutions to our convex objective &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;. Each successive solution has to come from a subspace &lt;script type=&quot;math/tex&quot;&gt;\mathcal{K}_k&lt;/script&gt; of progressively higher power &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. Formally, the Krylov sequence &lt;script type=&quot;math/tex&quot;&gt;x^{(1)},x^{(2)},\dots&lt;/script&gt; is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(k)} = \underset{x \in \mathcal{K}_k}{\mbox{argmin }} f(x)&lt;/script&gt;

&lt;p&gt;The CG algorithm generates the Krylov sequence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Properties of the Krylov Sequence&lt;/strong&gt;
It should be clear that &lt;script type=&quot;math/tex&quot;&gt;x^{(k)}=p_k(A)b&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p_k&lt;/script&gt; is a polynomial with degree &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p_k &lt; k %]]&gt;&lt;/script&gt;, because &lt;script type=&quot;math/tex&quot;&gt;x^{(k)} \in \mathcal{K}_k&lt;/script&gt;. Surprisingly enough, the Krylov sequence is a two-term recurrence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(k+1)} = x^{(k)} + \alpha_k r^{(k)} + \beta_k (x^{(k)} - x^{(k+1)})&lt;/script&gt;

&lt;p&gt;for some values &lt;script type=&quot;math/tex&quot;&gt;\alpha_k, \beta_k&lt;/script&gt;.This means the current iterate is a linear combination of the previous two iterates.  This is the basis of the CG algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;cayley-hamilton-thm&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cayley-hamilton-theorem&quot;&gt;Cayley-Hamilton Theorem&lt;/h2&gt;

&lt;p&gt;The reason the Krylov subspace is helpful is because &lt;script type=&quot;math/tex&quot;&gt;x^{\star} = A^{-1}b \in \mathcal{K}_n&lt;/script&gt;. I will show why. The Cayley-Hamilton Theorem states that if &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbf{R}^{n \times n}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^n + \alpha_1 A^{n-1} + \cdots + \alpha_n I = 0&lt;/script&gt;

&lt;p&gt;We can solve for &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; by rearranging terms:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_n I = - A^n + - \alpha_1 A^{n-1} + \cdots + \alpha_{n-1} A^{1}&lt;/script&gt;

&lt;p&gt;We now divide by &lt;script type=&quot;math/tex&quot;&gt;\alpha_n&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I = - \frac{1}{\alpha_n} A^n + -  \frac{\alpha_1}{\alpha_n} A^{n-1} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} A^{1}&lt;/script&gt;

&lt;p&gt;We now left-multiply all terms by &lt;script type=&quot;math/tex&quot;&gt;A^{-1}&lt;/script&gt; and simplify:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A^{-1}I &amp; = - \frac{1}{\alpha_n} A^{-1} A^n + -  \frac{\alpha_1}{\alpha_n} A^{-1} A^{n-1} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} A^{-1} A^{1} \\
A^{-1} &amp; = - \frac{1}{\alpha_n} A^{n-1} + -  \frac{\alpha_1}{\alpha_n}  A^{n-2} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} I \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now see that &lt;script type=&quot;math/tex&quot;&gt;x^{\star}&lt;/script&gt; is a linear combination of the vectors that span the Krylov subspace. Thus, &lt;script type=&quot;math/tex&quot;&gt;x^{\star} = A^{-1}b \in \mathcal{K}_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;cg-algo&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-cg-algorithm&quot;&gt;The CG Algorithm&lt;/h2&gt;

&lt;p&gt;We will maintain the square of the residual &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; at each step, which we call &lt;script type=&quot;math/tex&quot;&gt;r_k&lt;/script&gt;. If the square root of your residual is small enough, &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\rho_{k-1}}&lt;/script&gt;, then you can quit. Your search direction is &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;: the combination of your current r esidual and the previous search direction [2,3].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(x:=0\),  \(r:=b\),  \( \rho_0 := | r |^2 \)&lt;/li&gt;
  &lt;li&gt;for &lt;script type=&quot;math/tex&quot;&gt;k=1,\dots,N_{max}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;quit if &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\rho_{k-1}} \leq \epsilon \|b\|&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;if \(k=1\)
        &lt;ul&gt;
          &lt;li&gt;\( p:= r \)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;else
        &lt;ul&gt;
          &lt;li&gt;\( p:= r + \frac{\rho_{k-1}}{\rho_{k-2}} \)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;\( w:=Ap \)&lt;/li&gt;
      &lt;li&gt;\( \alpha := \frac{ \rho_{k-1} }{ p^Tw } \)&lt;/li&gt;
      &lt;li&gt;\( x := x + \alpha p \)&lt;/li&gt;
      &lt;li&gt;\( r := r - \alpha w \)&lt;/li&gt;
      &lt;li&gt;\( \rho_k := |r|^2 \)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Along the way, we’ve created the Krylov sequence &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. Operations like &lt;script type=&quot;math/tex&quot;&gt;x + \alpha p&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;r - \alpha w&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(n)&lt;/script&gt; (BLAS level-1).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;precondition&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;preconditioning&quot;&gt;Preconditioning&lt;/h2&gt;

&lt;p&gt;It turns out that CG will often just fail. The trick in CG is to change coordinates first (precondition) and then run CG on the system in the changed coordinates. This is because of round-off errors that accumulate, leading to unstability and divergence. For example, we may want to make the spectrum of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; clustered.&lt;/p&gt;

&lt;p&gt;A generic preconditioner is a diagonal matrix, e.g.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = \mbox{diag}(\frac{1}{A_{11}}, \dots, \frac{1}{A_{nn}})&lt;/script&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stephen Boyd. &lt;em&gt;Conjugate Gradient Method&lt;/em&gt;. EE364b Lectures Slides, Stanford University. &lt;a href=&quot;https://web.stanford.edu/class/ee364b/lectures/conj_grad_slides.pdf&quot;&gt;https://web.stanford.edu/class/ee364b/lectures/conj_grad_slides.pdf&lt;/a&gt;. &lt;a href=&quot;https://www.youtube.com/watch?v=E4gl91l0l40&quot;&gt;Video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] C.T. Kelley. &lt;em&gt;Iterative Methods for Optimization&lt;/em&gt;. SIAM, 2000.&lt;/p&gt;

&lt;p&gt;[3] C. Kelley. &lt;em&gt;Iterative Methods for Linear and Nonlinear Equations&lt;/em&gt;. Frontiers in Applied Mathematics SIAM, (1995). &lt;a href=&quot;https://archive.siam.org/books/textbooks/fr16_book.pdf&quot;&gt;https://archive.siam.org/books/textbooks/fr16_book.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Magnus Hestenes and Eduard Stiefel. &lt;em&gt;Method of Conjugate Gradients for Solving Linear Systems&lt;/em&gt;. Journal of Research of the National Bureau of Standards, Vol. 49, No. 6, December 1952. &lt;a href=&quot;https://web.njit.edu/~jiang/math614/hestenes-stiefel.pdf&quot;&gt;https://web.njit.edu/~jiang/math614/hestenes-stiefel.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">large systems of equations, Krylov subspaces, Cayley-Hamilton Theorem</summary></entry><entry><title type="html">Simultaneous Localization and Mapping (SLAM)</title><link href="http://johnwlambert.github.io/slam/" rel="alternate" type="text/html" title="Simultaneous Localization and Mapping (SLAM)" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/slam</id><content type="html" xml:base="http://johnwlambert.github.io/slam/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;slam&quot;&gt;SLAM&lt;/h2&gt;

&lt;h2 id=&quot;orbslam&quot;&gt;ORBSlam&lt;/h2&gt;

&lt;h2 id=&quot;graphslam&quot;&gt;GraphSLAM&lt;/h2&gt;

&lt;p&gt;SLAM methods [2, 10, 13, 14, 21, 30]&lt;/p&gt;

&lt;p&gt;M. Bosse, P. Newman, J. Leonard, M. Soika, W. Feiten, and S. Teller. Simultaneous localization and map building in large-scale cyclic envi- ronments using the atlas framework. IJRR, 23(12), 2004.&lt;/p&gt;

&lt;p&gt;T. Duckett, S. Marsland, and J. Shapiro. Learning globally consistent
maps by relaxation. ICRA 2000.&lt;/p&gt;

&lt;p&gt;J. Folkesson and H. I. Christensen. Robust SLAM. ISAV 2004.
[14] U. Frese, P. Larsson, and T. Duckett. A multigrid algorithm for
simultaneous localization and mapping. IEEE Transactions on Robotics,
2005.&lt;/p&gt;

&lt;p&gt;K. Konolige. Large-scale map-making. AAAI, 2004.&lt;/p&gt;

&lt;p&gt;S. Thrun and M. Montemerlo. The GraphSLAM algorithm with
applications to large-scale mapping of urban structures. IJRR, 25(5/6),
2005.&lt;/p&gt;

&lt;p&gt;. Folkesson and H. I. Christensen. Robust SLAM. ISAV 2004.&lt;/p&gt;</content><author><name></name></author><summary type="html">GraphSLAM, loop closures</summary></entry><entry><title type="html">Structure From Motion</title><link href="http://johnwlambert.github.io/sfm/" rel="alternate" type="text/html" title="Structure From Motion" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/sfm</id><content type="html" xml:base="http://johnwlambert.github.io/sfm/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-basic-structure-from-motion-sfm-pipeline&quot;&gt;A Basic Structure-from-Motion (SFM) Pipeline&lt;/h2&gt;

&lt;p&gt;As described in [1], there are generally four steps to the algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) Find interest points in each image&lt;/li&gt;
  &lt;li&gt;(2) Find candidate correspondences (match descriptors for each interest point)&lt;/li&gt;
  &lt;li&gt;(3) Perform geometric verification of correspondences (RANSAC + fundamental matrix)&lt;/li&gt;
  &lt;li&gt;(4) Solve for 3D points and camera that minimize reprojection error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;costfunctions&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cost-functionserror-modelling&quot;&gt;Cost Functions/Error Modelling&lt;/h2&gt;

&lt;p&gt;The choice of cost function quantifies the total prediction error of the model. It measures how well the model fits the observations and background knowledge.&lt;/p&gt;

&lt;h3 id=&quot;reprojection-error-for-a-single-keypoint-in-two-images&quot;&gt;Reprojection Error for a Single Keypoint in Two Images&lt;/h3&gt;

&lt;p&gt;Imagine we have matched two keypoints, &lt;script type=&quot;math/tex&quot;&gt;x_1,x_2&lt;/script&gt; in two different images &lt;script type=&quot;math/tex&quot;&gt;I_1,I_2&lt;/script&gt; via a SIFT-like feature matching pipeline. We are viewing the projection of the same 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; in both images. We now wish to identify the coordinates describing the location of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If image &lt;script type=&quot;math/tex&quot;&gt;I_1&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt;, and image &lt;script type=&quot;math/tex&quot;&gt;I_2&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt;, we can enforce that the 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is projected into the image into the right location in both images. This is called &lt;em&gt;triangulation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If the camera calibration determining &lt;script type=&quot;math/tex&quot;&gt;M_1,M_2&lt;/script&gt; are &lt;strong&gt;known&lt;/strong&gt;, then the optimization problem for a single matched keypoint in two images becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}} f(\mathbf{X}) = \| x_1 - Proj(\mathbf{X},M_1)\|^2 + \| x_2 - Proj(\mathbf{X},M_2)\|^2&lt;/script&gt;

&lt;h3 id=&quot;reprojection-error-for-many-keypoints-in-many-cameras&quot;&gt;Reprojection Error for Many Keypoints in Many Cameras&lt;/h3&gt;

&lt;p&gt;Imagine now that we have &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; different cameras, and each camera has a &lt;strong&gt;known&lt;/strong&gt; projection matrix &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt;. Suppose we match &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; different keypoints across the images, denoted &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{X}_j\}_{j=1}^n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The optimization problem becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;In this case &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is the observed location of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;‘th keypoint into the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th image, as discovered by a keypoint detector in the SIFT-like pipeline. We penalize solutions for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_1,\mathbf{X}_2,\dots&lt;/script&gt; in which &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is not very close to &lt;script type=&quot;math/tex&quot;&gt;Proj(\mathbf{X_j},M_i)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;bundleadjustment&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bundle-adjustment&quot;&gt;Bundle Adjustment&lt;/h2&gt;

&lt;p&gt;Imagine now that we are working with arbitrary images for which we have no calibration information. Thus, the projection matrices &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt; are &lt;strong&gt;unknown&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Bundle adjustment is the process of minimizing reprojection error over (1) multiple 3D points and (2) multiple cameras. Triggs &lt;em&gt;et al.&lt;/em&gt; define it as &lt;em&gt;“the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates”&lt;/em&gt; [2]. The optimization problem changes only by adding new, additional variables for which we solve.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots, M_1,M_2,\dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;According to [2], the name “Bundle Adjustment” refers to &lt;em&gt;bundles&lt;/em&gt; of light rays leaving each 3D point and converging on each camera center, &lt;em&gt;“which are ‘adjusted’ optimally with respect to both feature and camera positions”&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;gauss-newton&quot;&gt;Gauss-Newton&lt;/h2&gt;

&lt;p&gt;Gauss-Newton is generally preferred to Second Order methods like Newton’s Method for a simple reason: deriving and implementing calculations of the second derivatives of the projection model &lt;script type=&quot;math/tex&quot;&gt;Proj(X_j,M_i)&lt;/script&gt; is difficult and error-prone [2].&lt;/p&gt;

&lt;p&gt;For example, the seminal work in SfM, “Building Rome in a Day” [3] uses Trust-Region Gauss-Newton optimization (Levenberg-Marquardt) that chooses between a truncated and an exact step Levenberg-Marquardt algorithm.&lt;/p&gt;

&lt;!-- ## Network Graph  shows which features are seen in which images, --&gt;

&lt;h2 id=&quot;exploiting-sparsity&quot;&gt;Exploiting Sparsity&lt;/h2&gt;

&lt;p&gt;Those who use generic optimization routines to solve SfM problems will find the optimization slow. This would be unwise, however, since the problem sparsity can be exploited.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Ramanan, Deva. &lt;em&gt;Structure from Motion.&lt;/em&gt; &lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&quot;&gt;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Bill Triggs, Philip McLauchlan, Richard Hartley and Andrew Fitzgibbon. &lt;em&gt;Bundle Adjustment — A Modern Synthesis&lt;/em&gt;. &lt;a href=&quot;https://hal.inria.fr/inria-00548290/document&quot;&gt;https://hal.inria.fr/inria-00548290/document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, Richard Szeliski. &lt;em&gt;Building Rome in a Day&lt;/em&gt;. Communications of the ACM,Volume 54 Issue 10, October 2011. Pages 105-112. &lt;a href=&quot;https://grail.cs.washington.edu/rome/rome_paper.pdf&quot;&gt;https://grail.cs.washington.edu/rome/rome_paper.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Deriving bundle adjustment</summary></entry></feed>