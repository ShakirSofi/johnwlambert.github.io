<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://johnwlambert.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://johnwlambert.github.io/" rel="alternate" type="text/html" /><updated>2019-11-26T17:24:59-05:00</updated><id>http://johnwlambert.github.io/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">PyTorch Tutorial</title><link href="http://johnwlambert.github.io/pytorch-tutorial/" rel="alternate" type="text/html" title="PyTorch Tutorial" /><published>2019-09-13T07:00:00-04:00</published><updated>2019-09-13T07:00:00-04:00</updated><id>http://johnwlambert.github.io/pytorch-tutorial</id><content type="html" xml:base="http://johnwlambert.github.io/pytorch-tutorial/">&lt;p&gt;This tutorial was contributed by John Lambert.&lt;/p&gt;

&lt;p&gt;This tutorial will serve as a crash course for those of you not familiar with PyTorch. It is written in the spirit of &lt;a href=&quot;http://cs231n.github.io/python-numpy-tutorial/&quot;&gt;this Python/Numpy tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will be focusing on CPU functionality in PyTorch, not GPU functionality, in this tutorial. We’ll be working with PyTorch 1.1.0, in these examples.&lt;/p&gt;

&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#tensors&quot;&gt;PyTorch Tensors&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#create-tensor&quot;&gt;Creating a tensor&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-types&quot;&gt;Data types in Pytorch and Casting&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tensor-ops&quot;&gt;Operations on Tensors&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#indexing&quot;&gt;Tensor Indexing &lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reshaping&quot;&gt;Reshaping tensors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#arithmetic&quot;&gt;Tensor Arithmetic&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mat-mul&quot;&gt;Matrix Multiplication vs. Elementwise Multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#transcendental&quot;&gt;Other helpful transcendental functions &lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#combining-tensors&quot;&gt;Combining Tensors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#logical-ops&quot;&gt;Logical Operations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sorting-ops&quot;&gt;Sorting Operations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conv-layers&quot;&gt;Conv Layers&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#conv-weights&quot;&gt;Weights for Convolutional layers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conv-groups&quot;&gt;Groups in Conv Layers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conv-bias&quot;&gt;Bias in Convolutional Layers&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#maxpool&quot;&gt;Max-Pooling layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#creating-model&quot;&gt;Creating a Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#custommodule&quot;&gt;Creating a Pytorch Module, Weight Initialization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#forward&quot;&gt;Executing a forward pass through the model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#iterate-modules&quot;&gt;Instantiate Models and iterating over their modules&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sequential&quot;&gt;Sequential Networks&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;tensors&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;pytorch-tensors&quot;&gt;PyTorch Tensors&lt;/h2&gt;
&lt;p&gt;PyTorch’s fundamental data structure is the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;, an n-dimensional array. You may be more familiar with matrices, which are 2-dimensional tensors, or vectors, which are 1-dimensional tensors.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;create-tensor&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;creating-a-tensor&quot;&gt;Creating a tensor&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([1., 2., 3.])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([3])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[1.],[1.]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([0., 0., 0.])&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Alternatively, create a tensor by bringing it in from Numpy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;[0 1 2 3]&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([0, 1, 2, 3])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;data-types&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;data-types-in-pytorch-and-casting&quot;&gt;Data types in Pytorch and Casting&lt;/h3&gt;
&lt;p&gt;You’ll have a wide range of data types at your disposal, including:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Type Name&lt;/th&gt;
      &lt;th&gt;Code keywords (all equivalent)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;32-bit floating point&lt;/td&gt;
      &lt;td&gt;torch.float32, torch.float, torch.FloatTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64-bit floating point&lt;/td&gt;
      &lt;td&gt;torch.float64, torch.double, torch.DoubleTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8-bit integer (unsigned)&lt;/td&gt;
      &lt;td&gt;torch.uint8, torch.ByteTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;torch.int8, torch.CharTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;32-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;torch.int32, torch.int, torch.IntTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64-bit integer (signed)&lt;/td&gt;
      &lt;td&gt;torch.int64, torch.long, torch.LongTensor&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Boolean&lt;/td&gt;
      &lt;td&gt;torch.bool, torch.BoolTensor&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A tensor can be cast to any data type, with possible loss of precision:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.int64&quot;, currently 64-bit integer type&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.float32&quot;, now 32-bit float&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Still &quot;torch.float32&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DoubleTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([0., 1., 2., 3.], dtype=torch.float64)&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LongTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Cast back to int-64, prints &quot;tensor([0, 1, 2, 3])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;tensor-ops&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;indexing&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;tensor-indexing&quot;&gt;Tensor Indexing&lt;/h3&gt;
&lt;p&gt;Tensors can be indexed using MATLAB/Numpy-style n-dimensional array indexing. An RGB image is a 3-dimensional array. For a 2 pixel by 2 pixel RGB image, in CHW order, the image tensor would have dimensions (3,2,2). In HWC order, the image tensor would have dimensions (2,2,3). In NCHW order, the image tensor would have shape (1,3,2,2). &lt;em&gt;N&lt;/em&gt; represents the batch dimension (number of images present), &lt;em&gt;C&lt;/em&gt; represents the number of channels, and &lt;em&gt;H,W&lt;/em&gt; represent height and width.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Create an array of numbers [0,1,2,...,11]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Reshape tensor from (12,) to (3,2,2)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints 0th channel image, &quot;tensor([[0, 1], [2, 3]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints 1st channel image, &quot;tensor([[4, 5], [6, 7]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints 2nd channel image, &quot;tensor([[8, 9],[10, 11]])&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Index instead to get ALL channels at (0,0) pixel for 0th row, 0th col.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([0, 4, 8])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;reshaping&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;reshaping-tensors&quot;&gt;Reshaping tensors&lt;/h3&gt;
&lt;p&gt;Above, we used &lt;code class=&quot;highlighter-rouge&quot;&gt;reshape()&lt;/code&gt; to modify the shape of a tensor. Note that a reshape is valid only if we do not change the total number of elements in the tensor. For example, a (12,1)-shaped tensor can be reshaped to (3,2,2) since &lt;script type=&quot;math/tex&quot;&gt;12*1=3*2*2&lt;/script&gt;. Here are a few other useful tensor-shaping operations:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([3, 2, 2])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Add batch dimension for NCHW, prints &quot;torch.Size([1, 3, 2, 2])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([6, 2])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([6, 2])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Reshape back to flat vector, prints &quot;torch.Size([12])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a name=&quot;arithmetic&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;tensor-arithmetic&quot;&gt;Tensor Arithmetic&lt;/h3&gt;
&lt;p&gt;Typical Python or Numpy operators such as &lt;em&gt;+,-&lt;/em&gt; can be used for arithmetic, or explicit PyTorch operators:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([3, 4, 5])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#  Above is identical to using &quot;+&quot; op, prints &quot;tensor([3, 4, 5])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([3, 4, 5])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([-1,  0,  1])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([-1,  0,  1])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;mat-mul&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;matrix-multiplication-vs-elementwise-multiplication&quot;&gt;Matrix Multiplication vs. Elementwise Multiplication&lt;/h2&gt;
&lt;p&gt;Note that the operator &lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt; will not perform matrix multiplication – rather, it will perform elementwise multiplication, such as in Numpy:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([1, 3])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Take matrix transpose, prints torch.Size([3, 1])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([1, 3])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Elementwise multiplication of arrays, prints &quot;tensor([[2, 4, 6]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Outer product of (3,1) and (1,3), prints &quot;torch.Size([3, 3])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Dot product of (1,3) and (3,1), prints &quot;tensor([[12]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Same dot product/inner product, prints &quot;tensor([[12]])&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Identical to above, prints &quot;tensor([[12]])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;transcendental&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;other-helpful-transcendental-functions&quot;&gt;Other helpful transcendental functions:&lt;/h3&gt;
&lt;p&gt;PyTorch supports cosine, sine, and exponential operations with &lt;code class=&quot;highlighter-rouge&quot;&gt;cos()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sin()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;exp()&lt;/code&gt;, just like Numpy. The function input must be a PyTorch tensor:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints float as &quot;3.141592653589793&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# print(torch.cos(np.pi)) # Will crash with TypeError, since np.pi is a float, not tensor&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor(-1.)&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# print(torch.cos(torch.tensor(0))) # Will crash, Prints &quot;RuntimeError: cos_vml_cpu not implemented for 'Long'&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Must use float as argument, prints &quot;tensor(1.)&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints Euler's number e as &quot;tensor(2.7183)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;combining-tensors&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;combining-tensors&quot;&gt;Combining Tensors&lt;/h3&gt;
&lt;p&gt;Tensors can be combined along any dimension, as long as the dimensions align properly. &lt;em&gt;Concatenating&lt;/em&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cat()&lt;/code&gt;) or &lt;em&gt;stacking&lt;/em&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.stack()&lt;/code&gt;) tensors are considered different operations in PyTorch. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.stack()&lt;/code&gt; will combine a sequence of tensors along a new dimension, whereas &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cat()&lt;/code&gt; will concatenates tensors along a default dimension &lt;em&gt;dim=0&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints torch.Size([1, 3]), x is a row vector&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints torch.Size([1, 3]), y is a row vector&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# prints torch.Size([2, 1, 3])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#  prints &quot;torch.Size([2, 3])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;logical-ops&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;logical-operations&quot;&gt;Logical Operations&lt;/h3&gt;
&lt;p&gt;Logical operations like AND, OR, etc. can be computed on PyTorch tensors:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Create uint8/Byte tensor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Create uint8/Byte tensor&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.uint8 torch.uint8&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Logical and op., prints &quot;tensor([1, 0, 0], dtype=torch.uint8)&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Logical or op., prints &quot;tensor([1, 1, 1], dtype=torch.uint8)&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cond&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Create a condition with logical `AND`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Condition valid only at last index, prints &quot;tensor([0, 0, 1], dtype=torch.uint8)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;more-logical-operations&quot;&gt;More logical operations.&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.where(condition, input, other)&lt;/code&gt; looks at each index &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of 3 input tensors. It will return &lt;script type=&quot;math/tex&quot;&gt;input_i&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;condition_i&lt;/script&gt; true, else will return &lt;script type=&quot;math/tex&quot;&gt;other_i&lt;/script&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Only at 1st index is element &amp;gt; 0 and &amp;lt; 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([6, 1, 4])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Logical operations can be combined with &lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.nonzero()&lt;/code&gt; to retrieve relevant
array indices. &lt;code class=&quot;highlighter-rouge&quot;&gt;.nonzero()&lt;/code&gt; will return the indices of all non-zero elements of the input. It can be used much like &lt;code class=&quot;highlighter-rouge&quot;&gt;np.argwhere()&lt;/code&gt;, in the following manner:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# At index i=2 and i=3, x[i]=3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nonzero&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[2],[3]])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;sorting-ops&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sorting-operations&quot;&gt;Sorting Operations&lt;/h3&gt;
&lt;p&gt;Indices to sort an array can be computed:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([1, 2, 3, 0])&quot;, representing indices to sort x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;conv-layers&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;conv-weights&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;weights-for-convolutional-layers&quot;&gt;Weights for Convolutional layers&lt;/h3&gt;
&lt;p&gt;PyTorch convolutional layers require 4-dimensional inputs, in &lt;em&gt;NCHW&lt;/em&gt; order. As mentioned above, &lt;em&gt;N&lt;/em&gt; represents the batch dimension, &lt;em&gt;C&lt;/em&gt; represents the channel dimension, &lt;em&gt;H&lt;/em&gt; represents the image height (number of rows), and &lt;em&gt;W&lt;/em&gt; represents the image width (number of columns).&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Represents 3-channel image, each image has dims (2,2)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Convert to float32 since Conv2d cannot accept `Long` type&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[0, 1],[2, 3]]])&quot; as channel 0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[4, 5],[6, 7]]])&quot; as channel 1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[8, 9],[10, 11]]])&quot;&quot; as channel 2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;conv_1group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_1group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 3 filters, each of size (3 x 1 x 1), prints &quot;torch.Size([3, 3, 1, 1])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_1group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;None&quot;, since no bias here&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 0th filter, prints &quot;tensor([1., 1., 1.])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 1st filter, prints &quot;tensor([2., 2., 2.])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 2nd filter, prints &quot;tensor([3., 3., 3.])&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_1group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Initialize the layer weight&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Perform 1x1 convolution, i.e. the dot product&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  of [1,1,1] w/ [1,1,1] = 3, and [1,1,1] w/ [2,2,2] = 6, etc.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_1group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[[3.,3.],[3.,3.]],[[6.,6.],[6.,6.]],[[9.,9.],[9.,9.]]]],grad_fn=&amp;lt;MkldnnConvolutionBackward&amp;gt;)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;conv-groups&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;groups-in-conv-layers&quot;&gt;Groups in Conv Layers&lt;/h3&gt;
&lt;p&gt;Convolutional filters can be applied along a single channel, instead of over all channels, when groups is set to the number of channels.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d&quot;&gt;official documentation states&lt;/a&gt; that when &lt;em&gt;groups= in_channels, each input channel is convolved with its own set of filters&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 3 filters, each of size (1 x 1 x 1) now, prints &quot;torch.Size([3, 1, 1, 1])&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Create new weight, scalar 1 is for channel 0, etc.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Initialize CONV layer weight w/ Parameter&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;Parameter containing: tensor([[[[1.]]],[[[2.]]],[[[3.]]]], requires_grad=True)&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Input split into 3 groups, and conv layer split into 3 groups (along channel dim.)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Each pixel in channel 0 multiplied with 1, each pixel in channel 1 multiplied with 2, etc.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# prints &quot;tensor([[[[1.,1.],[1.,1.]],[[1.,1.],[1.,1.]],[[1.,1.],[1.,1.]]]])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([1, 3, 2, 2])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[[1.,1.],[1.,1.]],[[2.,2.],[2.,2.]],[[3.,3.],[3.,3.]]]], grad_fn=&amp;lt;MkldnnConvolutionBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;conv-bias&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;bias-in-convolutional-layers&quot;&gt;Bias in Convolutional Layers&lt;/h3&gt;
&lt;p&gt;Above, we set the bias to false in our conv layers. A bias will add an offset to the dot product result – below is an example when &lt;em&gt;bias=5&lt;/em&gt; for each of 3 filters. Bias is initialized randomly when the layer is constructed with &lt;code class=&quot;highlighter-rouge&quot;&gt;bias=True&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;Parameter containing:tensor([0.0452, 0.9189, 0.7354], requires_grad=True)&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# One bias term to add for each of 3 filters, prints &quot;torch.Size([3])&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Conv weight w as above&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Fill b w/ 5's -- will add 5 to result of each 1x1 convolution&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_3groups&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[[6.,6.],[6.,6.]],[[7.,7.],[7.,7.]],[[8.,8.],[8.,8.]]]],grad_fn=&amp;lt;MkldnnConvolutionBackward&amp;gt;)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;maxpool&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;max-pooling-layers&quot;&gt;Max-Pooling layers&lt;/h3&gt;
&lt;p&gt;A 2d max-pooling layer will slide a small window over the 2d feature map slices (each channel viewed independently) and will output the largest value in the window:&lt;/p&gt;

&lt;p&gt;For example, a max-pooling layer with kernel_size=2 will slide a 2x2 window over the 2d feature maps. With stride=2, this window will be shifted over by 2 pixels along any axis before the subsequent computation. With stride=1, this window will be placed at every possible position (shifted over by 1 pixel at a time). With kernel size &lt;script type=&quot;math/tex&quot;&gt;&gt;1&lt;/script&gt;, in order to preserve the input size, one must pad the input using the &lt;code class=&quot;highlighter-rouge&quot;&gt;padding&lt;/code&gt; argument to the convolution op.&lt;/p&gt;

&lt;p&gt;For a 4x4 image, a max-pooling kernel of &lt;script type=&quot;math/tex&quot;&gt;2x2&lt;/script&gt; and stride 2 will output the max in each 2x2 quadrant of the image:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 1-channel, 4x4 image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Slide `kernel` every 2 pixels&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#  prints &quot;tensor([[[[2., 4.],[2., 4.]]]])&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Step one pixel at a time, taking max in 2x2 cell&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Get back 3x3 image since no padding, prints &quot;torch.Size([1, 1, 3, 3])&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Now prints &quot;tensor([[[[2., 3., 4.],[2., 3., 4.],[2., 3., 4.]]]])&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Try padding&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Even kernel w/ padding kernel_size//2 will make size increase by 1,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;torch.Size([1, 1, 5, 5])&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;creating-model&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;custommodule&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;creating-a-pytorch-module-weight-initialization&quot;&gt;Creating a Pytorch Module, Weight Initialization&lt;/h3&gt;
&lt;p&gt;To define a custom layer, you’ll define a class that inherits from &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.Module&lt;/code&gt;. The class will require a constructor, which should be implemented with &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__()&lt;/code&gt; in Python.&lt;/p&gt;

&lt;p&gt;Consider a simple layer that applies a single convolutional filter to a 3-channel input. For &lt;em&gt;kernel_size=2&lt;/em&gt;, a filter (a cube of shape 3x2x2) will be slided over the input at default &lt;em&gt;stride=1&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Conv layer weights are randomly initialized by default, but can be explicitly specified in a number of ways. In order to initialize all weight values to a constant value, or to draw them from a specific type of distribution, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.init()&lt;/code&gt; may be used.&lt;/p&gt;

&lt;p&gt;To initialize weight values to a &lt;strong&gt;specific&lt;/strong&gt; tensor, the tensor must be wrapped inside a PyTorch &lt;code class=&quot;highlighter-rouge&quot;&gt;Parameter&lt;/code&gt;, meaning &lt;em&gt;a kind of Tensor that is to be considered a module parameter&lt;/em&gt; (a special subclass of Tensor that will make the tensor appear in the module’s &lt;code class=&quot;highlighter-rouge&quot;&gt;.parameters()&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyNewModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyNewModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# Starts with random weights&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#  prints &quot;torch.Size([1, 3, 2, 2])&quot;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints tensor([[[[ 0.2,0.2],[ 0.0,0.1]],...,[[0.0,-0.1],[-0.0,0.2]]]], requires_grad=True)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Fill weight with all ones&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;Parameter containing:tensor([[[[1.,1.],[1.,1.]],...[[1.,1.],[1.,1.]]]], requires_grad=True)&quot;&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Insert entirely new parameter&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;Parameter containing:tensor([[[[ 0.,1.],[ 2.,3.]],...[10.,11.]]]], requires_grad=True)&quot;&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Define the behavior for the &quot;forward&quot; pass&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;forward&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;executing-a-forward-pass-through-the-model&quot;&gt;Executing a forward pass through the model&lt;/h3&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;forward()&lt;/code&gt; function of a model can be executed on &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; as follows. With an input of all ones with shape 1x3x2x2, and a filter representing a 3x2x2 cube with numbers [0,1,2,3,…,10,11], the filter can only be applied in a single location, computing a single dot product of 
&lt;script type=&quot;math/tex&quot;&gt;[1,1,1,1,...,1,1,1] \cdot [0,1,2,3,...,9,10,11] = 66&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyNewModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Fill input with all ones&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints tensor([[[[66.]]]], grad_fn=&amp;lt;MkldnnConvolutionBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;iterate-modules&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;instantiate-models-and-iterating-over-their-modules&quot;&gt;Instantiate Models and iterating over their modules&lt;/h3&gt;
&lt;p&gt;The modules and parameters of a model can be inspected by iterating over the relevant iterators, which may be useful for debugging:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;MyNewModule( (conv): Conv2d(3, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)), ...&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;named_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Iterate over parameters&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;conv.weight Parameter containing: tensor([[[[ 0.,  1.],[ 2.,  3.]],...[10., 11.]]]], requires_grad=True)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;sequential&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sequential-networks&quot;&gt;Sequential Networks&lt;/h3&gt;
&lt;p&gt;A number of different operations can be stacked into a single, sequential network with &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Sequential()&lt;/code&gt;. In &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Sequential&lt;/code&gt;, the &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;’s stored inside are connected in a cascaded way. For example, to define a network that applied a convolution and then a max-pooling operation, we could pass these layers to &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Sequential()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For a single-channel input, with 1x1 convolution with filter weights all equal to 2, the operation will double every pixel’s value. In this case, the dot product is over a 1-dimensional input, so the dot product involves only multiplication, not sum. After subsequent max-pooling of &lt;em&gt;kernel_size&lt;/em&gt; 2x2 at &lt;em&gt;stride=2&lt;/em&gt;, a 1x1x2x2 tensor will be reduced to a single number, 1x1x1x1, as follows:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Prints &quot;tensor([[[[6.]]]], grad_fn=&amp;lt;MaxPool2DWithIndicesBackward&amp;gt;)&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">PyTorch Tutorial</summary></entry><entry><title type="html">Feature Descriptors</title><link href="http://johnwlambert.github.io/feature-descriptors/" rel="alternate" type="text/html" title="Feature Descriptors" /><published>2019-09-10T07:00:00-04:00</published><updated>2019-09-10T07:00:00-04:00</updated><id>http://johnwlambert.github.io/feature-descriptors</id><content type="html" xml:base="http://johnwlambert.github.io/feature-descriptors/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#state-estimation&quot;&gt;Master Theorem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Moravec Interest Operator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;SIFT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;SURF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Ratio Test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;state-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;local-image-features&quot;&gt;Local Image Features&lt;/h2&gt;

&lt;p&gt;Chapter 4 of the textbook focuses on points, patches, edges, lines
In video, can just track the points over local neighborhood – search for corresponding location in subsequent images
Other scenario – viewpoints are different enough that you cannot just search a local neighborhood
Chapters 6-7 will use the information from Chapter 4
Stitching will also need features
Stereo calibration will also need features
Local features used to be used for object recognition
High Level Idea
(1) the ability to identify salient points (“features” or “landmarks”) inside each image to track,and 
(2) the ability to find potential correspondences between these salient points across different images (in order to track them), and 
(3) the ability to identify and discard incorrect correspondences&lt;/p&gt;

&lt;p&gt;Timeline – most central parts of computer vision research
Question: why might these be useful?
Main idea is we wish to match features
Why? Image stitching, camera calibration, image registration, Image database retrieval, structure from motion, by understanding 2d projections of 3d points, and we understand the geometry of projection very well, and by understanding which 2d points correspond to identical 3d points, we can actually solve for 3d points, giving a 3d point cloud as structure, in a least-squares sense&lt;/p&gt;

&lt;h2 id=&quot;moravec-operator&quot;&gt;Moravec Operator&lt;/h2&gt;
&lt;p&gt;Hans Moravec [1] defines a corner to be a point with low self-similarity.
The algorithm tests each pixel in the image to see if a corner is present, by considering how similar a patch centered on the pixel is to nearby, largely overlapping patches. 
The similarity is measured by taking the sum of squared differences (SSD) between the corresponding pixels of two patches. 
A lower number indicates more similarity.
Look for local maxima
The corner strength is defined as the smallest SSD between the patch and its neighbours
if this number is high, then the variation along all shifts is either equal to it or larger than it, so capturing that all nearby patches look different.
Measure intensity variation at (x,y) by shifting a small window (3x3 or 5x5) by one pixel in each of the eight principle directions (horizontally, vertically, and four diagonals).
The “cornerness” of a pixel is the minimum intensity variation found over the eight shift directions:
• Use a window surrounding each pixel as its own matching template • Tests local autocorrelation of the image: – SSD = Sum of Squared Differences • Good matches in any direction • Flat image region • Good matches in only one direction • Linear feature or edge • No good matches in any direction • Distinctive point feature • Corner point&lt;/p&gt;

&lt;h3 id=&quot;hans-moravecs-stanford-cart-1960&quot;&gt;Hans Moravec’s &lt;em&gt;Stanford Cart&lt;/em&gt;, 1960&lt;/h3&gt;
&lt;p&gt;Looks like an absolute piece of junk. Car battery, 4, bicycle wheels
The Stanford Cart used a single black and white camera with a 1-Hz frame rate. It could follow an unbroken white line on a road for about 15 meters before breaking trac
developed the first stereo vision system for a mobile robot. Using a modified Cart, he obtained stereo images by moving a black and white video camera side to side to create a stereo baseline. Like human eye
research platform for studying the problem of controlling a Moon rover from Earth
The Cart moved 1 m every 10 to 15 min
A subroutine called the Interest Operator was applied to one of these pictures.
Another routine called the Correlator looked for these same regions in the other frames
At each pause on its computer-controlled itinerary, the Cart slid its camera from left to right on a 52-cm track, taking nine pictures at precise 6.5-cm intervals. Po
Find clouds of features on an object, don’t collide with it
Human eye baseline, distance between the two cameras, is around 5-7 centimeters
Harris
Theorem: If A is an n × n matrix, then the sum of the n eigenvalues of A is the trace of A and the product of the n eigenvalues is the determinant of A.
Flat region: no change in all directions
Edge: no change along the edge direction
Corner: significant change in all directions. shifting a window in any directionshould give a large change in intensity. Should easily recognize the point bylooking through a small window&lt;/p&gt;

&lt;h3 id=&quot;repeatability&quot;&gt;Repeatability&lt;/h3&gt;

&lt;p&gt;Why repeatable?
Add that there is no way to get a match if you don’t repeat the Keypoint in other image. Not optional to be repeatable. Most important property.
If you see the same image content/same pattern, you must fire in the same location. Want it to be repeatable! Can’t have a match if your keypoints aren’t even in repeatable locations&lt;/p&gt;

&lt;h2 id=&quot;deep-keypoints&quot;&gt;Deep Keypoints&lt;/h2&gt;

&lt;h3 id=&quot;tilde&quot;&gt;TILDE&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Find webcam images (same point of view, different seasons, different times of day)&lt;/li&gt;
  &lt;li&gt;Identify in these images a set of locations that we think can be found consistently over the different imaging conditions.&lt;/li&gt;
  &lt;li&gt;To collect a training set of positive samples, we first detect keypoints independently in each image of this dataset using SIFT&lt;/li&gt;
  &lt;li&gt;Iterate over the detected keypoints, starting with the keypoints with the smallest scale.&lt;/li&gt;
  &lt;li&gt;If a keypoint is detected at about the same location in most of the images from the same webcam, its location is likely to be a good candidate to learn.&lt;/li&gt;
  &lt;li&gt;The set of positive samples is then made of the patches from all the images, including the ones where the keypoint was not detected, and centered on the average location of the detections.&lt;/li&gt;
  &lt;li&gt;Learn 96 linear filters per each sequence&lt;/li&gt;
  &lt;li&gt;Piece-wise Linear Regressor&lt;/li&gt;
  &lt;li&gt;How can we define a descriptor vector for a keypoint? 
Sparse correspondences (it is not per every single pixel)
At a keypoint, all we have is a single color intensity, which can easily belong to any other object. Easy to make mistakes
What is a feature? Amir Zamir slide
Think of 2d case. 2d vectors. Want them to lie close by each other. L2 norm/Euclidean distance between these vectors. So that semantic/content similarity is related to proximity in high-dimensional space
Two species of fish
What is the right feature that makes the solution transparent? Weight, a single number
“Solving a problem simply means representing it so as to make the solution transparent.” - Herbert Simon, Sciences of the Artificial
Simple error metrics, such as the sum of squared differences between patches
A scaled and oriented patch around the detected point can be extracted and used to form a feature descriptor
Extracting a local scale, orientation, or affine frame estimate and then using this to resample the patch before forming the feature descriptor is thus usually preferable
Using histograms – 
Estimate some statistics of the patch. Then search for other patches with similar statistics (Histogram is a discrete estimate of the probability distribution of some random variable). Here, random variable is the grayscale intensity of any pixel, and it can take on 256 values [0,255], and we plot the counts of each
Histogram of grayscale values&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Can treat histograms as vectors, think of 2d vectors, and find similarity betwen two vectors by distance between them , L2 norm
How do images change?
a transformation could take the form of 
scene illumination, blur in video
 camera viewpoint changes.
 Camera viewpoint changes could take the form of scale changes or a rotation.
Want a feature vector that is not too large, because longer vectors will mean more computation when matching and finding nearest neighbors
Many of these lack a strong mathematical formulation like we saw on Monday, but are somewhat ad-hoc&lt;/p&gt;

&lt;h3 id=&quot;lowe-sift-1999&quot;&gt;[Lowe, SIFT, 1999]&lt;/h3&gt;
&lt;p&gt;Use image gradient
change in the value of a quantity (as temperature, pressure, or concentration) with change in a given variable and especially per unit on a linear scale.
Sobel
Forward, central difference, grayscale intensity centered at one pixel, above and below, left and righ
Derivative is rate of change: that is, the amount by which a function is changing at one given point
The gradient is the vector formed by the partial derivatives of a scalar function
Originally designed to use local features for object recognition (1999)
h transforms an image into a large collection of local feature vectors,
Template matching infeasible becuase of variation in object rotation, scale, illumination, and 3D pose
we used 8 orientation planes,
Preliminary paper
Store the SIFT keys for sample images and then identify matching keys from new images&lt;/p&gt;

&lt;h3 id=&quot;lowe-2004-scale-invariant-feature-sift&quot;&gt;Lowe 2004 Scale-Invariant feature (SIFT)&lt;/h3&gt;
&lt;p&gt;53k citations, refined version. Now states: “perform reliable matching between different views of an object or scene”
Can mix and match the detector: DOG for detecting blobs: 
The input image is first convolved with the Gaussian function using  = p 2 to give an image A. This is then repeated a second time with a further incremental smoothing of  = p 2 to give a new image, B, which now has an effective smoothing of  = 2. The difference of Gaussian function is obtained by subtracting image B from A, resulting in a ratio of 2=p 2 = p 2 between the two Gaussians.&lt;/p&gt;

&lt;p&gt;Compute image gradient magnitude and gradient orientation at every pixel
This is norm of L2 norm of two numbers
Arctangent, a mathematical function that is the inverse of the tangent function, atan2, arctan2&lt;/p&gt;

&lt;p&gt;SIFT feature is not just a keypoint (x,y) location in the image.
Feature also includes a scale estimate and an orientation estimate
Algorithm:
Step 1: Take 16x16 square window around detected interest point (8x8 window and 2x2 grid shown below for simplicity)
Compute x and y partial derivatives
Compute the gradient magnitude and orientation at each pixel
Step 2: angle of the gradient for each pixel
Step 3: threshold gradient magnitude
y thresholding the values in the unit feature vector to each be no larger than 0.2,
Step 4: Create histogram of surviving edge orientations (8 bins)
( length of each arrow corresponding to the sum of the gradient magnitudes near that direction within the region.)
Step 5: Divide the 16x16 window into a 4x4 grid of cells
Step 6: Spread out among 2 closest bins, Spread out to 4 nearest subgrids (using trilinear interpolation. 
It is important to avoid all boundary affects in which the descriptor abruptly changes as a sample shifts smoothly from being within one histogram to another or from one orientation to anothe
 trilinear interpolation is used to distribute the value of each gradient sample into adjacent histogram bins
Step 7: Compute an orientation histogram for each subgrid cell
Step 8: 16 cells * 8 orientations = 128 dimensional descriptor
Make it rotationally invariant!
Need a canonical orientation for every patch
We need orientation normalization. To find the dominant orientation, compute orientation histogram with 360 degrees split into 36 bins, detect peaks in this orientation histogram.  then figure out which is the up direction&lt;/p&gt;

&lt;p&gt;Translation invariance
You can shift over by 16 pixels, and you’ll get the same descriptor in the new location
Scale Invariance:
Or whole image: e image is rotated by 15 degrees, scaled by a factor of 0.9, and stretched by a factor of 1.1 in the horizontal direction
Partially invariant to illumination changes:
Could add 50 to all pixels, and barring overflow about 255, the x,y gradients would be identical
Totally brightness invariant (add constant to pixels)
Also contrast invariant, as long as now overflow
 (multiply pixels by constant)
Gradient values also all get scaled by a constant
If you divide all gradients by that same constant, you would get the same image gradients, leading to same 
To obtain brightness invariance, the SIFT descriptor is normalized to unit sum
128-dim vector normalized to 1
PLEASE DISCUSS WITH YOUR NEIGHBOR: WHAT ARE THE DISADVANTAGES OF THIS SIFT METHOD
SIFT: lossy summary of this data. Very good image descriptor
SIFTNet&lt;/p&gt;

&lt;h2 id=&quot;speeded-up-robust-features-surf-paper-integral-image-box-filters-bay-eccv06-cornelis-cvgpu08&quot;&gt;Speeded Up Robust Features (SURF) Paper, integral image, box filters [Bay, ECCV’06], [Cornelis, CVGPU’08]&lt;/h2&gt;

&lt;p&gt;New detector and a new descriptor, fast approximations of Harris and SIFT ideas
Harris corners are not scale-invariant
SURF is based on Hessian matrix
he determinant of the Hessian matrix is used as a measure of local change around the point and points are chosen where this determinant is maximal
Gaussian second order derivatives
Take derivative of Gaussian in each direction
Then take derivative again
Descriptor
extract the sum of the absolute values of the responses,
he descriptor for the sub square  is the sum of the x derivatives over its four quadrants, sum of the absolute values of the x derivatives and similarly for y. The total descriptor is 4 values per subgrid for a total of 64 values. (16x4)
The 4×4 sub-region division
responses dx and dy are summed up over each subregion and form a first set of entries to the feature vector
64-dimensional features
uses box filters to approximate the derivatives and integrals used in SIFT
box filter, which simply averages the pixel values in a K ×K window. This is equivalent to convolving the image with a kernel of all ones and then scaling
Each element of the integral image contains the sum of all pixels located on the up-left region of the original image (in relation to the element’s position). This allows to compute sum of rectangular areas in the image, at any position or scale, using only four lookups:
Turns out if you wish to determine the sum of pixels in any rectangular portion of an image, there is a very efficient way to do it&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;sum = A-B-C+D
A - (C-D) - (B-D) - D = A-B-C+D&lt;/script&gt;

&lt;p&gt;Only have to build the integral image once for an input image
Integral image gets bigger as you go down to the right
Rectangle filters (Haar wavelet). coefficient 1 and -1, dot product with the pixels that lie underneath them. Get one real-valued thing back \sum (pixels in white area}) - sum (pixels in black area)
Weak features individually
Build an integral image (Summed area table). Look at pixel (x,y), the value at pixel (x,y) is the sum of all of the pixels to the left and up from (x,y). Intermediate computation is fast to compute. Use dynamic programming (caching intermediate results): value above, and the sum of all the pixels up to you in your particular row
The integral image computes a value at each pixel (x,y) that is the sum of the pixel values above and to the left of (x,y), inclusive
Continue in scanline order, fast to compute the integral image
We can use the integral image to compute the features
Shape Context Paper, Belongie &amp;amp; Malik, ICCV 2001
How would you form a descriptor for a shape, not for a keypoint? Designed for Object recognition and digit recognition (image classification)
Some descriptors are extremely AD-HOC, some person just came up with some random idea, and found that it worked better than other methods at the time
Engineered/hand-crafted feature
Doesn’t have the strong mathematical derivation that we saw for other methods, like the Harris Corner Detector. True for SIFT also
What is disadvantage of SIFT?
Basic invariances e.g. to translation, scale and small amount of rotation must be obtained by suitable pre-processing (rotating and scaling patches), not built into the algorithm or theory
Distance between two point sets&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a set of points sampled from the contours on the object. Clear outline between the object, which we call foreground object, and the background object. Could use edge detector
Consider the set of vectors originating from a point to all other sample points on a shape. These vectors express the configuration of the entire shape relative to the reference point.
distribution over relative positions as a more robust and compact, yet highly discriminative descriptor.
Polar coordinate system
radius, angle two-dimensional coordinate system in which each point on a plane is determined by a distance from a reference point and an angle from a reference direction.
What is log-polar coordinates?
coordinate system in two dimensions, where a point is identified by two numbers, one for the logarithm of the distance to a certain point, and one for an angle
Log-polar histogram binning? Use reference point as an origin
Simply count the number of points that fall into each bin
Below, see 3 examples of what this descriptor looks like
use 5 bins for (log r) and 12 bins for (theta).
2d histogram. Can flatten into 12*5 = 60-dimensional vector
Stored prototype shapes. Capture global shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-learned-feature-descriptors--learn-the-weightsfilters-from-data&quot;&gt;Deep Learned Feature Descriptors – learn the weights/filters from data&lt;/h2&gt;

&lt;h3 id=&quot;learned-invariant-feature-transform-lift&quot;&gt;Learned Invariant Feature Transform (LIFT)&lt;/h3&gt;
&lt;p&gt;3 networks for detection, orientation, description. Trained separately
For matching patches, not full images
training the Descriptor first
n used to train the Orientation Estimator
Rotate patch according to the estimated patch orientation theta
d finally the Detector, based on the already learned Descriptor and Orientation Estimator (conditioned on the other two)
How does the soft argmax work? [12] from LIFT
This allows us to tune the Orientation Estimator for the Descriptor, and the Detector for the other two components
Correct matches recovered by each method are shown in green lines and the descriptor support regions with red circles
River scene, two stuffed animals, two skulls disturbing&lt;/p&gt;

&lt;h3 id=&quot;superpoint--build-dataset-what-is-homographic-adaptation&quot;&gt;SuperPoint – build dataset, what is homographic adaptation&lt;/h3&gt;
&lt;p&gt;output a semi-dense grid of descriptors (e.g., one every 8 pixels)
MagicPoint Net
The notion of interest point detection is semantically ill-defined.
create a large dataset of pseudo-ground truth interest point locations in real images, supervised by the interest point detector itself, rather than a large-scale human annotation effort
simple geometric shapes with no ambiguity in the interest point locations (Checkerboard grids, f quadrilaterals, triangles, lines and ellipses, Y-junctions, L-junctions, T-junctions as well as centers of tiny ellipses and end points of line segments.)
The MagicPoint detector performs very well on Synthetic Shapes, but does it generalize to real images? yes, but not as well as authors hoped
Automatically label images from a target, unlabeled domain.
Homographic Adaptation
The generated labels are used to (c) train a fully-convolutional network that jointly extracts interest points and descriptors from an image
Two images of the same planar surface in space are related by a homography. 3x3 matrix you can use to move from pixel coordinates to pixel coordinates in another image
The thing you are looking at is flat
Both images are viewing the same plane from a different angle
Move to a different viewpoint
SuperPoint Net
Take in unlabeled image and base detector
Sample a random homography, warp images, apply detector, get point response, unwarp heatmaps, then aggregate all heatmaps, get interest point superset&lt;/p&gt;
&lt;h3 id=&quot;d2net&quot;&gt;D2Net&lt;/h3&gt;
&lt;p&gt;Tensor viewed as descriptors and detector maps&lt;/p&gt;

&lt;h2 id=&quot;matching&quot;&gt;Matching&lt;/h2&gt;

&lt;p&gt;Establish some preliminary feature matches between these images
he feature descriptors have been designed so that Euclidean (vector magnitude) distances in feature space can be directly used for ranking potential matches
The simplest way to find all corresponding feature points is to compare all features against all other features in each pair of potentially matching images
A better approach is to devise an indexing structure, much like an index you find at the back of a textbook or book, that allows you to efficient find items
Like index in a back of a book. Book: find all pages on which a word occurs (index)
We will need some sense of accuracy for matching. Here are 4 useful metrics for evaluating predictions in machine learning:
TP: true positives, i.e., number of correct matches;
FN: false negatives, matches that were not correctly detected;
FP: false positives, proposed matches that are incorrect;
TN: true negatives, non-matches that were correctly rejected.&lt;/p&gt;

&lt;p&gt;For extremely large databases (millions of images or more), even more efficient structures based on ideas from document retrieval (e.g., vocabulary trees,
KD trees, divide the multidimensional feature space along alternating axis-aligned hyperplanes, choosing the threshold along each axis so as to maximize some criterion&lt;/p&gt;

&lt;h2 id=&quot;sift-descriptor-nearest-neighbor-distance-ratio&quot;&gt;SIFT Descriptor Nearest Neighbor Distance Ratio:&lt;/h2&gt;
&lt;p&gt;If you have many closeby features, then this is a bad feature!
will likely be a number of other false matches within similar distances due to the (high dimensionality of the feature space – Many high dimensional vector distances tend to a constant.)
PDF
Two axes:
In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values,
Probability density
With 100 samples, count how often they fall into these bins
Then normalize by dividing by one
Look at only the false positives&lt;/p&gt;

&lt;p&gt;Look at only the true positives&lt;/p&gt;

&lt;p&gt;second-closest match as providing an estimate of the density of false matches within this portion of the feature space
There is a lot to unpack in this plot.
The Ratio Test (Lowe, 2004) is an effective method for discarding incorrect SIFT descriptor matches but does not appear to be used in practice for CNN-trained descriptors. The Ratio Test discards nearest-neighbor matches if the ratio of distances to the first and second-nearest neighbors, known as the nearest neighbor distance ratio (NNDR), is greater than a specific threshold, usually 0.8. 
Intuitively, this means that the closest neighbor will be 4 units away, and 2nd closest was 10 units away, you are in good shape
However, if the closest neighbor is 9 units away, and second closest neighbor is 10 units away, then 
(Lowe, 2004) discovered that the NNDR probability density function (pdf) for true matches is centered at a much lower ratio than the pdf for false matches. In fact, the two pdfs are essentially disjoint, with a threshold of 0.8 providing the separating point. Lowe provides the intuition that many equally-distant nearest neighbor matches in a feature space are symptomatic of 
(1) matching with background clutter patches or 
(2) an undetected keypoint in one of the images from the pair to be matched
Questions to ask the students
What are the strengths of these descriptors
What are its weaknesses
How is the SIFT feature computed
How might you design a patch-based feature that is rotation or scale invariant
We can define this nearest neighbor distance ratio (Mikolajczyk and Schmid 2005) a
Once we have some hypothetical (putative) matches, we can often use geometric alignment (Section 6.1) to verify which matches are inliers and which ones are outliers.&lt;/p&gt;

&lt;p&gt;OpenCV’s SIFT C++ implementation (optimized for SIMD instructions) can be found &lt;a href=&quot;https://github.com/opencv/opencv_contrib/blob/master/modules/xfeatures2d/src/sift.cpp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;soft-argmax&quot;&gt;Soft Argmax&lt;/h2&gt;

&lt;p&gt;Given logits [0.1, 0.7, 0.05, 0.15], where the hard argmax would be 1, the soft argmax delivers 1.007 (as shown below). This float cannot be used as a hard index (i.e. integer), but fortunately the STN not need an integer to extract the patch.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1.007140612556097&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;LIFT [2] passes this subpixel keypoint localization to a STN.&lt;/p&gt;

&lt;h2 id=&quot;spatial-transformer-network&quot;&gt;Spatial Transformer Network&lt;/h2&gt;

&lt;p&gt;https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1]. Moravec&lt;/p&gt;

&lt;p&gt;[2]. LIFT: Learned Invariant Feature Transform. &lt;a href=&quot;https://icwww.epfl.ch/~trulls/pdf/eccv16-lift.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sandbox&quot;&gt;Sandbox:&lt;/h2&gt;

&lt;p&gt;Laplacian of Gaussians (Lindeberg (1993; 1998b)/ Difference of Gaussians will skip since we are talking about descriptors
Matching Local Self-Similarities across Images and Videos, Shechtman and Irani, 2007
Learning Local Image Descriptors, Winder and Brown, 2007
Comparison of Keypoint Detectors, Tuytelaars Mikolajczyk 2008
FAST, ORB
Mikolajczyk and Schmid -&amp;gt; GLOH performed best, followed closely by SIFT
Maximally Stable Extremal Regions [Matas ‘02]
stable connected component of some gray-level sets of the image .
regions which stay nearly the same through a wide range of thresholds
The concept can be explained informally as follows. 
Imagine all possible thresholdings of a gray-level image I. We will refer to the pixels below a threshold as ’black’ and 386 to those above or equal as ’white’. If we were shown a movie of thresholded images It, with frame t corresponding to threshold t, we would see first a white image. Subsequently black spots corresponding to local intensity minima will appear and grow. At some point regions corresponding to two local minima will merge. Finally, the last image will be black. The set of all connected components of all frames of the movie is the set of all maximal regions;
Regions whose rate of change of area with respect to the threshold is minimal are defined as maximally stable.
– Invariance to affine transformation of image intensities
where the relative area change as a function of relative change of threshold is at a local minimum, i.e. the MSER are the parts of the image where local binarization is stable over a large range of thresholds.[1][6]&lt;/p&gt;</content><author><name></name></author><summary type="html">SIFT, SURF, LIFT, SuperPoint, D2Net</summary></entry><entry><title type="html">Dynamic Programming</title><link href="http://johnwlambert.github.io/dp/" rel="alternate" type="text/html" title="Dynamic Programming" /><published>2019-09-10T07:00:00-04:00</published><updated>2019-09-10T07:00:00-04:00</updated><id>http://johnwlambert.github.io/dynamic-programming</id><content type="html" xml:base="http://johnwlambert.github.io/dp/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#state-estimation&quot;&gt;Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Integral Images&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;DP for Max Pooling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;DP for Graphical Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;state-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;applications&quot;&gt;Applications&lt;/h2&gt;

&lt;p&gt;Dynamic Programming (DP) …&lt;/p&gt;

&lt;p&gt;Linux utility &lt;code class=&quot;highlighter-rouge&quot;&gt;diff&lt;/code&gt;,&lt;/p&gt;

&lt;h2 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h2&gt;

&lt;p&gt;Longest Common Subsequence&lt;/p&gt;

&lt;h2 id=&quot;integral-images&quot;&gt;Integral Images&lt;/h2&gt;

&lt;p&gt;SURF, Viola Jones rely upon fast computation of Integral Images for fast local feature detection and object detection. Dynamic Programming&lt;/p&gt;

&lt;h2 id=&quot;max-pooling-in-convnets&quot;&gt;Max-Pooling in ConvNets&lt;/h2&gt;

&lt;p&gt;[1]&lt;/p&gt;

&lt;h2 id=&quot;graphical-models&quot;&gt;Graphical Models&lt;/h2&gt;

&lt;p&gt;Koller and Friedman [2] use DP extensively. Variable Elimination (pp. 292-296, 337) for the summation of the joint distribution from the inside out, rather from the outside in, and cache the intermediate results to avoid repeated computation.&lt;/p&gt;

&lt;p&gt;DP also arises in Belief Propagation (p. 356), Clique Tree Queries (p. 371), MAP and marginal MAP queries (p. 596).&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1]. Alessandro Giusti, Dan C. Cires¸an, Jonathan Masci, Luca M. Gambardella, Jurgen Schmidhuber. &lt;em&gt;Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6738831&amp;amp;tag=1&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2]. Daphne Koller and Nir Friedman. &lt;em&gt;Probabilistic Graphical Models: Principles and Techniques&lt;/em&gt;. 2009.&lt;/p&gt;</content><author><name></name></author><summary type="html">SIFT, SURF, LIFT, SuperPoint, D2Net</summary></entry><entry><title type="html">Algorithm Analysis</title><link href="http://johnwlambert.github.io/algorithm-analysis/" rel="alternate" type="text/html" title="Algorithm Analysis" /><published>2019-09-10T07:00:00-04:00</published><updated>2019-09-10T07:00:00-04:00</updated><id>http://johnwlambert.github.io/algorithm-analysis</id><content type="html" xml:base="http://johnwlambert.github.io/algorithm-analysis/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#state-estimation&quot;&gt;Master Theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;state-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;divide-and-conquer-algorithms&quot;&gt;Divide and Conquer Algorithms&lt;/h2&gt;

&lt;h3 id=&quot;recurrences&quot;&gt;Recurrences&lt;/h3&gt;

&lt;p&gt;Black box for solving recurrence, get an upper bound on the algorithm’s running time&lt;/p&gt;

&lt;p&gt;Only relevant when all of the subproblems have exactly the same size (recursing on 1/3 of the array, and then 1/2 of the array), would not be suitable&lt;/p&gt;

&lt;p&gt;Recurrence Format:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Base case &lt;script type=&quot;math/tex&quot;&gt;T(n) \leq&lt;/script&gt; a constant for all sufficiently small &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all larger &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;:
&lt;script type=&quot;math/tex&quot;&gt;T(n) \leq a T (\frac{n}{b}) + cn^d&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is number of recursive calls &lt;script type=&quot;math/tex&quot;&gt;(\geq 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;b =&lt;/script&gt; input size shrink factor &lt;script type=&quot;math/tex&quot;&gt;(&gt;1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;d =&lt;/script&gt; exponent in running time of combine step&lt;/p&gt;

&lt;p&gt;[a,b,d independent of n]&lt;/p&gt;

&lt;h3 id=&quot;formal-statement-of-master-theorem&quot;&gt;Formal Statement of Master Theorem&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T(n) \begin{cases} O(n^d \log n) &amp; \mbox{ if } a = b^d \mbox{ (Case 1)} \\
O(n^d) &amp; \mbox{ if } a &lt; b^d \mbox{ (Case 2)} \\
O(n^{\log_b a}) &amp; \mbox{ if } a &gt; b^d \mbox{ (Case 3)}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;At level j in the recursion tree, there are &lt;script type=&quot;math/tex&quot;&gt;a^j&lt;/script&gt; problems, and each subproblem is of size &lt;script type=&quot;math/tex&quot;&gt;n / b^j&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Get a bound on the work done at level &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \leq \underbrace{a^j}_{\text{# of level-j subproblems}} \cdot c \Bigg[ \underbrace{ \frac{n}{b^j}}_{\text{size of each level-j subproblem}} \Bigg]^d \\
&amp;= cn^d \cdot [\frac{a}{b^d}]^j
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, summing over all levels &lt;script type=&quot;math/tex&quot;&gt;j=0,1,2,\dots, \log_b n&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{Total Work} \leq cn^d \sum\limits_{j=0}^{\log_b n} [\frac{a}{b^d}]^j&lt;/script&gt;

&lt;p&gt;Surprisingly, 
Forces of good:
&lt;script type=&quot;math/tex&quot;&gt;b^d =&lt;/script&gt; rate of work shrinkage (RWS) (per sub-problem)&lt;/p&gt;

&lt;p&gt;Forces of evil:
&lt;script type=&quot;math/tex&quot;&gt;a =&lt;/script&gt; rate of subproblem proliferation (RSP), function of $$J&lt;/p&gt;

&lt;p&gt;Why is the rate of work-shrinkage not just &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;? Why is it &lt;script type=&quot;math/tex&quot;&gt;b^d&lt;/script&gt;? This is because we care about how much work goes down per sub-problem. Linear -&amp;gt; down to half, Quadratic -&amp;gt; down to quarter.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If RSP &amp;lt; RWS, then the amount of work is &lt;em&gt;decreasing&lt;/em&gt; with the recursion level &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If RSP &amp;gt; RWS, then the amount of work is &lt;em&gt;increasing&lt;/em&gt; with the recursion level &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If RSP and RWS are equal, then the amount of work is &lt;em&gt;the same&lt;/em&gt; at every recursion level &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RSP &amp;gt; RWS means most work at the leaves! (leaves dominant), get &lt;script type=&quot;math/tex&quot;&gt;O(\# Leaves)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;RSP &amp;lt; RWS means less work at each level (most work at the root!) Might expect &lt;script type=&quot;math/tex&quot;&gt;O(n^d)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;In the tie, there is same amount of work at each level (like MergeSort). Logarithmic number of levels, do &lt;script type=&quot;math/tex&quot;&gt;n^d&lt;/script&gt; work at each level. So we would expect &lt;script type=&quot;math/tex&quot;&gt;O(n^d \log n)&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practice, we see that in case #1, we did not get &lt;script type=&quot;math/tex&quot;&gt;O(\# Leaves)&lt;/script&gt;, but rather &lt;script type=&quot;math/tex&quot;&gt;O(n^{\log_b a})&lt;/script&gt;. Turns out, these are identical, since there are &lt;script type=&quot;math/tex&quot;&gt;a^{log_b n}&lt;/script&gt; leaves in the recursion tree. Since &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is the branching factor, this process continues until we get to the leaves (number of times we multiply by &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;In the Equilibrium, &lt;script type=&quot;math/tex&quot;&gt;a=b^d&lt;/script&gt;:
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned}
\mbox{Total Work} \leq cn^d \sum\limits_{j=0}^{\log_b n} [\frac{a}{b^d}]^j \\
\mbox{Total Work} \leq cn^d \sum\limits_{j=0}^{\log_b n} [1]^j \\
\mbox{Total Work} \leq cn^d (\log_b n + 1)
\end{aligned}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It turns out that by the Power Rule of logarithms, i.e. &lt;script type=&quot;math/tex&quot;&gt;y \log_a x = \log_a x^y&lt;/script&gt;, (see a proof here of this rule): &lt;a href=&quot;https://www.khanacademy.org/math/algebra2/x2ec2f6f830c9fb89:logs/x2ec2f6f830c9fb89:log-prop/a/justifying-the-logarithm-properties&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
a^{\log_b n} &amp;= n^{\log_b a} \\
(\log_b n)(\log_b a) &amp;= (\log_b a)(\log_b n)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;While the left side is more intuitive, the right side is easier to apply and evaluate.&lt;/p&gt;

&lt;h3 id=&quot;example-mergesort&quot;&gt;Example: MergeSort&lt;/h3&gt;

&lt;h3 id=&quot;closest-pair&quot;&gt;Closest Pair&lt;/h3&gt;

&lt;h3 id=&quot;quicksort&quot;&gt;QuickSort&lt;/h3&gt;

&lt;h3 id=&quot;fast-fourier-transform&quot;&gt;Fast Fourier Transform&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(n \log n)&lt;/script&gt;

&lt;p&gt;Interpolation as solving system of equations for polynomial coefficients.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h2&gt;

&lt;p&gt;Least Common Subsequence&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Tim Roughgarden, Lectures.&lt;/p&gt;</content><author><name></name></author><summary type="html">Master Theorem, Divide and Conquer</summary></entry><entry><title type="html">Multiprocessing</title><link href="http://johnwlambert.github.io/2019/07/19/multiprocessing.html" rel="alternate" type="text/html" title="Multiprocessing" /><published>2019-07-19T00:00:00-04:00</published><updated>2019-07-19T00:00:00-04:00</updated><id>http://johnwlambert.github.io/2019/07/19/multiprocessing</id><content type="html" xml:base="http://johnwlambert.github.io/2019/07/19/multiprocessing.html">&lt;h2 id=&quot;pytorch-multiprocessing&quot;&gt;&lt;a href=&quot;https://pytorch.org/docs/1.0.0/multiprocessing.html?highlight=multiprocessing#module-torch.multiprocessing&quot;&gt;PyTorch Multiprocessing&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;use shared memory to provide shared views on the same data in different processes&lt;/p&gt;

&lt;p&gt;Once the tensor/storage is moved to shared_memory,  it will be possible to send it to other processes without making any copies.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nprocs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;daemon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function is called as &lt;code class=&quot;highlighter-rouge&quot;&gt;fn(i, *args)&lt;/code&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; is the process index and &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt; is the passed-through tuple of arguments. The argument &lt;code class=&quot;highlighter-rouge&quot;&gt;nprocs&lt;/code&gt; is an integer represernting the number of processes to spawn.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Lane Detection</title><link href="http://johnwlambert.github.io/2019/07/04/lane-detection.html" rel="alternate" type="text/html" title="Lane Detection" /><published>2019-07-04T00:00:00-04:00</published><updated>2019-07-04T00:00:00-04:00</updated><id>http://johnwlambert.github.io/2019/07/04/lane-detection</id><content type="html" xml:base="http://johnwlambert.github.io/2019/07/04/lane-detection.html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Feature extraction: Low level features are extracted from the image to support lane and road detection. For road detection, these typically include color and texture statistics allowing road segmentation, road patch classification or curb detection. For lane detection, evidence for lane marks is collected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Road/lane model fitting: A road and lane hypothesis is formed by fitting a road/lane model to the evidence gathered.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Temporal integration: The road and lane hypothesis is reconciled with road/lane hypotheses from
the previous frame a&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image to world correspondence: This module provide services of translation between image and ground coordinates, using assumptions about the ground structure and camera parameters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;4.6
Inverse
perspective warping requires the system to be aware of
the geometrical connection between the 2D image and
the 3D ground plane.&lt;/p&gt;

&lt;p&gt;The lane model is transformed from image to real world coordinates, typically using an inverse perspective transformation.&lt;/p&gt;

&lt;p&gt;naive inverse perspective transformation, assuming stable camera calibration and flat zero-level ground
surface.&lt;/p&gt;

&lt;p&gt;Vehicle vibrations and changing ground slopes
inject noise and abrupt changes into the time series,
posing some difficulty for tracking methods with strong
smoothness assumptions&lt;/p&gt;

&lt;p&gt;Builds on He et al (2016)&lt;/p&gt;

&lt;p&gt;Xiong et al do not use stereo – they generate point cloud from monocular&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Metric Learning</title><link href="http://johnwlambert.github.io/metric-learning/" rel="alternate" type="text/html" title="Metric Learning" /><published>2019-06-18T07:00:00-04:00</published><updated>2019-06-18T07:00:00-04:00</updated><id>http://johnwlambert.github.io/metric-learning</id><content type="html" xml:base="http://johnwlambert.github.io/metric-learning/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-residual&quot;&gt;Why do we need residual connections?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-hyperplane-is-a-linear-classifier&quot;&gt;A hyperplane is a linear classifier&lt;/h2&gt;

&lt;p&gt;A hyperplane is a plane in n-dimensions that can split a space into two halfspaces. For the sake of simplicity, consider a hyperplane in 2-dimensional space (a line).&lt;/p&gt;

&lt;p&gt;A common and intuitive representation of a line in two-dimensions, &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt;, is &lt;script type=&quot;math/tex&quot;&gt;y=mx+b&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;y=x_2&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;x=x_1&lt;/script&gt;. Here &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; represents slope, and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; represents a y-intercept. This representation is a poor choice, however, because we cannot represent vertical lines, e.g. &lt;script type=&quot;math/tex&quot;&gt;x = c&lt;/script&gt;, with this expression. A vertical line equation would have to be something like &lt;script type=&quot;math/tex&quot;&gt;y = mx&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;m \rightarrow \infty&lt;/script&gt;. Thus, this isn’t a general equation of a line.&lt;/p&gt;

&lt;p&gt;Over &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt;, a better representation is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_0 + w_1 x_1 + w_2 x_2 = 0&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;w_0, w_1, w_2&lt;/script&gt; are weights (“parameters”), and &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; is often considered the “bias” term, denoting the distance from the origin.  When &lt;script type=&quot;math/tex&quot;&gt;w_2 =0&lt;/script&gt;, we have a vertical line, and when &lt;script type=&quot;math/tex&quot;&gt;w_1=0&lt;/script&gt;, we have a horizontal line. We can always convert back to our intuitive representation &lt;script type=&quot;math/tex&quot;&gt;y=mx+b&lt;/script&gt; by simple algebra. For example, consider the hyperplane&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
W = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix}^T = \begin{bmatrix} 2 \\ 1 \end{bmatrix}^T, &amp; w_0 = b = -2
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;By algebra:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}

w_0 + w_1 x_1 + w_2 x_2 = 0 \\
w_0 + w_1 x + w_2 y = 0 &amp; \mbox{ (let } y = x_2, x = x_1) \\
w_2 y =  -w_1 x - w_0 \\
y = -\frac{w_1}{w_2}x + -\frac{w_0}{w_2} \\
y = -\frac{2}{1}x + -\frac{-2}{1} &amp;  \mbox{ (let } w_0 = -2, w_1 = 2, w_2 = 1) \\
y = -2x + 2
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;http://cs231n.github.io/linear-classify/&lt;/p&gt;

&lt;p&gt;https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.1%20DiscFns.pdf&lt;/p&gt;

&lt;p&gt;http://grzegorz.chrupala.me/papers/ml4nlp/linear-classifiers.pdf&lt;/p&gt;

&lt;h2 id=&quot;angular-softmax&quot;&gt;Angular Softmax&lt;/h2&gt;

&lt;p&gt;SphereFace&lt;/p&gt;

&lt;h2 id=&quot;hinge-loss&quot;&gt;Hinge Loss&lt;/h2&gt;

&lt;p&gt;max-margin&lt;/p&gt;

&lt;h2 id=&quot;contrastive-loss&quot;&gt;Contrastive Loss&lt;/h2&gt;

&lt;p&gt;A contrastive loss includes bo&lt;/p&gt;

&lt;p&gt;Compute contrastive loss with push term for different class pairs and pull term for same-class pairs. Push term says if pair elements come from separate classes, push embeddings apart. Pull term says if pair elements come from the same class, pull embeddings together. Sum losses as contrastive loss*&lt;/p&gt;

&lt;p&gt;We could implement such a contrastive loss in Pytorch as follows&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	Contrastive loss also defined in:
	-	&quot;Dimensionality Reduction by Learning an Invariant Mapping&quot; 
			by Raia Hadsell, Sumit Chopra, Yann LeCun
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;contrastive_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Compute the similarities in the separation loss by 
        computing average pairwise similarities between points
        in the embedding space.

		element-wise square, element-wise maximum of two tensors.

        Args:
        -   y_c: Indicates if pairs share the same semantic class label or not
        -   pred_dists: Distances in the embeddding space between pairs. 

        Returns:
        -   tensor representing contrastive loss value.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# corresponds to &quot;d&quot; in the paper. If same class, pull together.&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Zero loss if all same-class examples have zero distance between them.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pull_losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# corresponds to &quot;k&quot; in the paper. If different class, push apart more than margin&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# if semantically different examples have distances are in [0,margin], then there WILL be loss&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;zero&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;zero&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clamped_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;push_losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamped_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pull_losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;push_losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Consider 5 data points: 2 belong to the same class, and the rest belong to different classes.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_contrastive_loss1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Should be no loss here (zero from pull term, and zero from push term)
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# which pairs share the same semantic class label&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# distances between pairs&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrastive_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_contrastive_loss2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; 
    There should be more loss here (coming only from push term)
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# which pairs share the same semantic class label&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# distances between pairs&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrastive_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3880&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_contrastive_loss3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    There should be the most loss here (some from pull term, and some from push term also)
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# which pairs share the same semantic class label&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# distances between pairs&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrastive_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.3880&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gt_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;triplet-loss&quot;&gt;Triplet Loss&lt;/h2&gt;

&lt;p&gt;https://github.com/lugiavn/generalization-dml&lt;/p&gt;

&lt;p&gt;Hard negative mining&lt;/p&gt;

&lt;h2 id=&quot;which-layer-to-fine-tune&quot;&gt;Which layer to fine-tune?&lt;/h2&gt;

&lt;h2 id=&quot;additive-margin-softmax&quot;&gt;Additive Margin Softmax&lt;/h2&gt;

&lt;p&gt;[1] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song. SphereFace: Deep Hypersphere Embedding for Face Recognition. &lt;a href=&quot;https://arxiv.org/abs/1704.08063&quot;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2]&lt;/p&gt;

&lt;p&gt;[3]&lt;/p&gt;</content><author><name></name></author><summary type="html">contrastive loss, triplet loss, angular softmax</summary></entry><entry><title type="html">Understanding Softmax Cross Entropy</title><link href="http://johnwlambert.github.io/cross-entropy/" rel="alternate" type="text/html" title="Understanding Softmax Cross Entropy" /><published>2019-06-18T07:00:00-04:00</published><updated>2019-06-18T07:00:00-04:00</updated><id>http://johnwlambert.github.io/cross-entropy</id><content type="html" xml:base="http://johnwlambert.github.io/cross-entropy/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-residual&quot;&gt;Why do we need residual connections?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;measures-of-information&quot;&gt;Measures of Information&lt;/h2&gt;

&lt;p&gt;Common measures of information include entropy, mutual information, joint entropy, conditional entropy, and the s Kullback-Leibler (KL) divergence (also known as relative entropy).&lt;/p&gt;

&lt;h2 id=&quot;surprise&quot;&gt;Surprise&lt;/h2&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; be a discrete random variable, taking values in &lt;script type=&quot;math/tex&quot;&gt;{\mathcal U} = \{ u_1, u_2, \dots, u_M\}&lt;/script&gt;. The &lt;em&gt;Surprise&lt;/em&gt; Function &lt;script type=&quot;math/tex&quot;&gt;S(u)&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(u) = \log_2 \Big( \frac{1}{p(U=u)} \Big) = \log_2 \Big( \frac{1}{p(u)} \Big)&lt;/script&gt;

&lt;p&gt;To make that more concrete,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;surprise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We obtain probability values &lt;script type=&quot;math/tex&quot;&gt;p(u)&lt;/script&gt;, from small to large:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p_u
array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and surprise values that decrease as &lt;script type=&quot;math/tex&quot;&gt;p(u)&lt;/script&gt; increases:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([ inf, 3.32, 2.32, 1.74, 1.32, 1.  , 0.74, 0.51, 0.32, 0.15, 0.  ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can plot the values:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r'$p(u)$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r'Surprise $S(u)$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/surprise_function.png&quot; width=&quot;45%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    Less probable events generate higher &quot;surprise&quot; values (they surprise us more than very likely outcomes).
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;

&lt;p&gt;The entropy of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is simply the &lt;em&gt;expected&lt;/em&gt; Surprise. More formally,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
H(U) &amp;= \mathbb{E}[S(U)] \\
     &amp;= \mathbb{E}\Big[\log_2\big(\frac{1}{p(U)}\big)\Big] \\
     &amp;= \mathbb{E}\Big[\log_2\big(p(U)^{-1}\big)\Big] \\
     &amp;= \mathbb{E}\Big[-\log_2 p(U)\Big] &amp; \text{because } \log x^{k} = k \log x \\
     &amp;= - \sum\limits_u p(u) \log_2 p(u) &amp; \text{by definition of expectation}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, entropy measures the amount of surprise or randomness in the random variable [1].&lt;/p&gt;

&lt;p&gt;Consider the entropy of 3 simple distributions over &lt;script type=&quot;math/tex&quot;&gt;{\mathcal U} = \{ u_1, u_2, u_3, u_4\}&lt;/script&gt;, and their probability masses &lt;script type=&quot;math/tex&quot;&gt;[p(U=u_1),p(U=u_2),p(U=u_3),p(U=u_4)]&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.97&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;surprise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.04&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The entropy of p1 is 0.24, since although we get some high surprise values, they have very little probability mass.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;surprise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.74&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.4&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The entropy of p2 is 1.54, and once again, although we get one high surprise value, it has almost no probability mass: &lt;script type=&quot;math/tex&quot;&gt;p(U=u_4)=0.01&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;surprise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Finally, the entropy of p3 is 2.0. We get the most average surprise from p3 (a uniform distribution).&lt;/p&gt;

&lt;h2 id=&quot;kl-divergence-relative-entropy&quot;&gt;KL Divergence (Relative Entropy)&lt;/h2&gt;

&lt;p&gt;The Kullback-Leibler (KL) divergence, also known as relative entropy, is defined as&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;need-for-residual&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross Entropy&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(p,q) = - \sum_x p(x) \log q(x)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(p,q) = H(p) + D_{KL}(p||q)&lt;/script&gt;

&lt;p&gt;In PyTorch, the negative log likelihood loss (&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss&quot;&gt;NLL loss&lt;/a&gt;) can be used for multinomial classification, and expects to receive log-probabilities for each class.&lt;/p&gt;

&lt;p&gt;Consider an empirical output distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;[0.2, 0.6, 0.2]&lt;/code&gt; and a target distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;[0,1,0]&lt;/code&gt;, in a one-hot format. The index of the target class is simply &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;, which is the format PyTorch expects for the target. &lt;code class=&quot;highlighter-rouge&quot;&gt;NLLLoss()&lt;/code&gt; expects log-probabilities:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NLLLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5108&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our goal is to minimize this quantity. We can do so by making the target and empirical distribution match more closely. When the two distributions come closer and closer, the loss (divergence between them) decreases:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NLLLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In this case, &lt;code class=&quot;highlighter-rouge&quot;&gt;0.9163, 0.5108, 0.2231, 0.&lt;/code&gt; is printed, meaning the loss drops as the distributions align, until it finally reaches zero when the distributions are identical.&lt;/p&gt;

&lt;p&gt;(&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.CrossEntropyLoss&lt;/code&gt; incorporates &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.LogSoftmax&lt;/code&gt; inside of it).&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;Consider a binary (two-class) classification problem. Here &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; can take on the values &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}&lt;/script&gt;. A common real-world example is a spam classifier for email, where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; denotes features for an email, and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is 1 if it is a piece of spam mail [2], and 0 otherwise. We call &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; the label for the training example.&lt;/p&gt;

&lt;p&gt;Since we wish to train a model to output probabilities &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; in the range &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p&gt;0.5&lt;/script&gt; might indicate a prediction that &lt;script type=&quot;math/tex&quot;&gt;y=1&lt;/script&gt;, we will use a function with that exact range (the sigmoid or logistic function):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(z) = \sigma(z) = \frac{1}{1 + e^{-z}}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/sigmoid_loss.png&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    The sigmoid function.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;g(z)&lt;/script&gt; tends towards 1 as &lt;script type=&quot;math/tex&quot;&gt;z \rightarrow \infty&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;g(x)&lt;/script&gt; tends toward 0 as &lt;script type=&quot;math/tex&quot;&gt;z \rightarrow -\infty&lt;/script&gt;. 
Our hypothesis class will be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x) = g(\theta^Tx) = \sigma(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;g(z)&lt;/script&gt; is bounded between &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt; will also be bounded. In order to fit a model, we need to learn a set of weights &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; from data. We can do so by identifying a set of probabilistic assumptions and ffiting the model via maximum likelihood estimation (MLE) [2]. Let us assume that two events are possible: either &lt;script type=&quot;math/tex&quot;&gt;y=1&lt;/script&gt;, or &lt;script type=&quot;math/tex&quot;&gt;y=0&lt;/script&gt;. The probability of the second event is equal to the complement of the probability of the first event, thus:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
P(y = 1 \mid x; \theta) = h_{\theta}(x) \\
P(y = 0 \mid x; \theta) = 1 - h_{\theta}(x) 
\end{aligned}&lt;/script&gt;

&lt;p&gt;A beautiful way to combine these two expressions is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y \mid x; \theta) = \Big( h_{\theta}(x)\Big)^y \Big(1 - h_{\theta}(x)\Big)^{1-y}&lt;/script&gt;

&lt;p&gt;We note that as desired, if &lt;script type=&quot;math/tex&quot;&gt;y=0&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;p(y=0 \mid x; \theta) = h_{\theta}(x)^0 \Big(1 - h_{\theta}(x)\Big)^{1-0} = 1 \cdot (1 - h_{\theta}(x))&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Also, if &lt;script type=&quot;math/tex&quot;&gt;y=1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\Big( h_{\theta}(x)\Big)^1 \Big(1 - h_{\theta}(x)\Big)^{0} = h_{\theta}(x) \cdot 1&lt;/script&gt;, as desired.&lt;/p&gt;

&lt;p&gt;If we assume that &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; training example were generated independently, then their joint probability is equal to the product of the probability of each independent event [2]. Let &lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt; denote a column vector with stacked &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; entries, and let &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; denote a matrix with stacked &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; entries. Then the likelihood is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
L(\theta) &amp;= p(\vec{y} \mid X; \theta) \\
		&amp;= \prod_{i=1}^m p( y^{(i)} \mid x^{(i)}; \theta ) \\
		&amp;= \prod_{i=1}^m \big( h_{\theta}(x^{(i)}) \big)^{y^{(i)}} \Big( 1 - h_{\theta}(x^{(i)}) \Big)^{1 - y^{(i)}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Maximizing the log-likelihood is easier, and we recall that the log of a product is a sum of logs:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ell(\theta) &amp;= \log L(\theta) \\
			&amp;= \log \Bigg( \prod_{i=1}^m \big( h_{\theta}(x^{(i)}) \big)^{y^{(i)}} \Big( 1 - h_{\theta}(x^{(i)}) \Big)^{1 - y^{(i)}} \Bigg) \\
			&amp;= \sum_{i=1}^m \log \Bigg(  \big( h_{\theta}(x^{(i)}) \big)^{y^{(i)}} \Big( 1 - h_{\theta}(x^{(i)}) \Big)^{1 - y^{(i)}} \Bigg) &amp; \mbox{log of a product is a sum of logs} \\
			&amp;= \sum_{i=1}^m \log  \big( h_{\theta}(x^{(i)}) \big)^{y^{(i)}} + \log \Big( 1 - h_{\theta}(x^{(i)}) \Big)^{1 - y^{(i)}} &amp; \mbox{log of a product is a sum of logs} \\
			&amp;= \sum\limits_{i=1}^m y^{(i)} \log h_{\theta}(x^{(i)}) + (1-y^{(i)}) \log \big(1-h_{\theta}(x^{(i)})\big)  &amp; \mbox{because } \log(M^k)  = k \cdot \log M
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can maximize the likelihood in this case by gradient ascent, equivalent to minimizing the negative log likelihood, &lt;script type=&quot;math/tex&quot;&gt;-\ell(\theta)&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;NLL(\theta) = -\ell(\theta) = \sum\limits_{i=1}^m y^{(i)} \log h_{\theta}(x^{(i)}) + (1-y^{(i)}) \log \big(1-h_{\theta}(x^{(i)})\big)&lt;/script&gt;

&lt;h2 id=&quot;the-sigmoid--binary-cross-entropy-bce-loss&quot;&gt;The Sigmoid / Binary Cross Entropy (BCE) Loss&lt;/h2&gt;

&lt;p&gt;As described in the TensorFlow &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits&quot;&gt;docs&lt;/a&gt;, the logistic regression loss (often called sigmoid loss or binary cross entropy loss) can be used to &lt;em&gt;measure the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.&lt;/em&gt; In PyTorch, there are two options for this loss: one with the sigmoid output, and one with the sigmoid input.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.BCELoss&lt;/code&gt; accepts the sigmoid output &lt;script type=&quot;math/tex&quot;&gt;h^{(i)}&lt;/script&gt;, not &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;h^{(i)} = h_{\theta}(x^{(i)}) = \sigma(\theta^T x^{(i)})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;NLL_i(x^{(i)}, y^{(i)}) = - \Bigg[ y^{(i)} \log h^{(i)} + (1-y^{(i)}) \log \big(1-h^{(i)}\big) \Bigg]&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt; accepts as input &lt;script type=&quot;math/tex&quot;&gt;\theta^Tx^{(i)}&lt;/script&gt;, and applies the sigmoid function to it.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;NLL_i(x^{(i)}, y^{(i)}) = - \Bigg[ y^{(i)} \log \sigma(\theta^Tx^{(i)}) + (1-y^{(i)}) \log \big(1-\sigma(\theta^Tx^{(i)})\big) \Bigg]&lt;/script&gt;

&lt;p&gt;Thus, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt; combines a &lt;code class=&quot;highlighter-rouge&quot;&gt;Sigmoid&lt;/code&gt; layer and the &lt;code class=&quot;highlighter-rouge&quot;&gt;BCELoss&lt;/code&gt; in one single class.&lt;/p&gt;

&lt;h2 id=&quot;numerical-stability-of-the-sigmoidbce-loss&quot;&gt;Numerical Stability of the Sigmoid/BCE Loss&lt;/h2&gt;

&lt;p&gt;https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits&lt;/p&gt;

&lt;h2 id=&quot;softmax-regression&quot;&gt;Softmax Regression&lt;/h2&gt;

&lt;p&gt;(Section 9.3 &lt;em&gt;Softmax Regression&lt;/em&gt; of [2]).&lt;/p&gt;

&lt;h2 id=&quot;the-softmax-operation&quot;&gt;The Softmax Operation&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;soft&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first row can identically be computed by:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Consider the Negative Log Likelihood (NLL) loss criterion.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Tsachy Weissman. Information Theory (EE 376, Stanford) Course Lectures. &lt;a href=&quot;http://web.stanford.edu/class/ee376a/files/2017-18/lecture_3.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Andrew Ng. CS229 Lecture notes. &lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes1.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">information theory, maximum likelihood</summary></entry><entry><title type="html">Residual Connections in Deep Networks</title><link href="http://johnwlambert.github.io/resnet/" rel="alternate" type="text/html" title="Residual Connections in Deep Networks" /><published>2019-06-18T07:00:00-04:00</published><updated>2019-06-18T07:00:00-04:00</updated><id>http://johnwlambert.github.io/resnet</id><content type="html" xml:base="http://johnwlambert.github.io/resnet/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-residual&quot;&gt;Why do we need residual connections?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#buildingblock&quot;&gt;The ResNet BuildingBlock (BasicBlock)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bottleneck&quot;&gt;The ResNet Bottleneck&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#full-arch&quot;&gt;Putting it all together: the full architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-residual&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;why-do-we-need-residual-connections&quot;&gt;Why do we need residual connections?&lt;/h2&gt;

&lt;p&gt;An interesting phenomenon observed by many is that naively increasing network depth leads to saturated accuracy, and then rapid degradation of accuracy. The question is, &lt;em&gt;why does this degradation happen?&lt;/em&gt;. 
He et &lt;em&gt;al.&lt;/em&gt; [1] provide a convincing answer as to why, as described below.&lt;/p&gt;

&lt;p&gt;Consider a desired mapping &lt;script type=&quot;math/tex&quot;&gt;{\mathcal H}(x)&lt;/script&gt; we hope to achieve by stacking a few layers in a network. Instead of training these layers to fit &lt;script type=&quot;math/tex&quot;&gt;{\mathcal H}(x)&lt;/script&gt; directly, it may be better to train them to fit a &lt;em&gt;residual mapping&lt;/em&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;{\mathcal F}(x) := {\mathcal H}(x) − x&lt;/script&gt;. The word residual means “a quantity remaining after other things have been subtracted or allowed for,” which exactly describes this relationship. We can always recover the original, desired mapping now by recasting it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathcal H}(x) = {\mathcal F}(x) + x&lt;/script&gt;

&lt;p&gt;Why might this be better? The original mapping would be “unreferenced” – have no reference to the original input afterwards. As an extreme example, consider if an identity mapping were optimal: &lt;script type=&quot;math/tex&quot;&gt;{\mathcal H}(x) = x&lt;/script&gt;. In this case, it is much easier to fit the residual mapping &lt;script type=&quot;math/tex&quot;&gt;{\mathcal F}(x)&lt;/script&gt; to return zero, then to train a stack of nonlinear layers to learn an identity mapping.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
{\mathcal H}(x) &amp;= {\mathcal F}(x) + x \\
x &amp;= 0 + x \\
\end{aligned} %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Perhaps small changes (adding a small residual) is what is optimal, and are easier to optimize. We call these connections from the input directly to the output &lt;em&gt;identity shortcuts&lt;/em&gt; or &lt;em&gt;skip-connections&lt;/em&gt;, and they are parameter-free. All information is always passed through them, with additional residual functions to be learned. Parameterizing &lt;script type=&quot;math/tex&quot;&gt;{\mathcal F}&lt;/script&gt; with weights &lt;script type=&quot;math/tex&quot;&gt;W_i&lt;/script&gt;, we obtain the following mapping:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = {\mathcal F} (x, {W_i }) + x&lt;/script&gt;

&lt;p&gt;Simply put, ResNets allow for paths where information can flow more directly.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;buildingblock&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-resnet-buildingblock-basicblock&quot;&gt;The ResNet BuildingBlock (BasicBlock)&lt;/h2&gt;

&lt;p&gt;A ResNet architecture with hundreds of layers is difficult to depict. However, it is composed of just two simple, repeated modules: The BuildingBlock and the Bottleneck. I will describe the BuildingBlock here.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/resnet_building_block.png&quot; width=&quot;35%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    Left: the basic building block (&quot;BasicBlock&quot;) of a ResNet model.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Consider a class that implements such a module. First we copy the input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; for later, then performs two successive convolutions on the input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, and then adds back the copy of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, and returns the variable “&lt;script type=&quot;math/tex&quot;&gt;\texttt{out}&lt;/script&gt;”. We use batch normalization after each convolution. Here is the Python code for the &lt;script type=&quot;math/tex&quot;&gt;\texttt{BasicBlock}&lt;/script&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BasicBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BasicBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv3x3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv3x3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;bottleneck&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-resnet-bottleneck&quot;&gt;The ResNet Bottleneck&lt;/h2&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/resnet_buildingblock_bottleneck.png&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    Left: BuildingBlock/BasicBlock architecture. Right: Bottleneck architecture.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A slightly more complicated instantiation of the &lt;script type=&quot;math/tex&quot;&gt;\texttt{BasicBlock}&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\texttt{Bottleneck}&lt;/script&gt;. Instead of just two successive convolutions, the Bottleneck applies 3 successive convolutions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Bottleneck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bottleneck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downsample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;full-arch&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;/h2&gt;

&lt;p&gt;There are many official variations of the ResNet architecture, with varying depth and varying performance.&lt;/p&gt;

&lt;p&gt;Every variation has a prefix layer, four main “layers”, then an average pool, and a fully connected layer:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplanes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_make_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_make_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;dilate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace_stride_with_dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_make_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;dilate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace_stride_with_dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_make_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;dilate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace_stride_with_dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdaptiveAvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Depending upon the particular depth variation, each “layer” has a variable number of blocks.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ResNet18: BasicBlock, [2, 2, 2, 2]&lt;/li&gt;
  &lt;li&gt;ResNet34: BasicBlock, [3, 4, 6, 3],&lt;/li&gt;
  &lt;li&gt;ResNet50: Bottleneck, [3, 4, 6, 3]&lt;/li&gt;
  &lt;li&gt;ResNet101: Bottleneck, [3, 4, 23, 3]&lt;/li&gt;
  &lt;li&gt;ResNet152: Bottleneck, [3, 8, 36, 3]&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Architecture Name&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Repeated sub-module&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# repetitions in Layer 0&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# repetitions in Layer 1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# repetitions in Layer 2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;# repetitions in Layer 3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BasicBlock&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BasicBlock&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottleneck&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet101&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottleneck&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet152&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bottleneck&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;inspired by the philosophy of VGG nets. The convolutional layers mostly have 3×3 filters and follow two simple design rules:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;for the same output feature map size, the layers have the same number of filters; and&lt;/li&gt;
  &lt;li&gt;if the feature map size is halved, the number of filters is doubled so as to preserve the time com- plexity per layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. CVPR 2016. &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2]. Pytorch ResNet Implementation. &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">BasicBlock, Bottleneck</summary></entry><entry><title type="html">Computing Eigenvectors and Eigenvalues</title><link href="http://johnwlambert.github.io/eigs/" rel="alternate" type="text/html" title="Computing Eigenvectors and Eigenvalues" /><published>2019-04-25T07:01:00-04:00</published><updated>2019-04-25T07:01:00-04:00</updated><id>http://johnwlambert.github.io/eigs</id><content type="html" xml:base="http://johnwlambert.github.io/eigs/">&lt;h2 id=&quot;why-compute-eigenvalues-and-eigenvectors-what-are-they&quot;&gt;Why compute eigenvalues and eigenvectors? What are they?&lt;/h2&gt;

&lt;p&gt;It can be difficult to gain intuition about large 2-d arrays of numbers (matrices) by simply looking at their entries. By examining a matrix’s &lt;em&gt;eigenvalues&lt;/em&gt;, however, we can see a glimpse into the “heart” of the matrix. We’ll discuss QR iteration and Jacobi iterations, which can be used to compute a full set of eigenvalues and eigenvectors for a matrix. We’ll also discuss the Power Method, a method for computing a specific eigenvalue and corresponding eigenvector instead of the entire eigenvalue decomposition.&lt;/p&gt;

&lt;h2 id=&quot;what-are-eigenvalues-and-eigenvectors&quot;&gt;What are eigenvalues and eigenvectors?&lt;/h2&gt;

&lt;p&gt;In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Av &amp;= \lambda v \\
Av - \lambda v &amp;= 0 &amp; \mbox{rearrange terms} \\
(A-I\lambda)v &amp;= 0 &amp; \mbox{factor terms} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out that the eigenvalue problem is related to polynomials, through the determinant of a matrix. Consider that if &lt;script type=&quot;math/tex&quot;&gt;\exists v&lt;/script&gt; such that the equation above holds, i.e. &lt;script type=&quot;math/tex&quot;&gt;(A-I\lambda)v = 0&lt;/script&gt;, then the matrix &lt;script type=&quot;math/tex&quot;&gt;A - \lambda I&lt;/script&gt; must be singular because &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; belongs to the nullspace of &lt;script type=&quot;math/tex&quot;&gt;(A - \lambda I)&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;v \neq 0, v \in Null(A - \lambda I)&lt;/script&gt;. Since the nullspace is nonzero, then &lt;script type=&quot;math/tex&quot;&gt;A - \lambda I&lt;/script&gt; must be rank deficient (singular), otherwise zero would be the only vector in the nullspace. Singular matrices always have determinant zero, so we can find elements of the nullspace by searching for values of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that drive the determinant to zero. Since a determinant is a polynomial,  eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; are roots of the characteristic polynomial &lt;script type=&quot;math/tex&quot;&gt;P_{A}(\lambda)&lt;/script&gt; so we seek for values of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that will drive the polynomial to zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{A}(\lambda) = \mbox{det}(\lambda I - A)&lt;/script&gt;

&lt;p&gt;We arbitrarily set eigenvector length to be 1, i.e. &lt;script type=&quot;math/tex&quot;&gt;\|v\|_2=1&lt;/script&gt;, because scaling an eigenvector doesn’t change the eigenvalue-eigenvector relationship:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Av &amp;= \lambda v, v \neq 0 \\
A (\alpha v) &amp;= \lambda (\alpha v)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Having &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; eigenvalues is not equivalent to &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; &lt;em&gt;distinct&lt;/em&gt; eigenvalues. If a matrix is already diagonal, then the diagonal elements are just the eigenvalues (by virtue of determinant).&lt;/p&gt;

&lt;h2 id=&quot;similarity-transformations&quot;&gt;Similarity Transformations&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Similarity&lt;/em&gt; transformations are a specific class of transformations &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that preserve a matrix’s eigenvalues. The transformation must be performed on both the left and right-hand side, with &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T^{-1}&lt;/script&gt;. The theorem states that if &lt;script type=&quot;math/tex&quot;&gt;B = X^{-1}AX&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\lambda \{A\} = \lambda \{B\}&lt;/script&gt;. Then also &lt;script type=&quot;math/tex&quot;&gt;X A X^{-1} = B&lt;/script&gt;. This is because for square matrices &lt;script type=&quot;math/tex&quot;&gt;A,B \in \mathbf{R}^{n \times n}&lt;/script&gt;, we can break apart terms of the determinant &lt;script type=&quot;math/tex&quot;&gt;\mbox{det}(AB) = \mbox{det}(A) \mbox{det}(B)&lt;/script&gt;. Observe:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P_A(\lambda) &amp;= \mbox{det}(\lambda I − A)\\
P_B(\lambda) &amp;= \mbox{det}(\lambda I − X^{−1}AX) \\
			&amp;= \mbox{det}(\lambda X^{-1}X − X^{−1}AX) \\
			&amp;= \mbox{det}\Big(X^{−1}(\lambda I − A)X\Big) \\
			&amp;= \mbox{det}(X^{−1}) \mbox{ det}(\lambda I − A) \mbox{ det}(X) &amp; \mbox{can break apart terms of determinant} \\
			&amp;= \mbox{det}(X^{−1}) \mbox{ det}(X) \mbox{ det}(\lambda I − A) \\
			&amp;= \mbox{det}(X^{−1}X)  \mbox{ det}(\lambda I − A) \\
			&amp;= \mbox{det}(I)  \mbox{ det}(\lambda I − A) \\
			&amp;= 1 \cdot \mbox{det}(\lambda I − A) = P_A(\lambda)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;relationship-to-the-schur-decomposition&quot;&gt;Relationship to the Schur Decomposition&lt;/h2&gt;
&lt;p&gt;A related concept is the Schur Decomposition, which relies upon the Hermitian transpose, which involves taking the transpose of a matrix and then taking the complex conjugate of each entry, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^H = (\overline{Q})^T = \overline{(Q^T)}&lt;/script&gt;

&lt;p&gt;The Schur Decomposition exists for any square matrix &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbf{C}^{n \times n}&lt;/script&gt; (even if complex) &lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt; unitary &lt;script type=&quot;math/tex&quot;&gt;Q \in \mathbf{C}^{n \times n}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;Q^HQ = I&lt;/script&gt; and upper triangular &lt;script type=&quot;math/tex&quot;&gt;T \in \mathbf{C}^{n \times n}&lt;/script&gt; s.t.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q^HAQ &amp;= T = D + N \\
A &amp;= QTQ^H 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;what are find along the diagonal are the eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Note that the Schur decomposition provides a similarity transformation since &lt;script type=&quot;math/tex&quot;&gt;Q^{-1} = Q^T&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof by Induction of Schur&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prove by induction: 
Let &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbf{C}^{n \times n}&lt;/script&gt;
 (1) For any matrix of order &lt;script type=&quot;math/tex&quot;&gt;\leq n-1&lt;/script&gt;, Schur Decomp exists.
 (2) &lt;script type=&quot;math/tex&quot;&gt;Ax = \lambda x&lt;/script&gt;, with &lt;script type=&quot;math/tex&quot;&gt;\|x\|_2=1&lt;/script&gt;
 Let  &lt;script type=&quot;math/tex&quot;&gt;U^HU=I&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\underbrace{ U }_{n \times n} = \begin{bmatrix} \underbrace{x}_{1} &amp; \underbrace{U_1}_{n-1} \end{bmatrix} %]]&gt;&lt;/script&gt;
where &lt;script type=&quot;math/tex&quot;&gt;U^TU=I&lt;/script&gt;
    \item There are $n$ linearly independent vectors, each is orthogonal to each other, each one has unit norm
    \item We didn’t say how to compute $U_1$ yet
    \item We form $U^HAU$, and plug in the expression above, and multiply them all together as block-by-block multiplication&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
U^HAU &amp;= \begin{bmatrix}
x^H \\ U_1^H
\end{bmatrix} A \begin{bmatrix}
x &amp; U_1
\end{bmatrix} \\
&amp;= \begin{bmatrix}
x^H \\ U_1^H
\end{bmatrix}  \begin{bmatrix}
\lambda x &amp; A U_1
\end{bmatrix} \\
&amp;= \begin{bmatrix} 
\lambda &amp; x^H AU_1 \\
0 &amp; U_1^H A U_1
\end{bmatrix} = \begin{bmatrix} \lambda &amp; x^H A U_1 \\ 0 &amp; \tilde{U_1} \tilde{T} \tilde{U_1}^H
\end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;we know that &lt;script type=&quot;math/tex&quot;&gt;\lambda U_1^Hx&lt;/script&gt; since these must be orthogonal to each other
we look at dimensions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{ U_1^H }_{(n-1)\times n} \underbrace{A}_{n \times n} \underbrace{U_1}_{n \times (n-1)}&lt;/script&gt;

&lt;p&gt;ending up with &lt;script type=&quot;math/tex&quot;&gt;(n-1)\times (n-1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Assume the Schur decomposition for a particular part of the matrix now, assume we can make it upper triangular. Take Schur of lower-right-hand block. We can always write as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    Q^H A Q = T \\
    A = Q T Q^H
\end{aligned}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{U_1} \tilde{U_1}^H (U_1^H A U_1) \tilde{U_1} \tilde{U_1}^H = \tilde{U_1} T \tilde{U_1}^H&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
U^H A U =  \begin{bmatrix}
\lambda &amp; x^H A U_1 \\
0 &amp; \tilde{U_1} \tilde{T} \tilde{U_1}^H
\end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \tilde{U_1} \end{bmatrix} \begin{bmatrix}
\lambda &amp; x^H A U_1 \\
0 &amp; \tilde{T}
\end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \tilde{U_1} \end{bmatrix}^H %]]&gt;&lt;/script&gt;

&lt;p&gt;meaning that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A  = U \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \tilde{U_1} \end{bmatrix} \begin{bmatrix}
\lambda &amp; x^H A U_1 \\
0 &amp; \tilde{T}
\end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \tilde{U_1} \end{bmatrix}^H U^H %]]&gt;&lt;/script&gt;

&lt;p&gt;How are eigenvalues and Schur related? Eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; are the diagonal elements of &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. But what are the eigenvectors? Columns of &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; in general are not eigenvectors. We usually call the columns of &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; &lt;em&gt;Schur vectors&lt;/em&gt; instead. Suppose a column of &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; was an eigenvector. Then things break unless non-diagonal upper triangular entries were zero (if the matrix were &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; diagonal).&lt;/p&gt;

&lt;h2 id=&quot;diagonalizability&quot;&gt;Diagonalizability&lt;/h2&gt;

&lt;p&gt;Schur vector = eigen vector when matrix is diagonalizable
matrix is normal if $A^HA = AA^H$
Consider the real cage: a symmetric matrix $A^T=A$, then of course $A^TA = AA^T$
Compute the SVD of the following matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    A &amp;= \begin{bmatrix}
    2 &amp; 0 \\
    0 &amp; -4 \\
    0 &amp; 0 \\
    0 &amp; 0 
    \end{bmatrix} \\
    E &amp;= \begin{bmatrix}
    1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; -1 &amp;0 &amp; 0 \\
    0 &amp;  0 &amp; 1 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 1
    \end{bmatrix} \\
    \Pi E A P 
    &amp;=\Pi E \begin{bmatrix}
    2 &amp; 0 \\
    0 &amp; -4 \\ 
    0 &amp; 0 \\
    0 &amp; 0
    \end{bmatrix} P
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;\subsection{Jordan Decomposition}
\begin{itemize}
    \item $X^{-1}AX = J$
    \item Eigenvalues along the diagonal of $J_i$
    \item superdiagonal has ones
    \item A matrix is \textbf{defective} it has an eigenvalue of algebraic multiplicity $k$ iwth fewer thatn $k$ linearly independent eigenvectors
    \item Geometric vs. Algebraic multiplicity!
    \item geometric has to do with linear independence&lt;/p&gt;

&lt;h2 id=&quot;real-schur-decomposition&quot;&gt;Real Schur Decomposition&lt;/h2&gt;
&lt;p&gt;\begin{itemize}
    \item Example of QR decomposition: if $A \in \mathbbm{R}^{m \times n}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    A = Q \begin{bmatrix} R \\ 0 \end{bmatrix} \\
    Q \in \mathbbm{R}^{m \times m}, R \in \mathbbm{R}^{n \times n}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Complex Schur: if $A \in \mathbbm{C}^{n \times n}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    A = Q^H AQ = T \in \mathbbm{C}^{n \times n}
\end{aligned}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is upper triangular
Real Schur: $A \in \mathbbm{R}^{n \times n}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^T A Q = T&lt;/script&gt;

&lt;p&gt;this does NOT mean that &lt;script type=&quot;math/tex&quot;&gt;Q \in \mathbf{R}^{n \times n}&lt;/script&gt; and $T \in \mathbbm{R}^{n \times n}$
Real Schur: &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbbm{R}^{n \times n}&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt; orthogonal &lt;script type=&quot;math/tex&quot;&gt;Q \in \mathbbm{R}^{n \times n}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Q^TQ= I&lt;/script&gt; and block upper triangular &lt;script type=&quot;math/tex&quot;&gt;T \in \mathbf{R}^{n \times n}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Q^TAQ = T = \begin{bmatrix} R_{11} &amp; &amp; \\
                        &amp; R_{22} &amp; \\
                        &amp; &amp; R_{mm} \end{bmatrix} = 
                        \begin{bmatrix}
                        x &amp; &amp; &amp; &amp; &amp; \\
                          &amp;x&amp; &amp; &amp; &amp; \\
                          &amp; &amp;x&amp;x&amp; &amp; \\
                          &amp; &amp;x&amp;x&amp; &amp; \\
                          &amp; &amp; &amp; &amp;x&amp;x\\
                          &amp; &amp; &amp; &amp;x&amp;x
                        \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $R_{ii} \in \mathbbm{R}^{1 \times 1}$ or  $R_{ii} \in \mathbbm{R}^{2 \times 2}$ (blocks). where $Q = \mathbbm{R}^{n \times n}$ and $Q^TQ = I$. Real Schur Decomposition: might only be block diagonal.
    Cannot be exactly upper trangular ($1 \times 1$ blocks would be exactly diagonal). Even when $A$ is real, it can have complex eigenvalues. The $(2 \times 2)$ blocks explain the $(2 \times 2)$ eigenvalues, that explain $\alpha + i \beta$ and $\alpha - i \beta$.&lt;/p&gt;

&lt;p&gt;However, if &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is symmetric, i.e. &lt;script type=&quot;math/tex&quot;&gt;A=A^T&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;Q^TAQ&lt;/script&gt; is also symmetric, and we get diagonal &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;
We get the eigenvalues: schur decomposition becomes the symmetric eigenvalue decomposition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q^TAQ &amp;= D \\
QQ^TAQ &amp;= QD \\
A \begin{bmatrix} q_1 &amp; \cdots &amp; q_n \end{bmatrix} &amp;= \begin{bmatrix} q_1 &amp; \cdots &amp; q_n \end{bmatrix} \begin{bmatrix} d_{11} &amp; &amp; \\ &amp; d_{ii} &amp; \\ &amp; &amp; d_{nn} \end{bmatrix} \\
A q_i = q_i \begin{bmatrix} 0 &amp; \vdots &amp; d_{ii} &amp; \vdots &amp; 0 \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is assumed to be symmetric, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
(Q^TAQ)^T &amp;= Q^TA^TQ \\
          &amp;= Q^TAQ &amp; \mbox{since } A \mbox{ is symmetric } \\
          R^T &amp;= R \\
          \begin{bmatrix} R_{11} &amp; &amp; \\ &amp; R_{22} &amp; \\ &amp; &amp; R_{mm} \end{bmatrix} &amp;= \begin{bmatrix} R_{11}^T &amp; &amp; \\ &amp; R_{22}^T &amp; \\ &amp; &amp; R_{mm}^T \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the upper triangular matrix equals the lower triangular matrix&lt;/p&gt;

&lt;p&gt;Recall that if the roots are real, then they are equal to their complex conjugate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\lambda - \lambda_1)(\lambda - \lambda_2)&lt;/script&gt;

&lt;p&gt;where we have &lt;script type=&quot;math/tex&quot;&gt;(1 + i)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;(1-i)&lt;/script&gt;. If real matrix exists, it either has all real eigenvalues, or else it has pairs of complex with complex conjugate.&lt;/p&gt;

&lt;p&gt;Eigenvalues only for square matrices. You can only have \textit{generalized eigenvalues} for rectangular matrices&lt;/p&gt;

&lt;p&gt;If itself and complex conjugate are identical, then must be real&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A = A^H = (\overline{A})^T \in \mathbf{C}^{n \times n} \\
Ax &amp;= \lambda x, \|x\|_2^2 = 1 \leftrightarrow x^Hx = 1 \\
x^HAx &amp;= \lambda x^Hx = \lambda \\
(x^HAx)^H &amp;= x^HA^Hx = (\lambda)^H = \overline{\lambda} \\
\lambda &amp;= \overline{\lambda} \\
&amp; \lambda  \in \mathbf{R}^{1 \times 1}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Upper Hessenberg. allow one subdiagonal to be nonzero. 
\end{itemize}
\subsection{How can we compute eigenvalues?}
\begin{itemize}
    \item Recall that&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\begin{equation}
    Q_{\square} \cdots Q_1 A = \begin{bmatrix} R \\ 0 \end{bmatrix} \\
\end{equation}

Do a bunch of orthogonal transformations, hoping that $ \dots Q^TAQ = D$
\item If $Q^HAQ$ is triangular, than Schur decomposition computation cannot be done in a finite number of steps
\begin{equation}
    Q^HAQ = T
\end{equation}
where $T$ is upper triangular

\item However, if we want to do something less: allow to be upper hessenberg instead of upper triangular $T$
\begin{equation}
    Q^{\prime H}A Q^{\prime}
\end{equation}
then this can be computed in a finite number of steps
\item From polynomial of degree, no closed form, finite number of steps: polynomial root finding is equivalent to finding eigenvalues
\begin{equation}
    \lambda \{ A\} = \mbox{roots}\Big( \mbox{det}(\lambda I - A) = 0 \Big)
\end{equation}

\item We won't iterate infinitely number of times; we have some termination criteria for when we know that we have a pretty good approximation of the eigenvalues
\item 

You can't do it with just multiple similarity transformations in the usual way to get diagonal!
\begin{equation}
\begin{aligned}
H_1 \begin{bmatrix} 
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x 
\end{bmatrix} H_1^T 
\end{aligned}
\end{equation}

On the left, you will combine the 5 rows
On the right, you will combine the 5 columns, filling back in the zero
\item Less ambitious: allow one subdiagonal to be nonzero.
\item 

Find $P_1$ so that

\begin{equation}
    P_1 \begin{bmatrix} x \\ x \\ x \\ x \end{bmatrix} = \begin{bmatrix} x \\ 0 \\ 0 \\ 0 \end{bmatrix}
\end{equation}
\item 
\begin{equation}
    H_1 = \begin{bmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp; P_1  &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    \end{bmatrix} 
\end{equation}
\item 

\begin{equation}
 \begin{bmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp; P_1  &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    \end{bmatrix}    \begin{bmatrix} 
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x 
\end{bmatrix} \begin{bmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp; P_1^T  &amp;amp; \\
    0 &amp;amp;   &amp;amp;   &amp;amp;   &amp;amp; \\
    \end{bmatrix}  = \begin{bmatrix} 
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x 
\end{bmatrix}
\end{equation}
\item Zeros introduced from the previous step stay as zeros
\item Householder matrix is symmetric, so $P_1$ is symmetric, so $H_1 = H_1^T$
\item When $A$ is symmetric, meaning $A=A^T$, we obtain a tridiagonal matrix:

\begin{equation}
 H_3 H_2 H_1   \begin{bmatrix} 
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x 
\end{bmatrix} H_1^T H_2^T H_3^T
= \begin{bmatrix} 
x &amp;amp; x &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
x &amp;amp; x &amp;amp; x &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; x &amp;amp; x &amp;amp; x \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; x &amp;amp; x 
\end{bmatrix}
\end{equation}
\item
\begin{equation}
    H_{n-2} \cdots H_1 A H_1^T \cdots H_{n-2} = tridiagonal 
\end{equation}
Then do infinitely many more $Q_i$ to do :
\begin{equation}
 \cdots Q_1   H_{n-2} \cdots H_1 A H_1^T \cdots H_{n-2} Q_1^T \cdots = diagonal
\end{equation}
\item This is Hessenberg QR iteration
\item 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-qr-algorithm&quot;&gt;The QR Algorithm&lt;/h2&gt;

&lt;p&gt;Iterate:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$[Q,R] = qr(A)$, i.e. thus compute $A=QR$&lt;/li&gt;
  &lt;li&gt;$A^{\prime} := RQ$&lt;/li&gt;
  &lt;li&gt;$A = A^{\prime}$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;qr-algorithm-explained&quot;&gt;QR Algorithm Explained&lt;/h2&gt;

&lt;p&gt;Where is the similarity transformation? &lt;script type=&quot;math/tex&quot;&gt;A, A^{\prime}&lt;/script&gt; are similar&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A^{1} &amp;= Q^{(1)} R^{(1)} \\
 A^{2} &amp;=  R^{(1)} Q^{(1)} \\
  A^{2} &amp;=   Q^{(2)} R^{(2)} \\
  A^{3} &amp;=   R^{(2)}  Q^{(2)} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Consider what’s going on here: is &lt;script type=&quot;math/tex&quot;&gt;A^{(1)}$ connected to $A^{(2)}&lt;/script&gt; by a similarity transformation? yes!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A^{(2)} &amp;= R^{(1)} Q^{(1)}
        &amp;= \Big( Q^{(1)}^T A^{(1)} \Big) Q^{(1)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;because&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A^{(1)} &amp;= Q^{(1)} R^{(1)} \\
(Q^{(1)})^T A^{(1)} &amp;= (Q^{(1)})^T Q^{(1)} R^{(1)} \\
(Q^{(1)})^T A^{(1)} &amp;= R^{(1)} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;so we can substitute this in above! We will use Givens’ rotations to get there!&lt;/p&gt;

&lt;p&gt;Nonsymmetric matrix eigenvalue decomposition -&amp;gt; existence is difficult to talk about, we will not discuss this! So we just focus on the symmetric case only.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
G_{45} G_{23} \begin{bmatrix} c &amp; s  &amp; &amp; &amp; \\
                             -s &amp; c &amp; &amp; &amp; \\
                                &amp;   &amp; &amp;I_3 &amp; \\
                                &amp;   &amp; &amp;  &amp; 
                                \end{bmatrix}T %]]&gt;&lt;/script&gt;

&lt;p&gt;get upper tri-diagonal&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T^{\prime} = (\textrm{upper tria-diagonal}) G_{12}^T G_{23}^T \cdots G_{n-1,n}^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
T=  \begin{bmatrix}
    x &amp; x &amp;  &amp;   &amp; \\
    x &amp; x &amp; x &amp;   &amp; \\
      &amp; x &amp; x &amp; x &amp;  \\
      &amp;   &amp; x &amp; x &amp; x \\
      &amp;   &amp;   &amp; x &amp; x \\
\end{bmatrix} \\
G_{12} T = \begin{bmatrix}
    x &amp; x &amp; + &amp;   &amp; \\
    0 &amp; x &amp; x &amp;   &amp; \\
      &amp; x &amp; x &amp; x &amp;  \\
      &amp;   &amp; x &amp; x &amp; x \\
      &amp;   &amp;   &amp; x &amp; x \\
\end{bmatrix} \\
T^{\prime} = G_{45} G_{34} G_{23} G_{12}   G_{12} T = \begin{bmatrix}
    x &amp; x &amp; + &amp;   &amp; \\
    0 &amp; x &amp; x &amp; +  &amp; \\
     0 &amp; 0 &amp; x &amp; x &amp; + \\
      &amp;   &amp; 0 &amp; x &amp; x \\
      &amp;   &amp;   &amp; 0 &amp; x \\
\end{bmatrix} \\
T^{\prime} = G_{45} G_{34} G_{23} G_{12}   G_{12} T G_{12}^T G_{23}^T G_{34}^T G_{45}^T \\
=
\begin{bmatrix}
    x &amp; x &amp; + &amp; +  &amp; + \\
    + &amp; x &amp; x &amp; +  &amp; + \\
     0 &amp; + &amp; x &amp; x &amp; + \\
      &amp;   &amp; + &amp; x &amp; x \\
      &amp;   &amp;   &amp; + &amp; x \\
\end{bmatrix} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;But we know that &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is symmetric! so &lt;script type=&quot;math/tex&quot;&gt;T^{\prime} = RQ = Q^TTQ&lt;/script&gt;. Tridiagonalize first, and then iterate&lt;/p&gt;

&lt;h2 id=&quot;the-shifted-qr-algorithm&quot;&gt;The Shifted QR Algorithm&lt;/h2&gt;

&lt;p&gt;Iterate:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Q,R = qr(T - \lambda I)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;T = RQ + \lambda I&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;justification-for-qr-w-shift&quot;&gt;Justification for QR w/ Shift&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T^{(1)} - \lambda I = Q^{(1)} R^{(1)}&lt;/script&gt;

&lt;p&gt;\begin{figure}[!h]
       \centering
       \includegraphics[width=\linewidth]{qr_with_shift_derivation.jpg}
       \caption{WHY DOES THIS HOLD TRUE?}
       \label{fig:my_label}
   \end{figure}
   \item&lt;/p&gt;

&lt;p&gt;\begin{equation}
       T - \lambda I = \cdots 
   \end{equation}
   \item We are not changing the eigenvalues: $T^{\prime}, T$ related by a similarity transformation
   \item If we have a matrix $A$, and we don’t know anything about if it is symmetric, then we can convert it to Upper Hessenberg matrix $H$.
   \item Then we do 
   \begin{equation}
   \begin{aligned}
       (Q,R) &amp;amp;\leftarrow qr(H-\lambda I) &lt;br /&gt;
       H &amp;amp;\leftarrow RQ + \lambda I
    \end{aligned}
   \end{equation}
   \item In terms of complexity:
   Now this takes only $6n + 6(n-1)$, so  better than $O(n^3)$ just $O(n^2)$
   \item 
   \begin{equation}
       6 \sum\limits_{i=2}^n i
   \end{equation}
   \item When the matrix is symmetric, since we can turn it into tridiagonal, it is even easier. We have to do this $(n-1)$ times. That takes $O(n)$ flops. We save a lot of computational complexity.
\end{itemize}&lt;/p&gt;

&lt;h2 id=&quot;deflation&quot;&gt;Deflation&lt;/h2&gt;
&lt;p&gt;\subsection{Deflation}
\begin{itemize}
\item 
\begin{equation}
\begin{aligned}
A = \begin{bmatrix} A_{11}| &amp;amp; &amp;amp; A_{12} &amp;amp; &lt;br /&gt;
                     —   &amp;amp; – &amp;amp; —— &amp;amp;  &lt;br /&gt;
                      \hspace{5mm} | &amp;amp; &amp;amp;  &amp;amp;  &lt;br /&gt;
                       0 \hspace{3mm} | &amp;amp; &amp;amp; A_{22} &amp;amp;  &lt;br /&gt;
                       \hspace{5mm} | &amp;amp; &amp;amp; &amp;amp;  &lt;br /&gt;
                        \end{bmatrix}
    \lambda {A} = \bigcup_{i=1}^4 \lambda { A_{ii} }
\end{aligned}
\end{equation}
You can just pad above the diagonal components, set everything below them to be zero
\end{itemize}
\subsection{Deflation: The Symmetric Case}
\begin{itemize}
    \item $A=A^T \in \mathbbm{R}^{n \times n}$&lt;/p&gt;

&lt;p&gt;In the middle of QR, check if anything in the off-diagonal became tiny – means one eigenvalue is already computed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Av &amp;= \lambda v (v \neq 0) \\
Av - \lambda v &amp;= 0 \\
(A-\lambda I) v &amp;= 0 \\
\begin{bmatrix} 
A_{11} - \lambda &amp; &amp; \\
                &amp; \ddots &amp; \\
                 &amp; &amp; a_{nn} - \lambda
\end{bmatrix} v &amp;= 0
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Start with &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H =  \begin{bmatrix}
x &amp; x &amp; x &amp; x &amp; x \\
+ &amp; x &amp; x &amp; x &amp; x \\
  &amp; + &amp; x &amp; x &amp; x \\
  &amp;   &amp; + &amp; x &amp; x \\
  &amp;   &amp;   &amp; + &amp; x \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Shift: Then you will get singular with $H - \mu I$, meaning $\mu$ is an eigenvalue&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H - \mu I =  \begin{bmatrix}
x &amp; x &amp; x &amp; x &amp; x \\
+ &amp; x &amp; x &amp; x &amp; x \\
  &amp; + &amp; x &amp; x &amp; x \\
  &amp;   &amp; + &amp; x &amp; x \\
  &amp;   &amp;   &amp; + &amp; x \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;After 1 iteration, you will get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\begin{equation}
    H^+ = RQ + \mu I = \begin{bmatrix}
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp;   &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp;   &amp;amp;   &amp;amp; 0 &amp;amp; \mu \\
    \end{bmatrix}
\end{equation}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You need to shift with a very good estimate of the eigenvalue (numerically the bottom row – one value might be tiny, and other close to eigenvalue). Role of shift is important!  Note that &lt;script type=&quot;math/tex&quot;&gt;H - \mu I = QR&lt;/script&gt;
 If &lt;script type=&quot;math/tex&quot;&gt;\mu \in \lambda\{H\}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;H-\mu I&lt;/script&gt; is singular. This implies that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
det(H - \mu I) = 0 \\
\mbox{det}(R) = 0, \mbox{ because } \mbox{det}(Q)=1$
\end{aligned}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\item is $r_{nn}= 0$?? Yes, it turns out it is!
\item By Givens rotations, for 2-vector must get zero and non-zero element to preserve norm magnitude
\begin{equation}
    \begin{bmatrix} c &amp;amp; s \\ -s &amp;amp; c \end{bmatrix} \begin{bmatrix} x &amp;amp; + \end{bmatrix} = \begin{bmatrix} + \\ 0 \end{bmatrix}
\end{equation}
\item All diagonal elements except for the last one must be nonzero! (because we shifted subdiagonal above into the diagonal), so all diagonal elements are now nonzero except for $r_{nn}$. In Hessenberg, assumption is that entire subdiagonal is nonzero --&amp;gt; otherwise you could have decoupled two blocks and that would have been good news
\item How do you extract the last row of a matrix? Multiply by $e_n^T$ on the left hand side
\begin{equation}
\begin{aligned}
    e_n^T H^{\prime} &amp;amp;= e_n^T RQ + e_n^T \mu I \\
                    &amp;amp;= \begin{bmatrix}0 &amp;amp; \cdots &amp;amp; 0\end{bmatrix} Q + \begin{bmatrix}0 &amp;amp; \cdots &amp;amp; 0 &amp;amp;  \mu \end{bmatrix} \\ &amp;amp;= \begin{bmatrix}0 &amp;amp; \cdots &amp;amp; 0 &amp;amp;  \mu \end{bmatrix}
\end{aligned}
\end{equation}
\item After one iteration of QR with shift, then that eigenvalue will appear as the last element $r_{nn}$. Rest of that row will be zero. From that point on, instead of dealing with the $n \times n$ problem, deal with an eigenvalue problem that is one less: $(n-1) \times (n-1)$.
\item As soon as you get down to a $(1 \times 1)$ block, that value is an eigenvalue
\item You can talk about all of these in terms of tridiagonal matrices (if matrix is symmetric) instead of Upper Hessenberg
\item This is the best algorithm for computing eigenvalues! Jacobi is easier to understand, of course. 
\item 

\begin{equation}
\begin{aligned}
    (Q,R) \leftarrow qr(H - \mu I) \\
    H^{\prime} \leftarrow RQ + \mu I
\end{aligned}
\end{equation}

\item If pretty close then, you will get

\begin{equation}
   H^{\prime} =  \begin{bmatrix}
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp;   &amp;amp; x &amp;amp; x &amp;amp; x \\
      &amp;amp;   &amp;amp;   &amp;amp; \varepsilon &amp;amp; \mu \\
    \end{bmatrix}
\end{equation}
After one more iteration, you will get $\varepsilon^2$, then $\varepsilon^4$, so you only need to do a few iterations. Then it's time to decouple!
\item The


\item You can diagonalize in one step and get the SVD! With rotations on both side, cosine and sine of different angles. 

\begin{equation}
    \begin{bmatrix}
    c_1 &amp;amp; - s_1 \\
    s_1 &amp;amp; c_1
    \end{bmatrix}
    \begin{bmatrix}
    a_{11} &amp;amp; a_{12} \\
    a_{21} &amp;amp; a_{22}
    \end{bmatrix}
    \begin{bmatrix}
    c_2 &amp;amp; - s_2 \\
    s_2 &amp;amp; c_2
    \end{bmatrix} = 
    \begin{bmatrix}
    x_1 &amp;amp; 0 \\
    0 &amp;amp; x_2
    \end{bmatrix}
\end{equation}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Consider symmetric &lt;script type=&quot;math/tex&quot;&gt;A=A^T \in \mathbbm{R}^{n \times n}&lt;/script&gt;: initial tridiagonalization. To compute $UAU^T = M_{tridiagonal}$, then multiplying on left and right  is just the double the work of multiplying on left.
    \item For non-symmetric case $A \in \mathbbm{R}^{n \times n}$: Start with moving to upper Hessenberg: $UAU^T=M_{upper_hessenberg}$
    \item Keep doing shifting and other steps etc.
    \item If no element on th subdiagonal of $H$ is zero, and $H-\mu I$ is singular, then $(H_+)_{n, n-1}=0$&lt;/p&gt;

&lt;p&gt;We can decouple since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda \{A\} = \big\cup \cdots&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A &amp;= A^T \in \mathbbm{R}^{n \times n} \\
UAU^T &amp;= T = Q^TDQ \\
QUAU^TQ^T &amp;= D
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\item In theory, have to do in infinitely many times. In practical algorithm, run in $2n$ or $3n$ iterations. Every eigenvalue requires 2 or 3 iterations to reveal. And we have $n$ eigenvalues
\item Each iteration takes only $O(n)$ flops
\item The role of shift is very important. With exact eigenvalue, would be revealed in a single iteration. But we can only get good estimates.
\item Two methods for estimation: (1) is $\lambda = T_{n,n}$
\begin{equation}
   T = \begin{bmatrix}
    x &amp;amp; x &amp;amp;  &amp;amp; &amp;amp; &amp;amp; \\
    x &amp;amp; x &amp;amp; x&amp;amp; &amp;amp; &amp;amp; \\
      &amp;amp; x &amp;amp; x&amp;amp;x&amp;amp; &amp;amp; \\
      &amp;amp;   &amp;amp; x&amp;amp;x&amp;amp;x&amp;amp; \\
      &amp;amp;   &amp;amp;  &amp;amp;x&amp;amp;x&amp;amp;x \\
      &amp;amp; &amp;amp; &amp;amp; &amp;amp; x &amp;amp; \mathbf{x}\\
    \end{bmatrix}
\end{equation}
\item (2) Other method is just using eigenvalue of last $(2 \times 2)$ submatrix (the Wilkinson shift). No foundational theoretical justification, largely experimental justification
\item When you accumulate orthogonal transformations, then it is $O(n^3)$, get eigenvectors
\item To get only $R$ factor (without accumulating orthogonal transformations), then it is just $O(n^2)$
\item  \end{itemize} \subsection{Jacobi Algorithm} \begin{itemize}
\item Jacobi algorithm is slower than QR w/ shift, but numerically better and easy to parallelize
\item 

\begin{equation}
\begin{aligned}
   A &amp;amp;=  \begin{bmatrix}
    x &amp;amp; y \\
    y &amp;amp; z
    \end{bmatrix} \in \mathbbm{R}^{2 \times 2} \\    \begin{bmatrix}
    c &amp;amp; -s \\
    s &amp;amp; c
    \end{bmatrix} \begin{bmatrix}
    x &amp;amp; y \\
    y &amp;amp; z
    \end{bmatrix} \begin{bmatrix}
    c &amp;amp; s \\
    -s &amp;amp; c
    \end{bmatrix} &amp;amp;= \begin{bmatrix} x &amp;amp; 0 \\ 0 &amp;amp; x \end{bmatrix} \\
    G A G^T  &amp;amp;= ...
\end{aligned}
\end{equation}
\item 

\begin{equation}
    y(c^2 - s^2) + (x-z) cs = 0
\end{equation}
\item Becomes quadratic equation in tangent
\item Assume $y$ nonzero, otherwise we wouldn't have to do anything
\item choose any $2\times 2$ submatrix,

\begin{equation}
    \begin{bmatrix} 
    \square &amp;amp; x &amp;amp; \square &amp;amp; x &amp;amp; x \\
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
    \square &amp;amp; x &amp;amp; \square &amp;amp; x &amp;amp; x \\
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
    x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
    \end{bmatrix}
\end{equation}
\item Find $(c,s)$ such that
\begin{equation}
     \begin{bmatrix}
    c &amp;amp; -s \\
    s &amp;amp; c
    \end{bmatrix} \begin{bmatrix}
    a_{11} &amp;amp; a_{13} \\
    a_{31} &amp;amp; a_{33}
    \end{bmatrix} \begin{bmatrix}
    c &amp;amp; s \\
    -s &amp;amp; c
    \end{bmatrix} &amp;amp;= \begin{bmatrix} x &amp;amp; 0 \\ 0 &amp;amp; x \end{bmatrix} \\
\end{equation}
\item embed it into $5 \times 5$ identity matrix, as follows:
\begin{equation}
\begin{aligned}
 J_{13} A J_{13}^T &amp;amp;= \cdots \\
    \begin{bmatrix}
   c &amp;amp; &amp;amp; -s &amp;amp; &amp;amp; \\
    &amp;amp; 1 &amp;amp; &amp;amp; &amp;amp; \\
   s &amp;amp; &amp;amp; c &amp;amp; &amp;amp; \\
    &amp;amp; &amp;amp; &amp;amp; 1 &amp;amp; \\
    &amp;amp; &amp;amp; &amp;amp; &amp;amp; 1 \\
    \end{bmatrix} \begin{bmatrix}
   a_{11} &amp;amp; x &amp;amp; a_{13} &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   a_{31} &amp;amp; x &amp;amp; a_{33} &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x\\
    \end{bmatrix} &amp;amp;= \begin{bmatrix}
   x &amp;amp; x &amp;amp; 0 &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x\\
    \end{bmatrix}\\
     J_{24}   J_{13} A J_{13}^T J_{24}^T &amp;amp;= \cdots \\
     J_{24} \begin{bmatrix}
   x &amp;amp; x &amp;amp; 0 &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   0 &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x \\
   x &amp;amp; x &amp;amp; x &amp;amp; x &amp;amp; x\\
    \end{bmatrix} J_{24}^T &amp;amp;= \cdots \\
\end{aligned}
\end{equation}


\item Zeros introduced are not destroyed...
\item Don't purely count zeros. Instead, count mass of off-diagonal part. Thus, our measure of progress is defined as $\textit{off}$ of off-diagonal mass:
\begin{equation}
  \Big(off(A) \Big)^2 = \sum\limits_{j=1}^n \sum\limits_{i=1, i \neq j}^n a_{ij}^2 = \|A\|_F^2 - \sum\limits_{i=1}^n a_{ii}^2
\end{equation}
\item Eventually want mass in off-diagonal part to be very small
\item Jacobi decreases $off(A)$ as you iterate, and then it can converge
\item Rotation does not change Frobenius norm, so must be zero-sum game between off-diagonal and diagonal
\item We chose rotation so that off-diagonal part gives all of its mass to on-diagonal part
\item $off(\cdot)$ never decreases:

\begin{equation}
\begin{aligned}
B &amp;amp;= J_{23} A J_{23}^T \\
off^2(B) &amp;amp;\leq off^2(A)
\end{aligned}
\end{equation}
\item Classical Jacobi method. Eliminate largest value in off-diagonal team
\item not explicitly trying to add zeros, combined with someone else from off-team, just gets distributed across off-team

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{jacobi_figure_choose_largest.jpg}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\item At most,
\begin{equation}
\begin{aligned}
    off^2(A^{(i+1)}) \leq \frac{n(n-1)-2}{n(n-1)} off^2(A^{(i)}) \\
    off^2(A^{(i+1)}) \leq \alpha off^2(A^{(i)}) \\
\end{aligned}
\end{equation}
which will go to zero
\item This is a sequence
\begin{equation}
    A^{(1)} \rightarrow A^{(2)} \rightarrow \cdots
\end{equation}
\item $\alpha$ comes from choosing the largest element every time
\item $|\alpha| &amp;lt; 1$

\begin{equation}    \underset{i \rightarrow \infty}{\mbox{lim}} off^2(A^{(i)}) = 0    \end{equation} \end{itemize}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-qr-iteration&quot;&gt;The QR Iteration&lt;/h2&gt;
&lt;p&gt;The roots of a polynomial &lt;script type=&quot;math/tex&quot;&gt;Ax = \lambda x, det(\lambda I − A) = 0&lt;/script&gt;, can not be solved in finite number of steps if the polynomial order is bigger than 5.&lt;/p&gt;

&lt;p&gt;With QR iteration and Jacobi iterations, we can compute the complete symmetric eigenvalue decomposition &lt;script type=&quot;math/tex&quot;&gt;A=Q \Lambda Q^T&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A &amp;= Q \Lambda Q^T \\
AQ &amp;= Q \Lambda \\
A \begin{bmatrix} q_1 &amp; \cdots &amp; q_n \end{bmatrix} &amp;= \begin{bmatrix} q_1 &amp; \cdots &amp; q_n \end{bmatrix} \begin{bmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;qr-iteration-with-shift&quot;&gt;QR Iteration with Shift&lt;/h2&gt;

&lt;h2 id=&quot;jacobi-iteration&quot;&gt;Jacobi Iteration&lt;/h2&gt;
&lt;p&gt;Better for parallelization than QR iteration. Slower than QR in practice (~5x slower), although theoretically we have to iterate infinitely many times for both algorithms.&lt;/p&gt;

&lt;h2 id=&quot;the-power-method&quot;&gt;The Power Method&lt;/h2&gt;
&lt;p&gt;The basic power method computes the largest eigenvalue &lt;script type=&quot;math/tex&quot;&gt;\lambda_1&lt;/script&gt; and corresponding eigenvector &lt;script type=&quot;math/tex&quot;&gt;v_1&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;|\lambda_1| \geq | \lambda_2| \geq \cdots \geq | \lambda_n|&lt;/script&gt;. It will only work for a specific type of matrix.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Cannot have two shared largest eigenvalues – one must be dominant, e.g. $$&lt;/td&gt;
      &lt;td&gt;\lambda_1&lt;/td&gt;
      &lt;td&gt;&amp;gt;&lt;/td&gt;
      &lt;td&gt;\lambda_2&lt;/td&gt;
      &lt;td&gt;$$.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We must start with an initial guess &lt;script type=&quot;math/tex&quot;&gt;q^{(0)}&lt;/script&gt;, which can be random. We wish to compute &lt;script type=&quot;math/tex&quot;&gt;\lambda_1&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbf{R}^{n \times n}&lt;/script&gt; and &lt;strong&gt;we make the assumption&lt;/strong&gt; that &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; has &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; linearly independent eigenvectors. For symmetric matrices, we know this is the case, since a decomposition then exists &lt;script type=&quot;math/tex&quot;&gt;A=Q \Lambda Q^T&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; contains the eigenvectors. Thus, we can write &lt;script type=&quot;math/tex&quot;&gt;q^{(0)}&lt;/script&gt; as a linear combination of all &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; eigenvectors &lt;script type=&quot;math/tex&quot;&gt;x_1, \cdots, x_n&lt;/script&gt;, since the eigenvectors can serve as a basis for &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
q^{(0)} &amp;= \alpha_1 x_1 + \cdots + \alpha_n x_n &amp; \mbox{as linear combination of n linearly independent eigenvectors} \\
z^{(1)} &amp;= A q^{(0)} = A \alpha_1 x_1 + \cdots + A \alpha_n x_n &amp; \mbox{ multiply all terms by } A \\
z^{(1)} &amp;= A q^{(0)} = \alpha_1 A x_1 + \cdots + \alpha_n A x_n &amp; \alpha \mbox{ is a scalar, so we can place it in anywhere} \\
z^{(1)} &amp;= A q^{(0)} = \alpha_1 \lambda_1 x_1 + \cdots + \alpha_n \lambda_n x_n &amp;  \mbox{ because each } x_i \mbox{ is an eigenvector} \\
q^{(1)} &amp;= A q^{(0)} / \|z^{(1)}\|_2 &amp;  \mbox{normalize}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The only reason we normalize here is because the vector size may grow or shrink drastically, so we can keep it stable by constraining it to be a unit vector. Thus, we divide by its magnitude. We iterate over and over again. Normalization won’t change the direction of the vector (only the magnitude), so for the sake of a convergence argument, we will ignore it:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
A (Aq^{(0)}) &amp;= A \Big( \alpha_1 \lambda_1 x_1 + \alpha_2 \lambda_2 x_2 + \alpha_3 \lambda_3 x_3 + \cdots + \alpha_n \lambda_n x_n \Big) \\
A (Aq^{(0)}) &amp;=  \alpha_1 \lambda_1 Ax_1 + \alpha_2 \lambda_2 A x_2 + \cdots + \alpha_n \lambda_n A x_n &amp; \mbox{reorder terms} \\
A (Aq^{(0)}) &amp;= \alpha_1 \lambda_1^2 x_1 + \alpha_2 \lambda_2^2 x_2 + \cdots + \alpha_n \lambda_n^2 x_n &amp; \mbox{each } x_i \mbox{ is an eigenvector} \\
A^{k} q^{(0)}&amp;= \alpha_1 \lambda_1^k x_1 + \alpha_2 \lambda_2^k x_2 + \cdots + \alpha_n \lambda_n^k x_n  &amp; \mbox{apply it k times} \\
\frac{A^{k} q^{(0)} }{\lambda_1^k} &amp;= \frac{\alpha_1 \lambda_1^k x_1 + \alpha_2 \lambda_2^k x_2 + \cdots + \alpha_n \lambda_n^k x_n}{\lambda_1^k}  &amp; \mbox{play with length -- divide both sides by } \lambda_1^k
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Big|\frac{\lambda_i}{\lambda_1}\Big| &lt; 1, \mbox{ for } i=2, \cdots, n %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, in the limit, the left-hand side becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{k \rightarrow \infty}{\mbox{lim}} \frac{A^k q^{(0)}}{\lambda_1^k} = \alpha_1 x_1&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; is our eigenvector, giving us the direction of the first leading eigenvector. However, if &lt;script type=&quot;math/tex&quot;&gt;\alpha_1&lt;/script&gt; is zero, then we will produce the zero vector as an eigenvector, so we start all over again with a different choice of &lt;script type=&quot;math/tex&quot;&gt;q^{(0)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Once we have the eigenvector, we can reclaim its eigenvalue since we can convert it into a quadratic form as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Ax &amp;= \lambda x, x \neq 0 &amp; \\
x^TAx &amp;= \lambda x^Tx &amp; \mbox{left multiply by } x^T \\
\frac{x^TAx}{x^Tx} &amp;= \frac{\lambda x^Tx}{x^Tx} &amp; \mbox{divide by scalar to get the Rayleigh quotient} \\
\lambda &amp;= \frac{x^TAx}{x^Tx} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Theoretically, we must iterate infinitely many times, but numerically, we don’t have to. We can halt when our eigenvalue, eigenvector pair satisfy the desired relationship sufficiently well:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Ax^{(k)} &amp;= \lambda^{(k)} x^{(k)} &amp; \\
| A x^{(k)} - \lambda^{(k)} x^{(k)}| &amp;\leq \varepsilon &amp; \mbox{rearrange terms}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt; is a desired tolerance.&lt;/p&gt;</content><author><name></name></author><summary type="html">Power iteration, QR iteration, QR with shift, Jacobi iteration.</summary></entry></feed>