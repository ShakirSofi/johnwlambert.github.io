<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://johnwlambert.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://johnwlambert.github.io/" rel="alternate" type="text/html" /><updated>2018-11-28T12:28:10-05:00</updated><id>http://johnwlambert.github.io/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">Piazza Answers</title><link href="http://johnwlambert.github.io/2018/11/28/piazza-answers.html" rel="alternate" type="text/html" title="Piazza Answers" /><published>2018-11-28T00:00:00-05:00</published><updated>2018-11-28T00:00:00-05:00</updated><id>http://johnwlambert.github.io/2018/11/28/piazza-answers</id><content type="html" xml:base="http://johnwlambert.github.io/2018/11/28/piazza-answers.html">&lt;h2 id=&quot;visualizing-cnns-via-deconvolution&quot;&gt;Visualizing CNNs via deconvolution&lt;/h2&gt;

&lt;p&gt;It concerns slide 4 in this presentation.
http://places.csail.mit.edu/slide_iclr2015.pdf&lt;/p&gt;

&lt;p&gt;I’m having a hard time understanding the deconvolution part of the slide.&lt;/p&gt;

&lt;p&gt;Great question. You can see a presentation from Matt Zeiler on the method here: https://www.youtube.com/watch?v=ghEmQSxT6tw. He summarizes his method from about minutes 8:40-20:00 in the presentation.&lt;/p&gt;

&lt;p&gt;Suppose there is some layer you want to visualize. Zeiler feeds in 50,000 ImageNet images through a learned convnet, and gets the activation at that layer for all of the images. Then they feed in the highest activations into their deconvolutional network.&lt;/p&gt;

&lt;p&gt;Their inverse network needs to make max pooling and convolution reversible. So they use unpooling and deconvolution to go backwards. This is how they can visualize individual layers.&lt;/p&gt;

&lt;h2 id=&quot;backprop-per-layer-equations&quot;&gt;Backprop per-layer equations&lt;/h2&gt;

&lt;p&gt;I would expect the quiz to include content from the lecture slides and what was discussed in lecture. Professor Hays didn’t go into detail how to perform backprop through every single layer, so I wouldn’t expect a detailed derivation for each layer. You can find the slides on backprop here.&lt;/p&gt;

&lt;p&gt;https://www.cc.gatech.edu/~hays/compvision/lectures/20.pdf&lt;/p&gt;

&lt;p&gt;If you’re interested in digging deeper into the equations, some basic intuition for derivatives can be found &lt;a href=&quot;http://cs231n.github.io/optimization-2/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture04.pdf&quot;&gt;here&lt;/a&gt;. The chain rules binds together the derivatives of each layer. For two simple examples, the (sub)gradient of a max of two arguments is 1 for the larger argument, 0 for the smaller argument. The derivative for the addition of two arguments is 1 with respect to each argument.&lt;/p&gt;

&lt;h2 id=&quot;fc-vs-conv2d-maxpool-linear&quot;&gt;FC vs. CONV2D MAXPOOL LINEAR&lt;/h2&gt;

&lt;p&gt;what are the reasons for not using fully connected layers and using conv2d+maxpool+linear layer combinations ? Is it only because the number in fully connected layers are very large for image processing to prohibits from learning the weights fast enough ? #vvm&lt;/p&gt;

&lt;p&gt;John Lambert
 John Lambert 18 hours ago We use convolutions and hierarchies of processing steps since we showed earlier in the course that this is the most effective way to work with image (gridded data).&lt;/p&gt;

&lt;p&gt;We don’t use fully-connected layers at every step because there would be way to many learnable parameters to learn without overfitting, and the memory size would be enormous. The fully-connected layers act as the classifier on top of the automatically-learned features.&lt;/p&gt;

&lt;p&gt;From CS 231N at Stanford:
http://cs231n.github.io/convolutional-networks/&lt;/p&gt;

&lt;p&gt;Regular Neural Nets. …Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.&lt;/p&gt;

&lt;p&gt;Regular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32&lt;em&gt;32&lt;/em&gt;3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200&lt;em&gt;200&lt;/em&gt;3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.&lt;/p&gt;

&lt;p&gt;3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depthhere refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension.&lt;/p&gt;

&lt;h2 id=&quot;why-does-batch-norm-help&quot;&gt;Why does Batch Norm help?&lt;/h2&gt;
&lt;p&gt;John: (adding this) I wouldn’t expect batch norm to help by 15% on the simple network. Maybe by about 4% on the SimpleNet. Are you confounding the influence of normalization, more layers, data augmentation, and dropout with the influence of batch norm?&lt;/p&gt;

&lt;p&gt;If you train your network to learn a mapping from X-&amp;gt;Y, and then the distribution of X changes, then you might need to retrain your network so that it can understand the changed distribution of X. (Suppose you go learned to classify black cats, and then suddenly you need to classify colored cats, example here).
https://www.youtube.com/watch?v=nUUqwaxLnWs&lt;/p&gt;

&lt;p&gt;Batch Norm speeds up learning. This is because it reduces the amount that the distribution of hidden values moves around. This is often called “reducing internal covariate shift”.  Since all the layers are linked, if the first layer changes, then every other layer was dependent on those values being similar to before (not suddenly huge or small).&lt;/p&gt;

&lt;p&gt;General ideas why Batch Norm helps:&lt;/p&gt;

&lt;p&gt;Improves gradient flow through the network (want variance=1 in your layers, avoid exponentially vanishing or exploding dynamics in both the forward and the backward pass)
Allows higher learning rates
Reduces the strong dependence on initialization
Acts as a form of regularization because it adds a bit of noise to training (uses statistics from random mini-batches to normalize) and it slightly reduces the need for dropout&lt;/p&gt;

&lt;p&gt;https://arxiv.org/pdf/1805.11604.pdf
Others say that BatchNorm makes the optimization landscape significantly smoother. They theorize that it reduces the Lipschitz constant of the loss function, meaning the loss changes at a smaller rate and the magnitudes of the gradients are smaller too.
https://en.wikipedia.org/wiki/Lipschitz_continuity&lt;/p&gt;

&lt;p&gt;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
In the AlexNet paper, the authors mentioned that local response normalization aids generalization, meaning that the network can accurately understand new examples.&lt;/p&gt;

&lt;h2 id=&quot;where-to-add-dropout&quot;&gt;Where to add Dropout?&lt;/h2&gt;
&lt;p&gt;Dropout was used in older convolutional network architectures like AlexNet and VGG. Dropout should go in between the fully connected layers (also known as 1x1 convolutions). Dropout doesn’t seem to help in the other convolutional layers.  It’s not exactly clear why.&lt;/p&gt;

&lt;p&gt;One hypothesis is that you only need to avoid overfitting in the layers with huge amounts of parameters (generally fully-connected layers), and convolutional layers usually have fewer parameters (just a few shared kernels) so there’s less need to avoid overfitting there. Of course, you could have the same number of parameters in both if you had very deep filter banks of kernels, but usually the max filter depth in VGG is only 512 and 384 in AlexNet.&lt;/p&gt;

&lt;p&gt;ResNet, a more modern convnet architecture, does not use dropout but rather uses BatchNorm.&lt;/p&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;p&gt;Could somebody post answers for Lecture-4 slides 12,13 and Lecture-5 slides 6?&lt;/p&gt;

&lt;p&gt;Lecture 4 Slide 12:&lt;/p&gt;

&lt;p&gt;(2) this is forward difference derivative approximation.
https://en.wikipedia.org/wiki/Finite_difference#Forward,_backward,_and_central_differences&lt;/p&gt;

&lt;p&gt;Lecture 4 Slide 13&lt;/p&gt;

&lt;p&gt;a) y-derivatives of image with Sobel operator&lt;/p&gt;

&lt;p&gt;b) derivative of Gaussian computed with Sobel operator&lt;/p&gt;

&lt;p&gt;c) image shifted with translated identity filter&lt;/p&gt;

&lt;p&gt;For explanations of Lecture 5 Slide 6,FOURIER MAGNITUDE IMAGES
 believe the answers are the following:&lt;/p&gt;

&lt;p&gt;1 and 3 are distinctive because they are not natural images.&lt;/p&gt;

&lt;p&gt;1D – Gaussian stays as a circle in the Fourier space.&lt;/p&gt;

&lt;p&gt;3A – Sobel has two separated ellipses.&lt;/p&gt;

&lt;p&gt;2,4,5 are all similar because natural images have fairly similar Fourier magnitude images&lt;/p&gt;

&lt;p&gt;2B – flower image has an even distribution of frequencies, so we see an even circular distribution in all directions in Fourier space.&lt;/p&gt;

&lt;p&gt;4E – because we have only lines along the x-axis, so we see a line only on the y-axis in the Fourier amplitude image.&lt;/p&gt;

&lt;p&gt;5C – because strong x,y-axis-aligned lines in the natural image related to x,y-axis-aligned lines in the Fourier magnitude images
For Lecture 4 Slide 12, I have:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;0 -1  0&lt;/p&gt;

&lt;p&gt;-1  4 -1&lt;/p&gt;

&lt;p&gt;0 -1  0&lt;/p&gt;

&lt;p&gt;2.&lt;/p&gt;

&lt;p&gt;0 -1 1&lt;/p&gt;

&lt;p&gt;For Lecture 4 Slide 13, I have:&lt;/p&gt;

&lt;p&gt;a) G = D * B&lt;/p&gt;

&lt;p&gt;b) A = B * C&lt;/p&gt;

&lt;p&gt;c) F = D * E&lt;/p&gt;

&lt;p&gt;d) I = D * D&lt;/p&gt;

&lt;p&gt;For Lecture 5 Slide 6, I have:&lt;/p&gt;

&lt;p&gt;1 - D&lt;/p&gt;

&lt;p&gt;2 - B&lt;/p&gt;

&lt;p&gt;3 - A&lt;/p&gt;

&lt;p&gt;4 - E&lt;/p&gt;

&lt;p&gt;5 - C&lt;/p&gt;

&lt;p&gt;I’m not sure if I can explain why all of those are the way they are, but I hope this helps!&lt;/p&gt;

&lt;h2 id=&quot;factor-graphs&quot;&gt;Factor Graphs&lt;/h2&gt;

&lt;p&gt;I’m trying to understand the few slides on factor graph variable elimination, does anyone have an intuitive explanation or good resources to explain what’s going on?&lt;/p&gt;

&lt;p&gt;A factor graph is a probabilistic graphical model (in the same family with Markov Random Fields (MRFs) and Bayesian Networks). It is an undirected graph (meaning there are no parents or topological ordering).&lt;/p&gt;

&lt;p&gt;Bayesian Networks are directed graphs where edges in the graph are associated with conditional probability distributions (CPDs), assigning the probability of children in the graph taking on certain values based on the values of the parents.&lt;/p&gt;

&lt;p&gt;In undirected models like MRFs and Factor Graphs, instead of specifying CPDs, we specify (non-negative) potential functions (or factors) over sets of variables associated with cliques (complete subgraphs) C of the graph.  Like Conditional Prob. Distributions, a factor/potential can be represented as a table, but it is not normalized (does not sum to one).&lt;/p&gt;

&lt;p&gt;A factor graph is a bipartite undirected graph with variable nodes (circles) and factor nodes (squares). Edges are only between the variable nodes and the factor nodes.&lt;/p&gt;

&lt;p&gt;The variable nodes can take on certain values, and the likelihood of that event for a set of variables is expressed in the potential (factor node) attached to those variables.  Each factor node is associated with a single potential, whose scope is the set of variables that are neighbors in the factor graph.&lt;/p&gt;

&lt;p&gt;A small example might make this clearer. Suppose we have a group of four people: Alex, Bob, Catherine, David A=Alex’s hair color (red, green, blue)&lt;/p&gt;

&lt;p&gt;B=Bob’s hair color&lt;/p&gt;

&lt;p&gt;C=Catherine’s hair color&lt;/p&gt;

&lt;p&gt;D=David’s hair color&lt;/p&gt;

&lt;p&gt;Alex and Bob are friends, Bob and Catherine are friends, Catherine and David are friends, David and Alex are friends&lt;/p&gt;

&lt;p&gt;Friends never have the same hair color!&lt;/p&gt;

&lt;p&gt;It turns out that this distribution p cannot be represented (perfectly) by any Bayesian network. But it is succinctly represented by a Factor Graph or MRF.
https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmsma7unw07s/Screen_Shot_20181002_at_11.48.05_PM.png&lt;/p&gt;

&lt;p&gt;The Factor Graph distribution is same as the MRF – this is just a different graph data structure&lt;/p&gt;

&lt;p&gt;If you’re wondering about the variable elimination part, we choose subsets of variables connected by factors and start combining them by taking the product of their factors and marginalizing out variables.&lt;/p&gt;

&lt;p&gt;In the table Prof. Dellaert showed, we have variables as the columns and factors as the rows. He combines factors progressively to involve more and more variables.&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmtgs7d71g2p/Screen_Shot_20181003_at_2.05.56_PM.png&lt;/p&gt;

&lt;p&gt;$$ Normalizing SIFT&lt;/p&gt;

&lt;p&gt;You’re welcome to experiment with the choice of norm.  However, normally when we say that we’ll normalize every feature vector on its own, we mean normalizing descriptor &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;D_{normalized} = \frac{D}{\|D\|_2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We do this because feature descriptors are points in high-dimensional space, so we want them all to have the same length. That way the distance between them is based only upon the different angles the vectors point in, rather than considering their different length.&lt;/p&gt;

&lt;h2 id=&quot;more-efficient-calculation-of-sift-descriptor&quot;&gt;More efficient calculation of sift descriptor&lt;/h2&gt;

&lt;p&gt;I’m finding it slightly hard to implement an efficient algorithm for calculating the sift descriptor. The way I’m doing it now is basically like this;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate all the gradients and their angle&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over all interest points&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over the rows of the 4x4 cell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over the columns of the 4x4 cell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each cell index, compute the histogram and save it in that cell&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m feeling like there might be a nice numpy-way to get around my two inner loops (the ones over the cell). Is this possible? Any ideas or tips?&lt;/p&gt;

&lt;p&gt;For reference: it can compute the descriptor for around 1000 interest points in one second, don’t know if that’s sufficiently fast?&lt;/p&gt;

&lt;p&gt;One good way to reduce your 3 for-loops into 2 for-loops would be to do the following:&lt;/p&gt;

&lt;p&gt;Instead of:&lt;/p&gt;

&lt;p&gt;for interest point&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for row

        for col
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You could do:&lt;/p&gt;

&lt;p&gt;for interest point&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   for 4x4 patch in 16x16 window
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Creating those patches can be done by reshaping and swapping the axes. For example, if you had an array x like&lt;/p&gt;

&lt;p&gt;x = np.reshape(np.array(range(16)),(4,4))
It would look like&lt;/p&gt;

&lt;p&gt;array([[ 0,  1,  2,  3],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [ 4,  5,  6,  7],

   [ 8,  9, 10, 11],

   [12, 13, 14, 15]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You could break it into 2 parts along dimension 0:&lt;/p&gt;

&lt;p&gt;x.reshape(2,-1)
You’d get&lt;/p&gt;

&lt;p&gt;array([[ 0,  1,  2,  3,  4,  5,  6,  7],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [ 8,  9, 10, 11, 12, 13, 14, 15]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you kept breaking the innermost array into 2 arrays you’d get&lt;/p&gt;

&lt;p&gt;x.reshape(2,2,-1)
array([[[ 0,  1,  2,  3],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    [ 4,  5,  6,  7]],



   [[ 8,  9, 10, 11],

    [12, 13, 14, 15]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then finally you would get&lt;/p&gt;

&lt;p&gt;x.reshape(2,2,2,2)
array([[[[ 0,  1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     [ 2,  3]],

    [[ 4,  5],

     [ 6,  7]]],



   [[[ 8,  9],

     [10, 11]],

    [[12, 13],

     [14, 15]]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point you have 2 cubes that are 2x2x2. There are 3 ways you could look at this cube: there are 2 planes along the x-direction, or 2 planes along the y-direction, or 2 planes along the z-direction. If you swap the direction from which you look at the cube (swapaxes), you could now have&lt;/p&gt;

&lt;p&gt;array([[[[ 0,  1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     [ 4,  5]],

    [[ 2,  3],

     [ 6,  7]]],



   [[[ 8,  9],

     [12, 13]],

    [[10, 11],

     [14, 15]]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which you’ll notice is effectively all of the original 2x2 patches with stride 2 from the original 4x4 matrix.&lt;/p&gt;

&lt;p&gt;Sad to say it, but the speedup will probably be completely unnoticeable since the for loop is only over 4 iterations, as opposed to something over 40,000 iterations&lt;/p&gt;

&lt;h2 id=&quot;visualizing-sift&quot;&gt;Visualizing SIFT&lt;/h2&gt;

&lt;p&gt;Your get_features() function should return a NumPy array of shape (k, feat_dim) representing k stacked feature vectors (row-vectors), where “feat_dim” is the feature_dimensionality (e.g. 128 for standard SIFT).&lt;/p&gt;

&lt;p&gt;Since this is a 2D matrix, we can treat it as a grayscale image. Each row in the image would correspond to the feature vector for one interest point. We would hope that each feature vector would unique, so the image shouldn’t be completely uniform in color (all identical features) or completely black (all zero values). That would be a clue that your features are degenerate.&lt;/p&gt;

&lt;p&gt;For example, you might see something like this if you were to call:&lt;/p&gt;

&lt;p&gt;import matplotlib.pyplot as plt; plt.imshow(image1_features); plt.show()&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm4fywn2xohb/features.png&lt;/p&gt;

&lt;h2 id=&quot;trilinear-interpolation&quot;&gt;Trilinear Interpolation&lt;/h2&gt;

&lt;p&gt;http://paulbourke.net/miscellaneous/interpolation/&lt;/p&gt;

&lt;p&gt;On slide 30 of Lecture 7, Professor Hays was discussing trilinear interpolation. Trilinear interpolation is the name given to the process of linearly interpolating points within a box (3D) given values at the vertices of the box. We can think of our histogram as a 3D spatial histogram with &lt;script type=&quot;math/tex&quot;&gt;N_{\theta} \times N_x \times N_y&lt;/script&gt;, bins usually &lt;script type=&quot;math/tex&quot;&gt;8 \times 4 \times 4&lt;/script&gt;.
https://www.cc.gatech.edu/~hays/compvision/lectures/07.pdf&lt;/p&gt;

&lt;p&gt;You aren’t required to implement the trilinear interpolation for this project, but you may if you wish. I would recommend getting a baseline working first where the x and y derivatives at each pixel &lt;script type=&quot;math/tex&quot;&gt;I_x, I_y&lt;/script&gt; form 1 orientation, and that orientation goes into a single bin.&lt;/p&gt;

&lt;p&gt;Then you could try the trilinear interpolation afterwards once that is working (without trilinear interpolation, you can still get »80% accuracy on Notre Dame).&lt;/p&gt;

&lt;h2 id=&quot;sobel-vs-gaussian&quot;&gt;Sobel vs. Gaussian&lt;/h2&gt;

&lt;p&gt;Hi, I’m trying to decide which is a better way to compute the gradient for the Harris corner detection, before I compute my cornerness function. I’m confused about the difference between both.&lt;/p&gt;

&lt;p&gt;If I run just Sobel on my image, that means I’m getting the derivative, and smoothing with Gaussian in one go, right? And if I want to use Gaussian, I find the derivatives of the pixels, and apply Gaussian separately on the image? Not sure if one way is better than the other, and why.&lt;/p&gt;

&lt;p&gt;You can also do both with one filter.&lt;/p&gt;

&lt;p&gt;Suppose we have the image  &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
&lt;br/&gt;&lt;br/&gt;I = \begin{bmatrix}&lt;br/&gt;&lt;br/&gt;a &amp; b &amp; c \\&lt;br/&gt;&lt;br/&gt;d &amp; e &amp; f \\&lt;br/&gt;&lt;br/&gt;g &amp; h &amp; i&lt;br/&gt;&lt;br/&gt;\end{bmatrix}&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;One way to think about the Sobel x-derivative filter is that rather than looking at only &lt;script type=&quot;math/tex&quot;&gt;\frac{rise}{run}=\frac{f-d}{2}&lt;/script&gt; (centered at pixel e), we also use the x-derivatives above it and below it, e.g. &lt;script type=&quot;math/tex&quot;&gt;\frac{c-a}{2}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{i-g}{2}&lt;/script&gt;. But we weight the x-derivative in the center the most (this is a form of smoothing) so we use an approximation like&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ 2 \cdot (f-d) + (i-g)+ (c-a) }{8}&lt;/script&gt;

&lt;p&gt;meaning our kernel resembles&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{1}{8}&lt;br/&gt;&lt;br/&gt;\begin{bmatrix}&lt;br/&gt;&lt;br/&gt;1 &amp; 0 &amp; -1 \\&lt;br/&gt;&lt;br/&gt;2 &amp; 0 &amp; -2 \\&lt;br/&gt;&lt;br/&gt;1 &amp; 0 &amp; -1&lt;br/&gt;&lt;br/&gt;\end{bmatrix}&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;It is actually derived with directional derivatives.
https://www.researchgate.net/publication/239398674_An_Isotropic_3_3_Image_Gradient_Operator&lt;/p&gt;

&lt;p&gt;This is not identical to first blurring the image with a Gaussian filter, and then computing derivatives. We blur the image first because it makes the gradients less noisy.&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm1ficspl6jw/smoothing_gradients.png&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm1fiqeigs2g/derivative_theorem.png&lt;/p&gt;

&lt;p&gt;And an elegant fact that can save a step in smoothed gradient computation is to simply blur with the x and y derivatives of a Gaussian filter, by the following property:&lt;/p&gt;

&lt;p&gt;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture5_edges_cs131_2016.pdf
Slides&lt;/p&gt;

&lt;h2 id=&quot;nearest-neighbor-distance-ratio-algorithm-418&quot;&gt;nearest neighbor distance ratio algorithm 4.18&lt;/h2&gt;
&lt;p&gt;1) in the formula 4.18, what is meant by target descriptor, here it is Da ?&lt;/p&gt;

&lt;p&gt;2) In the formula 4.18 what is meant by Db and Dc as being descriptors?&lt;/p&gt;

&lt;p&gt;3)  What makes Da to be target descriptor and Db and Dc nearest neighbors ?&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;4) In formular 4.18&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Da - Db&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;is norm? or euclidean distance ?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;5) how is value of the descriptor such as Da related to the euclidean distance to Db?&lt;/p&gt;

&lt;p&gt;6) Is there such a thing as x and y coordinates of the center for a specific descriptor that helps to calculate the distances between the&lt;/p&gt;

&lt;p&gt;descriptors ? if so how to calculate the center of each descriptor ?&lt;/p&gt;

&lt;p&gt;1) in the formula 4.18, what is meant by target descriptor, here it is Da ?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; is a high-dimensional point. In the case of SIFT descriptors, &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; would be the SIFT feature vector in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;2) In the formula 4.18 what is meant by Db and Dc as being descriptors?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_B, D_C&lt;/script&gt; are high-dimensional points. These are the closest points, as measured by the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm, from &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; in this high dimensional space.&lt;/p&gt;

&lt;p&gt;3)  What makes Da to be target descriptor and Db and Dc nearest neighbors ?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; is the feature vector corresponding to an (x,y) location, for which we are trying to find matches in another image. &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; could be from Image1, and &lt;script type=&quot;math/tex&quot;&gt;D_B, D_C&lt;/script&gt; might be feature vectors corresponding to points in Image2&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;4) In formula 4.18&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Da - Db&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;is norm? or euclidean distance ?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\|D_A-D_B\|&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm, which we often call the Euclidean norm.&lt;/p&gt;

&lt;p&gt;5) how is value of the descriptor such as Da related to the euclidean distance to Db?&lt;/p&gt;

&lt;p&gt;Euclidean distance from &lt;script type=&quot;math/tex&quot;&gt;D_B&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; depends upon knowing the location of &lt;script type=&quot;math/tex&quot;&gt;D_B,D_A&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;6) Is there such a thing as x and y coordinates of the center for a specific descriptor that helps to calculate the distances between the&lt;/p&gt;

&lt;p&gt;descriptors ? if so how to calculate the center of each descriptor ?&lt;/p&gt;

&lt;p&gt;Each SIFT descriptor corresponds to an (x,y) point. We form the SIFT descriptor by looking at a 16x16 patch, centered at that (x,y) location. This could be considered a “center” of the descriptor, although I think using that terminology could be confusing since the (x,y)  “center” location in &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is not necessarily related at all to center of the &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt; space.  It may be possible to use the spatial information in &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; to verify the matching of points in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;arctan-vs-arctan2&quot;&gt;Arctan vs. Arctan2&lt;/h2&gt;

&lt;p&gt;I would use arctan2 instead of arctan. Because of the sign ambiguity, a function cannot determine with certainty in which quadrant the angle falls only by its tangent value. 
https://stackoverflow.com/questions/283406/what-is-the-difference-between-atan-and-atan2-in-c&lt;/p&gt;

&lt;p&gt;Numpy arctan2 returns an “Array of angles in radians, in the range [-pi, pi].” (see here).
https://docs.scipy.org/doc/numpy/reference/generated/numpy.arctan2.html&lt;/p&gt;

&lt;p&gt;As long as you bin all of the gradient orientations consistently, it turns out it doesn’t matter if the histograms are created from 8 uniform intervals from [0,360] or 8 uniform intervals from [-180,180]. Histograms are empirical samples of a distribution, and translating the range won’t affect that distribution.&lt;/p&gt;

&lt;p&gt;More on arctan2 from StackOverflow:&lt;/p&gt;

&lt;p&gt;From school mathematics we know that the tangent has the definition&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;code&gt;tan(α) = sin(α) / cos(α)&lt;/code&gt;&lt;/em&gt;
and we differentiate between four quadrants based on the angle that we supply to the functions. The sign of the sin, cos and tan have the following relationship (where we neglect the exact multiples of π/2):&lt;/p&gt;

&lt;h2 id=&quot;--quadrant----angle--------------sin---cos---tan&quot;&gt;&lt;em&gt;&lt;code&gt;  Quadrant    Angle              sin   cos   tan&lt;/code&gt;&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;I           0    &amp;lt; α &amp;lt; π/2      +     +     +
  II          π/2  &amp;lt; α &amp;lt; π        +     -     -
  III         π    &amp;lt; α &amp;lt; 3π/2     -     -     +
  IV          3π/2 &amp;lt; α &amp;lt; 2π       -     +     -&amp;lt;/code&amp;gt;&amp;lt;/em&amp;gt;
Given that the value of tan(α) is positive, we cannot distinguish, whether the angle was from the first or third quadrant and if it is negative, it could come from the second or fourth quadrant. So by convention, atan() returns an angle from the first or fourth quadrant (i.e. -π/2 &amp;lt;= atan() &amp;lt;= π/2), regardless of the original input to the tangent.&lt;/p&gt;

&lt;p&gt;In order to get back the full information, we must not use the result of the division sin(α) / cos(α) but we have to look at the values of the sine and cosine separately. And this is what atan2() does. It takes both, the sin(α) and cos(α) and resolves all four quadrants by adding π to the result of atan() whenever the cosine is negative.&lt;/p&gt;

&lt;h2 id=&quot;autocorrelation-matrix&quot;&gt;Autocorrelation Matrix&lt;/h2&gt;

&lt;p&gt;Nicolas, that &lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt; in the equation you’ve written is not multiplication – it is convolution. Szeliski states that he has “replaced the weighted summations with discrete convolutions with the weighting kernel &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;”.&lt;/p&gt;

&lt;p&gt;So before we had values &lt;script type=&quot;math/tex&quot;&gt;w(x,y)&lt;/script&gt; that could have been values from a Gaussian probability density function. Let &lt;script type=&quot;math/tex&quot;&gt;z = \begin{bmatrix} x \\ y \end{bmatrix}&lt;/script&gt; be the stacked 2D coordinate locations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(z) = \frac{1}{(2 \pi)^{n/2} |\Sigma|^{1/2}} \mbox{exp} \Bigg( - \frac{1}{2} (z − \mu)^T \Sigma^{-1} (z − \mu) \Bigg)&lt;/script&gt;

&lt;p&gt;For example, where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is the center pixel location and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is the location of each pixel in the local neighborhood. These were used as weights in summations over a local neighborhood,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
M = \sum\limits_{x,y} w(x,y) \begin{bmatrix}I_x^2 &amp; I_xI_y \\I_xI_y &amp; I_y^2\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;But the elegant convolution approach is used here because it is equivalent to summing a bunch of elementwise multiplications –  each point in a local neighborhood with its weight value (as we saw in Proj 1, and here filtering is equivalent to convolution since Gaussian filter is symmetric).&lt;/p&gt;

&lt;h2 id=&quot;image-derivatives&quot;&gt;Image Derivatives&lt;/h2&gt;

&lt;p&gt;Great question – there are a number of ways to do it, and they will all give different results.&lt;/p&gt;

&lt;p&gt;Prof. Hays discussed in lecture how convolving (not cross-correlation filtering) with the Sobel filter is a good way to approximate image derivatives (here we could treat a Gaussian filter as the image to find its derivatives).&lt;/p&gt;

&lt;p&gt;We’ve recommended a number of potentially useful OpenCV and SciPy functions that can do so in the project page. These will be very helpful!&lt;/p&gt;

&lt;p&gt;Another simple way to approximate the derivative is to calculate the 1st discrete difference along the given axis. For example, in order to compute horizontal discrete differences, shift the image by 1 pixel to the left and subtract the two&lt;/p&gt;

&lt;p&gt;For example, suppose you have a matrix b&lt;/p&gt;

&lt;p&gt;b = np.array([[  0,   1,   1,   2],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [  3,   5,   8,  13],

   [ 21,  34,  55,  89],

   [144, 233, 377, 610]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;b[:,1:] - b[:,:-1]
We would get:&lt;/p&gt;

&lt;p&gt;array([[  1,   0,   1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [  2,   3,   5],

   [ 13,  21,  34],

   [ 89, 144, 233]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we could the pad the matrix with zeros on the right column to bring it back to the original size.&lt;/p&gt;

&lt;h2 id=&quot;harris&quot;&gt;Harris&lt;/h2&gt;

&lt;p&gt;A gaussian filter is expressed as &lt;script type=&quot;math/tex&quot;&gt;g(\sigma_1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The second moment matrix at each pixel is convolved as follows:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = g(\sigma_1) * \begin{bmatrix} I_x^2 (\sigma_D) &amp; I_xI_y (\sigma_D) \\ I_xI_y (\sigma_D) &amp; I_{y}^2 (\sigma_D) \end{bmatrix} %]]&gt;&lt;/script&gt;
Giving a cornerness function in the lecture slides:
&lt;script type=&quot;math/tex&quot;&gt;har = \mbox{ det }[\mu(\sigma_1,\sigma_D)] - \alpha[\mbox{trace }\Big(\mu(\sigma_1,\sigma_D)\Big)]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;or, when evaluated,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;har = g(I_x^2)g(I_y^2) - [g(I_xI_y)]^2 - \alpha [g(I_x^2) + g(I_y^2)]^2&lt;/script&gt;

&lt;p&gt;So in the lecture slides notation, we can write &lt;script type=&quot;math/tex&quot;&gt;\mu(\sigma_1,\sigma_D)&lt;/script&gt; more simply by bringing in the filtering (identical to convolution here) operation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = \begin{bmatrix} g(\sigma_1) * I_x^2 (\sigma_D) &amp; g(\sigma_1) * I_xI_y (\sigma_D) \\ g(\sigma_1) * I_xI_y (\sigma_D) &amp; g(\sigma_1) * I_{y}^2 (\sigma_D) \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;It may be easier to understand if we write the equation in the following syntax:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = \begin{bmatrix} g(\sigma_1) * \Big(g(\sigma_D) * I_x \Big)^2 &amp; g(\sigma_1) * \Bigg[ \Big(g(\sigma_D) * I_x \Big) \odot \Big(g(\sigma_D) * I_x \Big) \Bigg] \\g(\sigma_1) * \Bigg[ \Big(g(\sigma_D) * I_x \Big) \odot \Big(g(\sigma_D) * I_x \Big) \Bigg] &amp; g(\sigma_1) * \Bigg[g(\sigma_D) * I_y \Bigg]^2\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;I should have explained that above – &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt; is the Hadamard product (element wise multiplication).&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Epipolar Geometry</title><link href="http://johnwlambert.github.io/epipolar-geometry/" rel="alternate" type="text/html" title="Epipolar Geometry" /><published>2018-11-19T06:01:00-05:00</published><updated>2018-11-19T06:01:00-05:00</updated><id>http://johnwlambert.github.io/epipolar-geometry</id><content type="html" xml:base="http://johnwlambert.github.io/epipolar-geometry/">&lt;h2 id=&quot;why-know-epipolar-geometry&quot;&gt;Why know Epipolar Geometry?&lt;/h2&gt;
&lt;p&gt;Modern robotic computer vision tasks like Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM) would not be possible without feature matching. Tools from Epipolar Geometry are an easy way to discard outliers in feature matching and are widely used.&lt;/p&gt;

&lt;h2 id=&quot;the-fundamental-matrix&quot;&gt;The Fundamental Matrix&lt;/h2&gt;

&lt;p&gt;The Fundamental matrix provides a correspondence \(x^TFx^{\prime} = 0\), where \(x,x^{\prime}\) are 2D corresponding points in separate images. In other words,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} u^{\prime} &amp; v^{\prime} &amp; 1 \end{bmatrix} \begin{bmatrix} f_{11} &amp; f_{12} &amp; f_{13} \\ f_{21} &amp; f_{22} &amp; f_{23} \\ f_{31} &amp; f_{32} &amp; f_{33} \end{bmatrix} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 0 %]]&gt;&lt;/script&gt;

&lt;p&gt;Longuet-Higgins’ 8-Point Algorithm [1] provides the solution for estimating \(F\) if at least 8 point correspondences are provided. A system of linear equations is formed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Af = \begin{bmatrix} u_1 u_1^{\prime} &amp; u_1v_1^{\prime} &amp; u_1 &amp; v_1 u_1^{\prime} &amp; v_1 v_1^{\prime} &amp; v_1 &amp; u_1^{\prime} &amp; v_1^{\prime} &amp; 1 \\ \vdots &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots &amp; \vdots  \\   u_n u_n^{\prime} &amp; u_n v_n^{\prime} &amp; u_n &amp; v_n u_n^{\prime} &amp; v_n v_n^{\prime} &amp; v_n &amp; u_n^{\prime} &amp; v_n^{\prime} &amp; 1 \end{bmatrix} \begin{bmatrix} f_{11} \\ f_{12} \\ f_{13} \\ f_{21} \\ \vdots \\ f_{33} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The matrix-vector product above can be driven to zero by minimizing the norm, and avoiding the degenerate solution that \(x=0\) with a constraint that the solution lies upon the unit ball, e.g.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
  \underset{\|x\|=1}{\mbox{minimize}} &amp; \|A x \|_2^2 = x^TA^TAx = x^TBx
  \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;By the Courant-Fisher characterization, it is well known that if \(B\) is a \(n \times n\) symmetric matrix with eigenvalues \(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n \) and corresponding eigenvectors \(v_1, \dots, v_n\), then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_n = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} x^TBx&lt;/script&gt;

&lt;p&gt;meaning the eigenvector associated with the smallest eigenvalue \(\lambda_n\) of \(B\) is the solution \(x^{\star}\). The vector \(x^{\star}\) contains the 9 entries of the Fundamental matrix \(F^{\star}\).&lt;/p&gt;

&lt;p&gt;This is a specific instance of the extremal trace \([2]\) (or trace minimization on a unit sphere) problem, with \(k=1\), i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \begin{array}{ll}
    \mbox{minimize} &amp; \mathbf{\mbox{Tr}}(X^TBX) \\
    \mbox{subject to} &amp; X^TX=I_k
    \end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(I_k\) denotes the \(k \times k\) identity matrix. The unit ball constraint avoids the trivial solution when all eigenvalues \(\lambda_i\) are zero instead of a single zero eigenvalue.&lt;/p&gt;

&lt;p&gt;In our case, since \(U,V\) are orthogonal matrices (with orthonormal columns), then \(U^TU=I\). Thus, the SVD of \(B\) yields 
\begin{equation}
A^TA = (U\Sigma V^T)^T (U\Sigma V^T) = V\Sigma U^TU \Sigma V^T = V \Sigma^2 V^T.
\end{equation}&lt;/p&gt;

&lt;p&gt;Since \(B=A^TA\), \(B\) is symmetric, and thus the columns of \(V=\begin{bmatrix}v_1 \dots v_n \end{bmatrix}\) are eigenvectors of \(B\). \(V\) can equivalently be computed with the SVD of \(A\) or \(B\), since \(V\) appears in both decompositions: \(A=U \Sigma V^T\) and \(B=V\Sigma^2V^T\).&lt;/p&gt;

&lt;h2 id=&quot;proof-the-svd-provides-the-solution&quot;&gt;Proof: the SVD provides the solution&lt;/h2&gt;
&lt;p&gt;The proof is almost always taken for granted, but we will provide it here for completeness. Because \(B\) is symmetric, there exists a set of \(n\) orthonormal eigenvectors, yielding an eigendecomposition \(B=V^T \Lambda V\). Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{llll}
    \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; x^TBx = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; x^TV^T \Lambda Vx = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; (Vx)^T \Lambda (Vx)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since \(V\) is orthogonal, \(|Vx| = |x|\), thus minimizing \((Vx)^T \Lambda (Vx)\) is equivalent to minimizing \(x^T \Lambda x\). Since \(\Lambda\) is diagonal, \(x^TBx = \sum\limits_{i=1}^{n} \lambda_i x_i^2\) where \({\lambda_i}_{i=1}^n\) are the eigenvalues of \(B\). Let \(q_i=x_i^2\), meaning \(q_i\geq 0\) since it represents a squared quantity. Since \(|x|=1\), then \(\sqrt{\sum\limits_i x_i^2}=1\), \(\sum\limits_i x_i^2=1 \), \(\sum\limits_i q_i = 1\). Thus,&lt;/p&gt;

&lt;p&gt;\begin{equation}
 \underset{|x|=1}{\mbox{min }}  x^TBx= \underset{|x|=1}{\mbox{min }} \sum\limits_{i=1}^{n} \lambda_i x_i^2= \underset{q_i}{\mbox{min }} \sum\limits_i \lambda_i q_i = \underset{q_i}{\mbox{min }} \lambda_n \sum\limits_i q_i = \lambda_n
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\lambda_n\) is the smallest eigenvalue of \(B\). The last line follows since \(q_i \geq 0\) and \(\sum\limits_i q_i = 1\), therefore we have a convex combination of a set of numbers \( {\lambda_i}_{i=1}^n \) on the real line. By properties of a convex combination, the result must lie in between the smallest and largest number. Now that we know the minimum is \(\lambda_n\), we can obtain the argmin by the following observation:&lt;/p&gt;

&lt;p&gt;If \(v\) is an eigenvector of \(B\), then 
\begin{equation}
    Bv = \lambda_n v
\end{equation}
Left multiplication with \(v^T\) simplifies the right side because \(v^Tv=1\), by our constraint that \(|x|=1\). We find:
\begin{equation}
    v^T(Bv) = v^T (\lambda_n v) = v^Tv \lambda_n = \lambda_n
\end{equation}
Thus the eigenvector \(v\) associated with the eigenvalue \(\lambda_n\) is \(x^{\star}\). \( \square\)&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;[1] H. Longuet Higgins. A computer algorithm for reconstructing a scene from two projections. &lt;em&gt;Nature&lt;/em&gt;, 293, 1981.&lt;/p&gt;

&lt;p&gt;[2] S.  Boyd.   Low  rank  approximation  and  extremal  gain  problems,  2008.   URL &lt;a href=&quot;http://ee263.stanford.edu/notes/low_rank_approx.pdf&quot;&gt;http://ee263.stanford.edu/notes/low_rank_approx.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Fundamental matrix, solution via SVD.</summary></entry><entry><title type="html">Parallel Computing with MPI</title><link href="http://johnwlambert.github.io/mpi/" rel="alternate" type="text/html" title="Parallel Computing with MPI" /><published>2018-10-02T07:01:00-04:00</published><updated>2018-10-02T07:01:00-04:00</updated><id>http://johnwlambert.github.io/mpi</id><content type="html" xml:base="http://johnwlambert.github.io/mpi/">&lt;p&gt;mpiexec -x -np 2 xterm -e cuda-gdb ./myapp&lt;/p&gt;

&lt;p&gt;mpirun –np 2 nvprof –log-file profile.out.%p&lt;/p&gt;

&lt;p&gt;nvprof ./addMatrices -n 4000&lt;/p&gt;

&lt;p&gt;cuda-memcheck ./memcheck_demo&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 mpirun -np 4 nvprof –output-profile profile.%p.nvprof ./main [args]&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 makes MVAPICH2 CUDA aware.&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 mpirun -np 1 nvprof –kernels gpu_GEMM –analysis-metrics
–output-profile GEMMmetrics.out.%p.nvprof ./main [args]&lt;/p&gt;

&lt;p&gt;nvvp &amp;amp;&lt;/p&gt;</content><author><name></name></author><summary type="html">Demystify backprop.</summary></entry><entry><title type="html">Fully Connected Neural Networks From Scratch</title><link href="http://johnwlambert.github.io/fully-connected/" rel="alternate" type="text/html" title="Fully Connected Neural Networks From Scratch" /><published>2018-10-02T07:01:00-04:00</published><updated>2018-10-02T07:01:00-04:00</updated><id>http://johnwlambert.github.io/fully-connected-network</id><content type="html" xml:base="http://johnwlambert.github.io/fully-connected/">&lt;p&gt;Neural networks are often explained in the most complicated ways possible, but we’ll show just how simple they can be.&lt;/p&gt;

&lt;p&gt;Suppose we wish to implement a fully-connected feedforward neural network with 1 input layer, 1 hidden layer, and 1 output layer. We call such a network to be a two-layer neural network (ignoring the input layer as it is trivially present).&lt;/p&gt;

&lt;p&gt;In the feedforward step, we feed an input \(x \in R^{1 \times d}\) through the network.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z^{(1)} = W^{(1)}x + b^{(1)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^{(1)} = \sigma(z^{(1)})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y} = a^{(2)} = \mbox{ softmax }(z^{(2)} )&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;python_feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'X'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;pdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repmat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'z1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repmat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'z2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;#logits = np.exp(z2) / np.matlib.repmat( np.sum(np.exp(z2.T),axis=1).T, cache['D_out'], 1)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repmat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'logits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# also called cache.yc, a2&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ll add L2 regularization on the weights of the network:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;norms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;all_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;all_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sum&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use a cross-entropy loss:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CE(y, \hat{y}) = - \sum\limits_{i=0}^{C-1} y_i \mbox{ log }(\hat{y}_i)&lt;/script&gt;

&lt;p&gt;And we also add the penalization term for the magnitude of \(W\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(W,b;x,y) = \frac{1}{N} \sum\limits_{i=1}^N CE^{(i)}(y, \hat{y}) + 0.5 \lambda \| p \|^2&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;python_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	a = array([[ 0,  1,  2],
		       [ 3,  4,  5],
		       [ 6,  7,  8],
		       [ 9, 10, 11]])
	logits[np.array([3,2,1]),np.arange(3)]
	
	returns array([9, 7, 5])
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;yc_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'logits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;ce_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yc_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;data_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# divide by batch size&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;reg_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg_loss&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Loss: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we use the chain rule to backpropagate gradients:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial CE(y, \hat{y}) }{ \partial z^{(2)}} = \hat{y} − y&lt;/script&gt;

&lt;p&gt;Recall that \(z^{(2)} = W^{(2)}a^{(1)} + b^{(2)} \). Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial CE(y, \hat{y} )}{\partial W^{(2)}} =
\frac{ \partial CE(y, \hat{y} )}{\partial z^{(2)}} \frac{\partial z^{(2)}}{\partial W^{(2)}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial CE(y, \hat{y} )}{ \partial W^{(2)} }= ( \hat{y} − y) a^{(1)^T}&lt;/script&gt;

&lt;p&gt;Similarly,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial CE(y, yˆ) }{ \partial b^{(2)} }= \hat{y} − y&lt;/script&gt;

&lt;p&gt;Going across L2:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial z^{(2)} }{\partial a^{(1)} }= W^{(2)^T}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial CE(y, yˆ) }{\partial a^{(1)} } = \frac{\partial CE(y, yˆ)}{\partial z^{(2)} } \frac{ \partial z^{(2)} }{\partial a^{(1)} } = W^{(2)^T} ( \hat{y} − y)&lt;/script&gt;

&lt;p&gt;Going across the non-linearity of L1:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial CE(y, yˆ) }{\partial z^{(1)} }= \frac{ \partial CE(y, yˆ)}{ \partial a^{(1)}} \frac{ \partial σ(z)}{ \partial z^{(1)} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{\partial CE(y, yˆ)}{\partial a^{(1)} } ◦ σ(z^{(1)}) ◦ (1 − \sigma(z^{(1)}))&lt;/script&gt;

&lt;p&gt;Note that we have assumed that &lt;script type=&quot;math/tex&quot;&gt;\sigma(·)&lt;/script&gt; works on matrices by applying an element-wise sigmoid, and ◦ is the
element-wise (Hadamard) product.
That brings us to our final gradients:
\partial CE(y, yˆ)
\partial W(1) =
\partial CE(y, yˆ)
\partial z(1)
\partial z(1)
\partial W(1)
\partial CE(y, yˆ)
\partial W(1) =
∂CE(y, yˆ)
∂z(1) 
x
T
(6)
Similarly,
\partial CE(y, yˆ)
\partial b(1) =
\partial CE(y, yˆ)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;python_backprop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# divide by batch size&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;one_hot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;one_hot_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'logits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dW2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 0.5 cancels&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#grads['db2'] = np.sum(diff, axis=1)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'db2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;da1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# elementwise&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dz1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;da1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dW1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dz1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'X'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 0.5 cancels&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#grads['db1'] = np.sum(dz1, axis=1)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'db1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dz1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;implementing-the-same-network-in-pytorch&quot;&gt;Implementing the same network in Pytorch&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.optim&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pdb&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.autograd&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.matlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.linalg&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PytorchFCNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PytorchFCNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_in'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weights_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;c&quot;&gt;#classname = m.__class__.__name__&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'idx = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;c&quot;&gt;#m.weight.data = torch.from_numpy(self.cache['W1'])&lt;/span&gt;
					&lt;span class=&quot;c&quot;&gt;#m.bias.data = torch.from_numpy(self.cache['b1'])&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;c&quot;&gt;#m.weight.data = torch.from_numpy(self.cache['W2'])&lt;/span&gt;
					&lt;span class=&quot;c&quot;&gt;#m.bias.data = torch.from_numpy(self.cache['b2'])&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# # Create random Tensors for weights; setting requires_grad=True means that we&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# # want to compute gradients for these Tensors during the backward pass.&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# W1 = torch.fromnumpy(cache['W1'] , requires_grad=True)&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# b1 = torch.fromnumpy(cache['b1'] device=device, requires_grad=True)&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# W2 = torch.fromnumpy(cache['W2'] device=device, requires_grad=True)&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# b2 = torch.fromnumpy(cache['b2'] device=device, requires_grad=True)&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# W1 = torch.as_tensor(W1, device=device)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;pdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run_pytorch_fc_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PytorchFCNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#device = torch.device('cpu')&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# Create random Tensors to hold input and outputs&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#, device=device)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#, device=device)&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;volatile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LongTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;volatile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=cache['reg'])&lt;/span&gt;



	&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#, dim=1)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nll_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size_average&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# exclude bias weights&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# sum up batch loss&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# Use autograd to compute the backward pass. This call will compute the&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# gradient of loss with respect to all Tensors with requires_grad=True.&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# After this call w1.grad and w2.grad will be Tensors holding the gradient&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# of the loss with respect to w1 and w2 respectively.&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#optimizer.zero_grad()&lt;/span&gt;


	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#optimizer.step()&lt;/span&gt;


	&lt;span class=&quot;c&quot;&gt;# optimizer weight_decay=0&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W1 grad:'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b1 grad:'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W2 grad:'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b2 grad:'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;


	&lt;span class=&quot;c&quot;&gt;# print 'W1 weight:'&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print list(model.modules())[1].weight.data&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# print 'b1 weight:'&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print list(model.modules())[1].bias.data&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# print 'W2 weight:'&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print list(model.modules())[2].weight.data&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# print 'b2 weight:'&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print list(model.modules())[2].bias.data&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# Update weights using gradient descent. For this step we just want to mutate&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# the values of w1 and w2 in-place; we don't want to build up a computational&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# graph for the update steps, so we use the torch.no_grad() context manager&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# to prevent PyTorch from building a computational graph for the updates&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# with torch.no_grad():&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# list(model.modules())[1].weight.data -= learning_rate * list(model.modules())[1].weight.grad.data&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# list(model.modules())[2].weight.data -= learning_rate * list(model.modules())[2].weight.grad.data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run_python_fc_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;python_feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'logits:'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'logits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;python_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;python_backprop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W1 grad'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dW1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b1 grad'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'db1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W2 grad'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dW2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b2 grad'&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'db2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# # Update weights using gradient descent&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# cache['W1'] -= learning_rate * grads['dW1']&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# cache['b1'] -= learning_rate * grads['db1']&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# cache['W2'] -= learning_rate * grads['dW2']&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# cache['b2'] -= learning_rate * grads['db2']&lt;/span&gt;


	&lt;span class=&quot;c&quot;&gt;# print 'weight for W1', cache['W1']&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print 'weight for b1', cache['b1']&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print 'weight for W2', cache['W2']&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print 'weight for b2', cache['b2']&lt;/span&gt;



&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# N is batch size; D_in is input dimension;&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# H is hidden dimension; D_out is output dimension.&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_in'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# Create random input and output data&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; 
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 
				&lt;span class=&quot;c&quot;&gt;#	[1,0],&lt;/span&gt;
				&lt;span class=&quot;c&quot;&gt;#	[0,1],&lt;/span&gt;
				&lt;span class=&quot;c&quot;&gt;#	[0,0]]	)&lt;/span&gt;


		&lt;span class=&quot;c&quot;&gt;#np.random.randn(D_out,N)&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;# Randomly initialize weights&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;run_pytorch_fc_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;run_python_fc_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;(1) Eric Darve, Stanford University.&lt;/p&gt;</content><author><name></name></author><summary type="html">Demystify backprop.</summary></entry><entry><title type="html">Kernel Trick</title><link href="http://johnwlambert.github.io/kernel-trick/" rel="alternate" type="text/html" title="Kernel Trick" /><published>2018-10-02T07:01:00-04:00</published><updated>2018-10-02T07:01:00-04:00</updated><id>http://johnwlambert.github.io/kernel-trick</id><content type="html" xml:base="http://johnwlambert.github.io/kernel-trick/">&lt;h2 id=&quot;the-kernel-trick&quot;&gt;The Kernel Trick&lt;/h2&gt;
&lt;p&gt;The Kernel Trick is a poorly taught but beautiful piece of insight that makes SVMs work.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) = (x^Tz)^2 = (\sum\limits_{i=1}^n x_iz_i)^2&lt;/script&gt;

&lt;p&gt;You might be tempted to simplify this immediately to \( \sum\limits_{i=1}^n (x_iz_i)^2 \), but this would be incorrect. This expression is actually a square of sums, which involves a large product (remember expanding binomial products like \( (a-b)(a-b) \) which involve \(2^2\) terms ).&lt;/p&gt;

&lt;p&gt;We can expand the square of sums, and we’ll use different indices \(i,j\) to keep track of the respective terms:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) = (\sum\limits_{i=1}^n x_iz_i) (\sum\limits_{j=1}^n x_jz_j)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) = \sum\limits_{i=1}^n \sum\limits_{j=1}^n (x_ix_j) (z_iz_j)&lt;/script&gt;

&lt;p&gt;This is much easier to understand with an example. Consider \(n=3\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) = \begin{bmatrix} x_1z_1 + x_2z_2 + x_3z_3 \end{bmatrix} \begin{bmatrix} x_1z_1 + x_2z_2 + x_3z_3 \end{bmatrix}&lt;/script&gt;

&lt;p&gt;Expanding terms, we get a sum with 9 total terms:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) = \bigg(x_1z_1x_1z_1 +  x_1z_1x_2z_2 + x_1z_1x_3z_3\bigg) + \bigg(x_2z_2 x_1z_1 +  x_2z_2 x_2z_2 +   x_2z_2x_3z_3\bigg) + \bigg( x_3z_3x_1z_1 + x_3z_3x_2z_2 + x_3z_3x_3z_3 \bigg)&lt;/script&gt;

&lt;p&gt;Surprisingly, this sum can be written as a simple inner product of two vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z) =\phi(x)^T \phi(z) = \begin{bmatrix} x_1x_1  \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3  \\ \vdots  \\ \end{bmatrix} \begin{bmatrix} z_1z_1  \\ z_1z_2 \\ z_1z_3 \\ z_2z_1 \\ z_2z_2 \\ z_2z_3  \\ \vdots  \\ \end{bmatrix}&lt;/script&gt;

&lt;p&gt;Thus, we’ve shown that for \(n=3\),
&lt;script type=&quot;math/tex&quot;&gt;K(x,z) = \phi(x)^T \phi(z)&lt;/script&gt;
which contains all of the cross-product terms.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Kernel Trick is a poorly taught but beautiful piece of insight that makes SVMs work.</summary></entry><entry><title type="html">Fast Nearest Neighbors</title><link href="http://johnwlambert.github.io/fast-nearest-neighbor/" rel="alternate" type="text/html" title="Fast Nearest Neighbors" /><published>2018-10-02T07:00:00-04:00</published><updated>2018-10-02T07:00:00-04:00</updated><id>http://johnwlambert.github.io/fast-nearest-neighbor</id><content type="html" xml:base="http://johnwlambert.github.io/fast-nearest-neighbor/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#rank&quot;&gt;Brute Force&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fastaffinitymatrices&quot;&gt;Fast Affinity Matrices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nullspace&quot;&gt;Speedup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nearest-neighbor&quot;&gt;Nearest Neighbor&lt;/h2&gt;
&lt;p&gt;Finding closest points in a high-dimensional space is a re-occurring problem in computer vision, especially when performing feature matching (e.g. with &lt;a href=&quot;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&quot;&gt;SIFT&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Brute force methods can be prohibitively slow and much faster ways exist of computing with a bit of linear algebra.&lt;/p&gt;

&lt;p&gt;We are interested in the quantity&lt;/p&gt;

&lt;p&gt;Since \(a,b \in R^n\) are vectors, we can expand the Mahalanobis distance. When \(A=I\), we are working in Euclidean space (computing \(\ell_2\) norms):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=(a-b)^TA(a-b)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=(a^T-b^T)A(a-b)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=(a^TA-b^TA)(a-b)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=a^TAa-b^TAa - a^TAa + b^TAb&lt;/script&gt;

&lt;p&gt;Now we wish to compute these on entire datasets simultaneously. We can form matrices \(A \in R^{m_1 \times n}, B \in R^{m_2 \times n}\) that hold our high-dimensional points.&lt;/p&gt;

&lt;p&gt;Consider \(AB^T\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
AB^T = \begin{bmatrix} - &amp; a_1 &amp; - \\ - &amp; a_2 &amp; - \\ - &amp; a_3 &amp; - \end{bmatrix} \begin{bmatrix} | &amp; | &amp; | \\ b_1^T &amp; b_2^T &amp; b_3^T \\ | &amp; | &amp; | \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;extremaltrace&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;brute-force-nearest-neighbors&quot;&gt;Brute Force Nearest Neighbors&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;naive_upper_triangular_compute_affinity_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Create an mxn matrix, where each (i,j) entry denotes
    the Mahalanobis distance between point i and point j,
    as defined by the metric &quot;A&quot;. A has dimension (dim x dim).
    Use of a for loop makes this function somewhat slow.

    not symmetric
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# make sure feature vectors have the same length&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;affinity_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# rows&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# cols&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;affinity_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#affinity matrix contains the Mahalanobis distances&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;affinity_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;fastaffinitymatrices&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;fast-affinity-matrix-computation&quot;&gt;Fast Affinity Matrix Computation&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fast_affinity_mat_compute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    X is (m,n)
    A is (n,n)
    K is (m,m)
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ab_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_sqr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_sqr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_sqr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ab_T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We now demonstrate the&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;unit_test_arr_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; &quot;&quot;&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;gt_aff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;naive_upper_triangular_compute_affinity_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;pred_aff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_affinity_mat_compute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gt_aff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_aff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gt_aff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_aff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 100&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 135&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# unit_test_pt_arr(m1,m2,n)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;unit_test_arr_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now consider the speedup we’ve achieved:&lt;/p&gt;</content><author><name></name></author><summary type="html">Vectorizing nearest neighbors (with no for-loops!)</summary></entry><entry><title type="html">Subgradient Methods in 10 Minutes</title><link href="http://johnwlambert.github.io/subgradient-methods/" rel="alternate" type="text/html" title="Subgradient Methods in 10 Minutes" /><published>2018-04-02T07:00:00-04:00</published><updated>2018-04-02T07:00:00-04:00</updated><id>http://johnwlambert.github.io/subgradient</id><content type="html" xml:base="http://johnwlambert.github.io/subgradient-methods/">&lt;!-- 
&lt;svg width=&quot;800&quot; height=&quot;200&quot;&gt;
	&lt;rect width=&quot;800&quot; height=&quot;200&quot; style=&quot;fill:rgb(98,51,20)&quot; /&gt;
	&lt;rect width=&quot;20&quot; height=&quot;50&quot; x=&quot;20&quot; y=&quot;100&quot; style=&quot;fill:rgb(189,106,53)&quot; /&gt;
	&lt;rect width=&quot;20&quot; height=&quot;50&quot; x=&quot;760&quot; y=&quot;30&quot; style=&quot;fill:rgb(77,175,75)&quot; /&gt;
	&lt;rect width=&quot;10&quot; height=&quot;10&quot; x=&quot;400&quot; y=&quot;60&quot; style=&quot;fill:rgb(225,229,224)&quot; /&gt;
&lt;/svg&gt;
 --&gt;

&lt;p&gt;Now that we’ve covered topics from convex optimization in a very shallow manner, it’s time to go deeper.&lt;/p&gt;

&lt;h2 id=&quot;subgradient-methods&quot;&gt;Subgradient methods&lt;/h2&gt;

&lt;h3 id=&quot;convexity-review&quot;&gt;Convexity Review&lt;/h3&gt;

&lt;p&gt;To understand subgradients, first we need to review convexity. The Taylor series of a real valued function that is infinitely differentiable at a real number \(x\) is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \approx \sum\limits_{k=0}^{\infty} \frac{f^{(k)}(x)}{k!}(y-x)^k&lt;/script&gt;

&lt;p&gt;Expanding terms, the series resembles&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \approx  f(x) + \frac{f^{\prime}(x)}{1!}(y-x) + \frac{f^{\prime\prime}(x)}{2!}(y-x)^2 + \frac{f^{\prime\prime\prime}(x)}{3!}(y-x)^3 + \cdots.&lt;/script&gt;

&lt;p&gt;If a function is convex, its first order Taylor expansion (a tangent line) will always be a global underestimator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \geq f(x) + \frac{f^{\prime}(x)}{1!}(y-x)&lt;/script&gt;

&lt;p&gt;If \(y=x + \delta\), then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
f(x+ \delta) \geq f(x) + \nabla f(x)^T (x+\delta-x) \\
f(x+ \delta) \geq f(x) + \nabla f(x)^T \delta
\end{aligned}&lt;/script&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/tangent_underestimates_quadratic.jpg&quot; width=&quot;25%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;From the picture, it’s obvious that an affine function is always an underestimator for this quadratic (thus convex) function.&lt;/p&gt;

&lt;!-- But there is also intuition behind why this is true. A univariate function is convex if its derivative is increasing (thus second derivative is positive). Since slope is a measure of function increase/decrease over some interval \\(\delta\\), where usually \\(\delta=1\\), if we multiply slope by \\(\delta = y-x = \\) &quot;run&quot;, then we will be left with only the change in function value over the interval (\\(slope=\frac{rise}{run}\\) and \\(\frac{rise}{run}\cdot(run)=rise\\) ).

&lt;div align=&quot;center&quot;&gt;
&lt;img  src=&quot;/assets/tangent_underestimates_4cases.jpg&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

If \\(y&gt;x\\), then \\(\delta &gt; 0\\):
- (Case 1 above) If we are to the right of the stationary point (here global minimum) \\(0,0)\\) then \\(\nabla f&gt;0\\), and since the derivative is increasing (which an affine function cannot account for), we never predict a large enough function increase (underestimating).
- (Case 2 above) If we are to the left of \\(0,0)\\) then \\(\nabla f&lt;0\\), we estimate too large of a function decrease (the derivative is increasing). So we underestimate.

-If \\(x&gt;y\\), our \\(\delta &lt; 0\\):
- (Case 3 above) If we are to the right of \\(0,0)\\), then \\(\nabla f&gt;0\\). We predict too large of a function decrease and underestimate.
- (Case 4 above) If we are to the left of \\(0,0)\\), then \\(\nabla f&lt;0\\). We overestimate the function decrease, so we underestimate. --&gt;

&lt;h2 id=&quot;subgradients&quot;&gt;Subgradients&lt;/h2&gt;

&lt;p&gt;Consider a function \(f\) with a kink inside of it, rendering the function non-differentiable at a point \(x_2\).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/subgradients.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The derivative jumps up by some amount (over some interval) at this kink. Any slope within that interval is a valid subgradient. A subgradient is also a supporting hyperplane to the epigraph of the function. The set of subgradients of \(f\) at \(x\) is called the subdifferential \(\partial f(x)\) (a point-to-set mapping). To determine if a function is subdifferentiable at \(x_0\), we must ask, “is there a global affine lower bound on this function that is tight at this point?”.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a function is convex, it has at least one point in the relative interior of the domain.&lt;/li&gt;
  &lt;li&gt;If a function is differentiable at \(x\), then \(\partial f(x) = {\nabla f(x)} \) (a singleton set).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, for absolute value, we could say the interval \( [\nabla f_{\text{left}}(x_0), \nabla f_{\text{right}}(x_0)] \) is the range of possible slopes (subgradients):&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/subdifferential.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;basic-subgradient-method-negative-subgradient-update&quot;&gt;Basic Subgradient Method: Negative Subgradient Update&lt;/h3&gt;

&lt;p&gt;From [1]: Given a convex function \(f:\mathbb{R}^n \rightarrow \mathbb{R}\), not necessarily differentiable. The subgradient method is just like gradient descent, but replacing gradients with subgradients. i.e., initialize \(x^{(0)}\), then repeat&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
x^{(k+1)} = x^{(k)} − \alpha_k \cdot g^{(k)}, &amp; k = 0, 1, 2, 3, \cdots
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(g^{(k)}\) is &lt;strong&gt;any&lt;/strong&gt; subgradient of \(f\) at \(x^{(k)}\), and \(\alpha_k &amp;gt;0 \) is the \(k\)’th step size.&lt;/p&gt;

&lt;p&gt;Unlike gradient descent, in the negative subgradient update it’s entirely possible that \(-g^{(k)}\) is not a descent direction for \(f\) at \(x^{(k)}\). In such cases, we always have \(f(x^{(k+1)}) &amp;gt; f(x^{(k)})\), meaning an iteration of the subgradient method can increase the objective function. To resolve this issue, we keep track of best iterate \(x_{best}^k\) among \(x^{(1)}, \cdots , x^{(k)}\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{best}^{(k)}) = \underset{i=1,\cdots ,k}{\mbox{ min  }}  f(x^{(i)})&lt;/script&gt;

&lt;p&gt;To update each \(x^{(i)}\), there are at least 5 common ways to select the step size, all with different convergence properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Constant step size&lt;/li&gt;
  &lt;li&gt;Constant step length&lt;/li&gt;
  &lt;li&gt;Square summable but not summable.&lt;/li&gt;
  &lt;li&gt;Nonsummable diminishing&lt;/li&gt;
  &lt;li&gt;Nonsummable diminishing step lengths.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See &lt;a href=&quot;https://stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf&quot;&gt;[2]&lt;/a&gt; for details regarding each of these possible step size choices and the &lt;strong&gt;associated guarantees on convergence&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;subgradient-methods-for-constrained-problems&quot;&gt;Subgradient methods for constrained problems&lt;/h3&gt;

&lt;h3 id=&quot;primal-dual-subgradient-methods&quot;&gt;Primal-dual subgradient methods&lt;/h3&gt;

&lt;h3 id=&quot;stochastic-subgradient-method&quot;&gt;Stochastic subgradient method&lt;/h3&gt;

&lt;h3 id=&quot;mirror-descent-and-variable-metric-methods&quot;&gt;Mirror descent and variable metric methods&lt;/h3&gt;

&lt;h2 id=&quot;localization-methods&quot;&gt;Localization methods&lt;/h2&gt;

&lt;h3 id=&quot;localization-and-cutting-plane-methods&quot;&gt;Localization and cutting-plane methods&lt;/h3&gt;

&lt;h3 id=&quot;analytic-center-cutting-plane-method&quot;&gt;Analytic center cutting-plane method&lt;/h3&gt;

&lt;h3 id=&quot;ellipsoid-method&quot;&gt;Ellipsoid method&lt;/h3&gt;

&lt;h2 id=&quot;decomposition-and-distributed-optimization&quot;&gt;Decomposition and distributed optimization&lt;/h2&gt;

&lt;h3 id=&quot;primal-and-dual-decomposition&quot;&gt;Primal and dual decomposition&lt;/h3&gt;

&lt;h3 id=&quot;decomposition-applications&quot;&gt;Decomposition applications&lt;/h3&gt;

&lt;h3 id=&quot;distributed-optimization-via-circuits&quot;&gt;Distributed optimization via circuits&lt;/h3&gt;

&lt;h2 id=&quot;proximal-and-operator-splitting-methods&quot;&gt;Proximal and operator splitting methods&lt;/h2&gt;

&lt;h3 id=&quot;proximal-algorithms&quot;&gt;Proximal algorithms&lt;/h3&gt;

&lt;h3 id=&quot;monotone-operators&quot;&gt;Monotone operators&lt;/h3&gt;

&lt;h3 id=&quot;monotone-operator-splitting-methods&quot;&gt;Monotone operator splitting methods&lt;/h3&gt;

&lt;h3 id=&quot;alternating-direction-method-of-multipliers-admm&quot;&gt;Alternating direction method of multipliers (ADMM)&lt;/h3&gt;

&lt;h2 id=&quot;conjugate-gradients&quot;&gt;Conjugate gradients&lt;/h2&gt;

&lt;h3 id=&quot;conjugate-gradient-method&quot;&gt;Conjugate-gradient method&lt;/h3&gt;

&lt;h3 id=&quot;truncated-newton-methods&quot;&gt;Truncated Newton methods&lt;/h3&gt;

&lt;h2 id=&quot;nonconvex-problems&quot;&gt;Nonconvex problems&lt;/h2&gt;

&lt;h3 id=&quot;l_1-methods-for-convex-cardinality-problems&quot;&gt;\(l_1\) methods for convex-cardinality problems&lt;/h3&gt;

&lt;h3 id=&quot;l_1-methods-for-convex-cardinality-problems-part-ii&quot;&gt;\(l_1\) methods for convex-cardinality problems, part II&lt;/h3&gt;

&lt;h3 id=&quot;sequential-convex-programming&quot;&gt;Sequential convex programming&lt;/h3&gt;

&lt;h2 id=&quot;branch-and-bound-methods&quot;&gt;Branch-and-bound methods&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Gordon, Geoff. CMU 10-725 Optimization Fall 2012 Lecture Slides, &lt;a href=&quot;https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture6.pdf&quot;&gt;Lecture 6&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Boyd, Stephen. &lt;a href=&quot;https://stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf&quot;&gt;Subgradient Methods: Notes for EE 364B&lt;/a&gt;, January 2007.&lt;/p&gt;</content><author><name></name></author><summary type="html">Convex Optimization Part II</summary></entry><entry><title type="html">Convex Optimization Without the Agonizing Pain</title><link href="http://johnwlambert.github.io/convex-opt/" rel="alternate" type="text/html" title="Convex Optimization Without the Agonizing Pain" /><published>2018-04-01T07:00:00-04:00</published><updated>2018-04-01T07:00:00-04:00</updated><id>http://johnwlambert.github.io/cvx</id><content type="html" xml:base="http://johnwlambert.github.io/convex-opt/">&lt;!-- 
&lt;svg width=&quot;800&quot; height=&quot;200&quot;&gt;
	&lt;rect width=&quot;800&quot; height=&quot;200&quot; style=&quot;fill:rgb(98,51,20)&quot; /&gt;
	&lt;rect width=&quot;20&quot; height=&quot;50&quot; x=&quot;20&quot; y=&quot;100&quot; style=&quot;fill:rgb(189,106,53)&quot; /&gt;
	&lt;rect width=&quot;20&quot; height=&quot;50&quot; x=&quot;760&quot; y=&quot;30&quot; style=&quot;fill:rgb(77,175,75)&quot; /&gt;
	&lt;rect width=&quot;10&quot; height=&quot;10&quot; x=&quot;400&quot; y=&quot;60&quot; style=&quot;fill:rgb(225,229,224)&quot; /&gt;
&lt;/svg&gt;
 --&gt;

&lt;h2 id=&quot;convexity&quot;&gt;Convexity&lt;/h2&gt;

&lt;h3 id=&quot;first-order-condition&quot;&gt;First-Order Condition&lt;/h3&gt;

&lt;p&gt;If \(f\) is convex and differentiable, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) + \nabla f(x)^T (y-x) \leq f(y)&lt;/script&gt;

&lt;p&gt;That is to say, a tangent line to \(f\) is a &lt;strong&gt;global underestimator&lt;/strong&gt; of the function.&lt;/p&gt;

&lt;h3 id=&quot;second-order-condition&quot;&gt;Second-Order Condition&lt;/h3&gt;

&lt;p&gt;Assuming \(f\) is twice differentiable, that is, its Hessian or second derivative \(\nabla^2f\) exists at each point in the domain of \(f\), then \(f\) is convex **if and only if ** the domain of \(f\) is convex and its Hessian is positive semidefinite.&lt;/p&gt;

&lt;p&gt;Formally, we state, for all \(x \in \mbox{dom}(f)\),&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\nabla^2 f(x) \succeq 0&lt;/script&gt;
This condition can be interpreted geometrically as the requirement that the graph of the function have positive (upward) curvature at \(x\).&lt;/p&gt;

&lt;h3 id=&quot;known-convex-and-concave-functions&quot;&gt;Known Convex and Concave Functions&lt;/h3&gt;

&lt;p&gt;Convex:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linear.&lt;/li&gt;
  &lt;li&gt;Affine. \(f(x) = Ax+b\), where \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\). This is the sum of a linear function and a constant.&lt;/li&gt;
  &lt;li&gt;Exponential. \(e^{ax}\) is convex on \(\mathbb{R}\), for any \(a \in \mathbb{R}\).&lt;/li&gt;
  &lt;li&gt;Powers. \(x^a\) is convex on \(R_{++}\) when \(a \geq 1\) or \(a \leq 0\).&lt;/li&gt;
  &lt;li&gt;Powers of absolute value. \(|x|^p\), for \(p\geq 1\), is convex on \(\mathbb{R}\).&lt;/li&gt;
  &lt;li&gt;Negative Entropy. \(x \mbox{log}(x)\) is convex on \(\mathbb{R}_{++}\).&lt;/li&gt;
  &lt;li&gt;Norms. Every norm on \(\mathbb{R}^n\) is convex.&lt;/li&gt;
  &lt;li&gt;Max function. \(f(x) = \mbox{max} { x_1, \dots, x_n } \) is convex on \(\mathbb{R}^n\).&lt;/li&gt;
  &lt;li&gt;Quadratic-over-linear function.&lt;/li&gt;
  &lt;li&gt;Log-sum-exp. \(f(x) = \mbox{log }(e^{x_1}+\cdots+e^{x_n})\) is convex on \(\mathbb{R}^n\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Concave:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Logarithm. \(\mbox{log}(x)\) concave on \(\mathbb{R}_{++}\).&lt;/li&gt;
  &lt;li&gt;Powers. \(x^a\) is concave for \(0 \leq a \leq 1\).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Geometric mean. \(f(x) = (\prod\limits_{i=1}^n x_i)^{1/n}\) is concave on \(\mathbb{R}_{++}^n\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Log-determinant. \(f(X) = \mbox{log } \mbox{det } X\) is concave on \(S_{++}^n\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;constrained-optimization-problems&quot;&gt;Constrained Optimization Problems&lt;/h2&gt;

&lt;p&gt;These problems take on a very general form, that we’ll revisit over and over again. In math, that form is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} &amp; f_0(x) &amp; \\
\mbox{subject to} &amp; f_i(x) \leq 0, &amp; i=1,\dots,m \\
&amp; h_i(x) = 0, &amp; i=1,\dots,p
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In prose, the problem is to find an \(x\) that minimizes \(f_0(x)\) among all \(x\) that satisfy the conditions \( f_i(x) \leq 0\) for \(i=1,\dots,m \) and \( h_i(x) = 0\) for \( i=1,\dots,p\).&lt;/p&gt;

&lt;p&gt;The inequalities \(f_i(x) \leq 0\) are called inequality constraints, and the equations \(h_i(x) = 0\) are called the equality constraints.&lt;/p&gt;

&lt;h3 id=&quot;the-epigraph-form-of-the-above-standard-problem-is-the-problem&quot;&gt;The Epigraph form of the above standard problem is the problem&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} &amp; t &amp; \\
\mbox{subject to} &amp; f_0(x) - t \leq 0 &amp; \\
&amp; f_i(x) \leq 0, &amp; i=1,\dots,m \\
&amp; h_i(x) = 0, &amp; i=1,\dots,p
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Geometrically, from [1]:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/epigraph_problem.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-lagrangian&quot;&gt;The Lagrangian&lt;/h2&gt;

&lt;p&gt;The basic idea in Lagrangian duality is to take the constraints in the standard problem into account by &lt;strong&gt;augmenting the objective function with a weighted sum of the constraint functions&lt;/strong&gt;. The Lagrangian associated with the standard problem is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, \lambda, \nu) = f_0(x) + \sum\limits_{i=1}^{m} \lambda_i f_i(x) + \sum\limits_{i=1}^p \nu_i h_i(x)&lt;/script&gt;

&lt;p&gt;We call \(\lambda_i\) as the Lagrange multiplier associated with the \(i\)’th &lt;strong&gt;inequality&lt;/strong&gt; constraint \(f_i(x) \leq 0\).  We refer to \(\nu_i\) as the Lagrange multiplier associated with the \(i\)’th &lt;strong&gt;equality&lt;/strong&gt; constraint \(h_i(x) = 0\).&lt;/p&gt;

&lt;h3 id=&quot;the-lagrange-dual-function&quot;&gt;The Lagrange Dual Function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\lambda, \nu) = \underset{x \in \mathcal{D}}{\mbox{inf }} L(x,\lambda,\nu)&lt;/script&gt;

&lt;p&gt;In detail, the dual function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\lambda, \nu) = \underset{x \in \mathcal{D}}{\mbox{inf }}\Bigg( f_0(x) + \sum\limits_{i=1}^{m} \lambda_i f_i(x) + \sum\limits_{i=1}^p \nu_i h_i(x) \Bigg)&lt;/script&gt;

&lt;p&gt;This is the pointwise infimum of a family of affine functions of \( (\lambda, \nu)\), so the dual function is &lt;strong&gt;concave&lt;/strong&gt;, even when the standard optimization problem is not convex.&lt;/p&gt;

&lt;h3 id=&quot;the-lagrange-dual-problem&quot;&gt;The Lagrange Dual Problem&lt;/h3&gt;

&lt;p&gt;For each pair \( (\lambda, \nu)\), the Lagrange dual function gives us a lower bound on the optimal value \(p^{*}\) of the standard optimization problem. It is a &lt;strong&gt;lower bound&lt;/strong&gt; that depends on some parameters \( (\lambda,\nu)\). But the question of interest for us is, what is the &lt;strong&gt;best&lt;/strong&gt; lower bound that can be obtained from the Lagrange dual function. This leads to the following optimization problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{ll}
\mbox{maximize} &amp; g(\lambda, \nu) \\
\mbox{subject to} &amp; \lambda \succeq 0
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We refer to this problem as the &lt;strong&gt;Lagrange dual problem&lt;/strong&gt; associated with the standard optimization problem.&lt;/p&gt;

&lt;h3 id=&quot;weak-duality&quot;&gt;Weak Duality&lt;/h3&gt;

&lt;p&gt;Let us define \( d^* \) as the optimal value of the Lagrange dual problem. This is &lt;strong&gt;the best lower bound&lt;/strong&gt; on \( p^* \) that can be obtained from the Lagrange dual function.&lt;/p&gt;

&lt;p&gt;Even if the original problem is not convex, we can always say \(d^* \leq p^*\). We call this property weak duality.&lt;/p&gt;

&lt;h3 id=&quot;slaters-constraint-qualification&quot;&gt;Slater’s Constraint Qualification&lt;/h3&gt;

&lt;p&gt;Slater’s condition is a qualification on the problem constraints. It states that there exists an \(x \in \mbox{relint}(\mathcal{D})\) such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{lll}
f_i(x) &lt; 0, &amp; i=1,\dots,m, &amp; Ax=b.
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Why is that important? Well, because if the problem is convex, and &lt;strong&gt;if Slater’s condition&lt;/strong&gt; holds, then &lt;strong&gt;strong duality&lt;/strong&gt; holds.&lt;/p&gt;

&lt;h2 id=&quot;kkt-conditions-see1-p-243&quot;&gt;KKT Conditions (See[1] p. 243)&lt;/h2&gt;

&lt;p&gt;Let \(x^{\star}\) and \(\lambda^{\star}, \nu^{\star})\) be any primal and dual optimal points with “zero duality gap” (we will explain what this means shortly).&lt;/p&gt;

&lt;p&gt;Since this is a feasible (and optimal) point, each of the \(p\) equality constraints must be fulfilled, meaning:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{ll}
h_i(x^{\star}) = 0, i = 1, \dots, p
\end{array}&lt;/script&gt;

&lt;p&gt;Also, we can certainly say that each of the inequality constraints \(f_i\) must also be fulfilled:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_i(x^{\star}) \leq 0, i = 1, \dots , m&lt;/script&gt;

&lt;p&gt;The point \(x^{\star}\) minimizes \(L(x, \lambda^{\star}, \nu^{\star}\) over \(x\in \mathbb{R}^n\), so its gradient must vanish at \(x^{\star}\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla f_0(x^{\star}) + \sum\limits_{i=1}^m \lambda_i^{\star} \nabla f_i(x^{\star} ) + \sum\limits_{i=1}^p \nu_i^{\star} \nabla h_i(x^{\star}) = 0.&lt;/script&gt;

&lt;h3 id=&quot;complementary-slackness&quot;&gt;Complementary Slackness&lt;/h3&gt;
&lt;p&gt;The next condition is called “complementary slackness”, which at first makes absolutely zero sense. However, I will explain it, from [1] and [2]&lt;/p&gt;

&lt;p&gt;Suppose that primal and dual optimal values are attained and equal (so, in
particular, strong duality holds). Let \(x^{\star}\) be a primal optimal and \( (\lambda^{\star}, \nu^{\star})\)  be a dual optimal point. This means that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{ll}
\mbox{primal value} = \mbox{dual value}, &amp; (\mbox{ bc optimal duality gap is zero})\\
f_0(x^{\star}) = g(\lambda^{\star}, \nu^{\star}) &amp; (\mbox{ by definitions of primal and dual function values})\\
= \underset{x}{\mbox{inf }} \Bigg(f_0(x) + \sum\limits_{i=1}^m \lambda_i^{\star} f_i(x) + \sum\limits_{i=1}^p \nu_i^{\star} h_i(x) \Bigg) &amp; (\mbox{ by definition of the dual function})\\
\leq f_0(x^{\star}) + \sum\limits_{i=1}^m \lambda_i^{\star} f_i(x^{\star}) + \sum\limits_{i=1}^p \nu_i^{\star} h_i(x^{\star}) &amp;  \mbox{ (bc the infimum of the Lagrangian over } x \mbox{ is less than or equal to its value at } x = x^{\star} )\\
\leq f_0(x^{\star}) &amp; (\mbox{ bc } \lambda_i^{\star} \geq 0, f_i(x^{\star}) \leq 0, i= 1,\dots,m \mbox{ and } h_i(x^{\star}) = 0, i = 1, \dots, p)
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a somewhat suprising result: all these inequalities are actually equalities.&lt;/p&gt;

&lt;p&gt;We can safely conclude from this proof that the following term is zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^m \lambda_i^{\star} + f_i(x^{\star}) = 0.&lt;/script&gt;

&lt;p&gt;At the same time, we know that at \(x^{\star}\) our inequality constraints \(f_i(x^{\star}) \leq 0\) are all fulfilled (\(\forall i)\). Thus, combining this fact with the above result, we know that every single term in the sum must also equal zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda_i^{\star} f_i(x^{\star}) = 0, i = 1, \dots , m&lt;/script&gt;

&lt;p&gt;This is &lt;strong&gt;complementary slackness&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Finally, the Lagrange dual required that each element of \(\mathbf{\lambda}\) be positive:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;λ_i^{\star} \geq 0, i = 1, \dots , m&lt;/script&gt;

&lt;p&gt;We call these 5 conditions the &lt;strong&gt;Karush-Kuhn-Tucker (KKT) conditions&lt;/strong&gt;. In short, If \(x^{\star}\) and \(u^{\star}, v^{\star}\)
are primal and dual solutions, with zero duality gap, then \(x^{\star}, \lambda^{\star}, \nu^{\star}\) satisfy the KKT conditions.&lt;/p&gt;

&lt;p&gt;In stating this, we have assumed nothing a priori about convexity of our problem, i.e. of \(f_0, f_i, h_i\).&lt;/p&gt;

&lt;h2 id=&quot;simple-example-with-matrix-vector-product-see-2&quot;&gt;Simple Example with Matrix-vector product (See [2])&lt;/h2&gt;

&lt;p&gt;Consider for \(Q \succeq 0\) (i.e. positive semidefinite),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\underset{x \in \mathbb{R}^n}{\mbox{min}} &amp; \frac{1}{2}x^TQx + c^Tx \\
\mbox{subject to} &amp; Ax = 0
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a convex problem, so by the KKT conditions, \(x\) is a solution iff&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla L(x,\lambda) = \nabla \Bigg(\frac{1}{2}x^TQx + c^Tx + \lambda^T (Ax) \Bigg) = 0 \\&lt;/script&gt;

&lt;p&gt;For \(\nabla_x\), \( Qx + c + A^T \lambda = 0  \).&lt;/p&gt;

&lt;p&gt;For \(\nabla_{\lambda}\), \(Ax=0\).&lt;/p&gt;

&lt;p&gt;This system of equations can be written in matrix form as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} Q &amp; A^T \\ A &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ \lambda \end{bmatrix} = \begin{bmatrix} -c \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;for some \(\lambda\).&lt;/p&gt;

&lt;h2 id=&quot;newtons-method&quot;&gt;Newton’s Method&lt;/h2&gt;

&lt;p&gt;Instead of minimizing the first-order Taylor approximation of a function \(f\), we may want to minimize the second-order Taylor approximation, \(f^k(\mathbf{x})\). That approximation around a point \(\mathbf{x}^k\) is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^k(\mathbf{x}) = f(\mathbf{x}^k) + \nabla f(\mathbf{x}^k)^T (\mathbf{x}-\mathbf{x}^k) + \frac{1}{2}(\mathbf{x}-\mathbf{x}^k)^T \nabla^2 f(\mathbf{x}^k) (\mathbf{x}-\mathbf{x}^k)&lt;/script&gt;

&lt;p&gt;By setting derivative w.r.t \(\mathbf{x}\) to 0, we find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla f(\mathbf{x}^k) + \nabla^2 f(\mathbf{x}^k) (\mathbf{x} - \mathbf{x}^k) = 0&lt;/script&gt;

&lt;p&gt;Multiply all terms on the left by the inverse of the Hessian, \(H^{-1}= (\nabla^2 f)^{-1} \),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\nabla^2 f)^{-1} \nabla f(\mathbf{x}^k) + (\mathbf{x}-\mathbf{x}^k) = 0&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} = \mathbf{x}^{k} - (\nabla^2 f)^{-1} \nabla f(\mathbf{x}^k)&lt;/script&gt;
The expression you may be familiar with for a Newton step is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}^{k+1} = \mathbf{x}^{k} - \frac{ \nabla f(\mathbf{x}^k) }{(\nabla^2 f)}&lt;/script&gt;

&lt;p&gt;However, a Newtons step requires not only the computation of the Hessian, but also being able to invert it. Thus, one may need to regularize \(H=\nabla^2 f\) so that your Hessian \(H\) is invertible (if the Hessian is not positive definite).&lt;/p&gt;

&lt;p&gt;If the function \(f\) is quadratic, then minimizing the 2nd order Taylor approximation given above, \(f^k\), will give us an exact minimizer of \(f\). If the function \(f\) is nearly quadratic, intuition suggests that minimizing this 2nd order expansion should be a very good estimate of the minimizer of \(f\), i.e., \(x^*\).&lt;/p&gt;

&lt;h2 id=&quot;interior-point-methods&quot;&gt;Interior Point Methods&lt;/h2&gt;

&lt;p&gt;Suppose we have the following inequality constrained optimization problem:&lt;/p&gt;

&lt;p&gt;We can approximate the problem as an &lt;strong&gt;equality constrained problem&lt;/strong&gt;. This is highly desirable because we know that Newton’s method can be applied to equality constrained problems. We can make the inequality constraints implicit in the objective:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} &amp; f_0(x) + \sum\limits_{i=1}^m I_{-}\bigg(f_i(x)\bigg) &amp; \\
\mbox{subject to} &amp; Ax = b
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The indicator function expresses our displeasure with respect to the satisfaction of the inequality constraints. If the indicator function is violated, its value becomes negative infinity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
I_{-}(u) = \begin{cases} 0 &amp; u\leq 0 \\ \infty &amp; u &gt; 0 \end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Otherwise, we ignore its presence in the objective function.  Although we were able to remove the inequality constraints, the objective function is, in general, &lt;strong&gt;not differentiable&lt;/strong&gt;, so Newton’s method cannot be applied.&lt;/p&gt;

&lt;h3 id=&quot;the-log-barrier&quot;&gt;The Log-Barrier&lt;/h3&gt;

&lt;p&gt;However, we could approximate the indicator function \(I_{-}\) with the &lt;strong&gt;log-barrier&lt;/strong&gt; function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{I}_{-}(u) = -\bigg(\frac{1}{t}\bigg) \mbox{log }(-u)&lt;/script&gt;

&lt;p&gt;Here, \(t\) is a parameter that sets the accuracy of the approximation. As \(t\) increases, the approximation becomes more accurate.&lt;/p&gt;

&lt;p&gt;A figure shows the quality of the approximation [1]:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/log_barrier_approximation.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now have a new problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\begin{array}{lll}
\mbox{minimize} &amp; f_0(x) + \sum\limits_{i=1}^m -\bigg(\frac{1}{t}\bigg) \mbox{log }\bigg(-f_i(x)\bigg) &amp; \\
\mbox{subject to} &amp; Ax = b
\end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This objective function is convex, and as long as an &lt;strong&gt;appropriate closedness condition&lt;/strong&gt; holds, Newton’s method can be used to solve it.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Stephen Boyd and Lieven Vandenberghe. 2004. &lt;a href=&quot;http://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf&quot;&gt;Convex Optimization&lt;/a&gt;. Cambridge University Press, New York, NY, USA.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gordon, Geoff. CMU 10-725 Optimization Fall 2012 Lecture Slides, &lt;a href=&quot;https://www.cs.cmu.edu/~ggordon/10725-F12/slides/16-kkt.pdf&quot;&gt;Lecture 16&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Constrained Optimization, Lagrangians, Duality, and Interior Point Methods</summary></entry><entry><title type="html">Understanding Policy Gradients</title><link href="http://johnwlambert.github.io/policy-gradients/" rel="alternate" type="text/html" title="Understanding Policy Gradients" /><published>2018-03-31T07:00:00-04:00</published><updated>2018-03-31T07:00:00-04:00</updated><id>http://johnwlambert.github.io/policy-gradients</id><content type="html" xml:base="http://johnwlambert.github.io/policy-gradients/">&lt;h2 id=&quot;policy-gradients&quot;&gt;Policy Gradients&lt;/h2&gt;

&lt;p&gt;As opposed to Deep Q-Learning, policy gradients is a method to directly output probabilities of actions. We scale gradients of actions by reward.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;policy is often easier to approximate than value function&lt;/strong&gt;. For example, if playing Pong, it is hard to assign a specific score to moving your paddle up vs. down.&lt;/li&gt;
  &lt;li&gt;Policy Gradients &lt;strong&gt;works well empirically&lt;/strong&gt; and was a key to AlphaGo’s success.&lt;/li&gt;
  &lt;li&gt;Policy gradients &lt;strong&gt;learns stochastic optimal policies&lt;/strong&gt;, which is crucial for many applications. For example, in the game of &lt;em&gt;Rock, Paper, Scissors&lt;/em&gt;, a deterministic policy is easily exploited, but a uniform random policy is optimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Training takes forever. The use of sampled data is not efficient. High variance confounds actions. Need tons of data for estimator to be good enough.&lt;/li&gt;
  &lt;li&gt;Converge to local optima. Often there are many optima.&lt;/li&gt;
  &lt;li&gt;Unlike human learning: humans can use rapid, abstract model building.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;h3 id=&quot;geometric-intuition&quot;&gt;Geometric Intuition&lt;/h3&gt;

&lt;p&gt;Suppose we have a function \(f(x)\), such that \(f(x) \geq 0 \forall x\)
For every \(x_i\), the gradient estimator \( \hat{g}_i\) tries to push up on its density.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/policy_gradients_geometric_intuition.png&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    With higher function values f(x) to the right, the probability density p(x) will be pushed up by vectors with higher magnitude on the right.  Image source: John Schulman [2].
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;math-background&quot;&gt;Math Background&lt;/h3&gt;
&lt;p&gt;To understand the proofs, you’ll need to understand 3 simple tricks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The derivative of the natural logarithm: \( \nabla_{\theta} \mbox{log }z = \frac{1}{z} \nabla_{\theta} z \)&lt;/li&gt;
  &lt;li&gt;The definition of expectation:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\mathbb{E}_{x \sim p(x)}[f(x)] = \sum\limits_x p(x) f(x) \\
\mathbb{E}_{x \sim p(x)}[f(x)] = \int_x p(x) f(x) \,dx
\end{align}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Multiplying a fraction’s numerator and denominator by some arbitrary, non-zero constant does not change the fraction’s value.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{a}{b} = \frac{a \cdot p(x)}{b \cdot p(x)}&lt;/script&gt;

&lt;h2 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;Let \(x\) be the action we will choose, \(s\) be the parameterization of our current state, \(\theta\) be the weights of our neural network. Then our neural network will directly output probabilities \(p(x \mid s; \theta)\) that depend upon the input (\(s\)) and the network weights (\( \theta \)).&lt;/p&gt;

&lt;p&gt;We start with a desire to maximize our expected reward. Our reward function is \(r(x)\), which is dependent upon the action \(x\) that we take.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_{\theta} \mathbb{E}_{x \sim p( x \mid s; \theta)} [r(x)] &amp;= \nabla_{\theta} \sum_x p(x \mid s; \theta) r(x) &amp; \text{Defn. of expectation} \\
&amp; = \sum_x r(x) \nabla_{\theta} p(x \mid s; \theta) &amp; \text{Push gradient inside sum} \\
&amp; = \sum_x r(x) p(x \mid s; \theta) \frac{\nabla_{\theta} p(x \mid s; \theta)}{p(x \mid s; \theta)}  &amp; \text{Multiply and divide by } p(x \mid s; \theta) \\
&amp; = \sum_x r(x) p(x \mid s; \theta) \nabla_{\theta} \log p(x \mid s; \theta) &amp; \text{Apply } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
&amp; = \mathbb{E}_{x \sim p( x \mid s; \theta)} [r(x) \nabla_{\theta} \log p(x \mid s; \theta) ] &amp; \text{Definition of expectation}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A similar derivation can be found in [1] or in [2].&lt;/p&gt;

&lt;p&gt;What does this mean for us? It means that if you want to maximize your expected reward, you could do something like gradient ascent. It turns out that the gradient of the expected reward is simple to compute and analytical – it is simply &lt;strong&gt;the expectation of the reward times the log probabilities of actions&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;consider-trajectories&quot;&gt;Consider Trajectories&lt;/h2&gt;
&lt;p&gt;Now, let’s consider trajectories, which are sequences of actions.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Markov Property&lt;/em&gt; states that Markov Property: the next state \(s^{\prime}\) depends on the current state \(s\) and the decision maker’s action \(a\). But given \(s\) and \(a\), it is conditionally independent of all previous states and actions.&lt;/p&gt;

&lt;p&gt;The probability of a trajectory is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\tau \mid \theta) = \underbrace{ \mu(s_0) }_{\text{initial state distribution}}  \cdot \prod\limits_{t=0}^{T-1} \Big[ \underbrace{\pi (a_t \mid s_t, \theta)}_{\text{policy}} \cdot \underbrace{p(s_{t+1},r_t \mid s_t, a_t)}_{\text{transition fn.}} \Big]&lt;/script&gt;

&lt;p&gt;Before we had:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta} \mathbb{E}_{x \sim p( x \mid s; \theta)} [r(x)] = \mathbb{E}_{x \sim p( x \mid s; \theta)} [r(x) \nabla_{\theta} \log p(x \mid s; \theta) ]&lt;/script&gt;

&lt;p&gt;Consider a stochastic generating process that gives us a trajectory of \((s,a,r)\) tuples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_{T-1}, a_{T-1} r_{T-1}, s_T)&lt;/script&gt;

&lt;p&gt;We want the expected reward over a trajectory…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} [ \underbrace{\nabla_{\theta} \log p(\tau \mid \theta)}_{\text{What is this?}} \underbrace{R(\tau)}_{\text{Reward of a trajectory}}  ]&lt;/script&gt;

&lt;p&gt;The reward over a trajectory is simple: \(R(\tau) = \sum\limits_{t=0}^{T-1}r_t\)&lt;/p&gt;

&lt;h2 id=&quot;the-probability-of-a-trajectory-given-a-policy&quot;&gt;The Probability of a Trajectory, Given a Policy&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(\tau \mid \theta) &amp;= \underbrace{ \mu(s_0) }_{\text{initial state distribution}}  \cdot \prod\limits_{t=0}^{T-1} \Big[ \underbrace{\pi (a_t \mid s_t, \theta)}_{\text{policy}} \cdot \underbrace{p(s_{t+1},r_t \mid s_t, a_t)}_{\text{transition fn.}} \Big] &amp; \text{Markov Property} \\
\mbox{log } p(\tau \mid \theta) &amp;= \mbox{log } \mu(s_0) + \mbox{log } \prod\limits_{t=0}^{T-1} \Big[ \pi (a_t \mid s_t, \theta) \cdot p(s_{t+1},r_t \mid s_t, a_t) \Big] &amp; \text{Log of product = sum of logs} \\
 &amp;= \mbox{log } \mu(s_0) +  \sum\limits_{t=0}^{T-1} \mbox{log }\Big[ \pi (a_t \mid s_t, \theta) \cdot p(s_{t+1},r_t \mid s_t, a_t) \Big] &amp; \text{Log of product = sum of logs} \\
 &amp;= \mbox{log } \mu(s_0) +  \sum\limits_{t=0}^{T-1} \Big[ \mbox{log }\pi (a_t \mid s_t, \theta) + \mbox{log } p(s_{t+1},r_t \mid s_t, a_t) \Big] &amp; \text{Log of product = sum of logs} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;maximizing-expected-return-of-a-trajectory&quot;&gt;Maximizing Expected Return of a Trajectory&lt;/h2&gt;
&lt;p&gt;As we discussed earlier, in order to perform gradient ascent, we’ll need to be able to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} [ \underbrace{\nabla_{\theta} \log p(\tau \mid \theta)}_{\text{Derived Above}} \underbrace{R(\tau)}_{\text{Reward of a trajectory}}  ]&lt;/script&gt;

&lt;p&gt;We have just shown that the inner term is simply:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log p(\tau \mid \theta) = \mbox{log } \mu(s_0) +  \sum\limits_{t=0}^{T-1} \Big[ \mbox{log }\pi (a_t \mid s_t, \theta) + \mbox{log } p(s_{t+1},r_t \mid s_t, a_t) \Big]&lt;/script&gt;

&lt;p&gt;We need its gradient. Only one term depends upon \(\theta\) above, so all other terms fall out in the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} \Big[ R(\tau) \nabla_{\theta} \sum\limits_{t=0}^{T-1} \mbox{log }\pi (a_t \mid s_t, \theta) \Big]&lt;/script&gt;

&lt;p&gt;This concludes the derivation of the Policy Gradient Theorem for entire trajectories.&lt;/p&gt;

&lt;h2 id=&quot;the-reinforce-algorithm&quot;&gt;The REINFORCE Algorithm&lt;/h2&gt;

&lt;p&gt;The REINFORCE algorithm is a Monte-Carlo Policy-Gradient Method. Below we describe the episodic version:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input: a differentiable policy parameterization \(\pi(a \mid s, \theta)\)&lt;/li&gt;
  &lt;li&gt;Initialize policy parameter \(\theta \in \mathbb{R}^d\)&lt;/li&gt;
  &lt;li&gt;Repeat forever:
    &lt;ul&gt;
      &lt;li&gt;Generate an episode \(S_0\), \(A_0\), \(R_1\), … , \(S_{T−1}\), \(A_{T−1}\), \(R_T\) , following \( \pi(\cdot \mid \cdot, \theta) \)&lt;/li&gt;
      &lt;li&gt;For each step of the episode \(t = 0, … , T-1\):
        &lt;ul&gt;
          &lt;li&gt;\(G \leftarrow\) return from step \(t\)&lt;/li&gt;
          &lt;li&gt;\( \theta \leftarrow \theta + \alpha \gamma^t G \nabla_{\theta} \mbox{ ln }\pi(A_t \mid S_t, \theta) \)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See page 271 of [1] for more details. Andrej Karpathy has a &lt;a href=&quot;https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5&quot;&gt;very simple 130-line script here&lt;/a&gt; that illustrates this in Python.&lt;/p&gt;

&lt;h2 id=&quot;reinforce-with-a-baseline&quot;&gt;REINFORCE With a Baseline&lt;/h2&gt;

&lt;!--- TODO: FIX THIS PART --&gt;

&lt;p&gt;In the section on geometric intuition, we discussed how:
Suppose we have a function \(f(x)\), such that \(f(x) \geq 0 \forall x\)
For every \(x_i\), the gradient estimator \( \hat{g}_i\) tries to push up on its density.&lt;/p&gt;

&lt;p&gt;However, we really want to only push up on the density for better-than-average \(x_i\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_{\theta} \mathbb{E}_x [f(x)] &amp;= \nabla_{\theta} \mathbb{E}_x [f(x) -b] \\
&amp;= \mathbb{E}_x \Big[ \nabla_{\theta}  \mbox{log } p(x \mid \theta) (f(x) -b) \Big]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A near optimal choice of baseline \(b\) is always the mean return, \( \mathbb{E}[f(x)]\).&lt;/p&gt;

&lt;h2 id=&quot;actor-critic&quot;&gt;Actor Critic&lt;/h2&gt;

&lt;p&gt;Monte Carlo Policy Gradient has high variance. What if we used a critic to estimate the action-value function?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_w(s,a) \approx Q^{\pi_\theta}(s,a)&lt;/script&gt;

&lt;p&gt;The Actor contains the policy and is the agent that makes decisions. The Critic makes no decisions or actions, but merely watches what the critic does and evaluates if it was good or bad.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-with-approximate-policy-gradient&quot;&gt;Actor-Critic with Approximate Policy Gradient&lt;/h2&gt;

&lt;p&gt;The critic says, “Hey, I think if you go in this direction, you can actually do better!”&lt;/p&gt;

&lt;p&gt;In each step, move a little bit in the direction that the critic says is good or bad.&lt;/p&gt;

&lt;h2 id=&quot;advantage-function&quot;&gt;Advantage Function&lt;/h2&gt;

&lt;p&gt;We remember the state-value function \(V^{\pi}(s)\), which tells us the goodness of a state.
We remember the state-action function \(Q(s,a)\).     &lt;br /&gt;
We want an advantage function \(A^{\pi}(s,a)\)  to tell us how much better is action \(a\) than what the policy \(\pi\) would have done otherwise:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^{\pi}(s,a) = Q(s,a) - V^{\pi}(s)&lt;/script&gt;

&lt;p&gt;Did things get better after one time step? Of course, a Critic could estimate both. But there’s a better way!&lt;/p&gt;

&lt;h3 id=&quot;td-error-as-advantage-function&quot;&gt;TD Error as Advantage Function&lt;/h3&gt;

&lt;p&gt;Make the good trajectories more probable, make the bad trajectories less probable (high variance)&lt;/p&gt;

&lt;p&gt;Monte Carlo&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;unbiased but high variance&lt;/li&gt;
  &lt;li&gt;conflates actions over whole trajectory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TD Learning&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;introduces bias (treating guess as truth) but estimate have lower variance&lt;/li&gt;
  &lt;li&gt;incorporates 1 step&lt;/li&gt;
  &lt;li&gt;usually more efficient&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the true value function \(V^{\pi_{\theta}}(s)\), the TD error \( \delta^{\pi_{\theta}}(s)\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta^{\pi_{\theta}}(s) = r + \gamma V^{\pi_{\theta}}(s^{\prime}) - V^{\pi_{\theta}}(s)&lt;/script&gt;

&lt;p&gt;is an unbiased estimate of the advantage function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbb{E}_{\pi_{\theta}} \Big[\delta^{\pi_{\theta}}(s) \mid s,a \Big] &amp;= \mathbb{E}_{\pi_{\theta}} \Big[ r + \gamma V^{\pi_{\theta}}(s^{\prime}) - V^{\pi_{\theta}}(s) \Big] \\
= Q^{\pi_{\theta}}(s,a) - V^{\pi_{\theta}}(s) \\
= A^{\pi_{\theta}}(s,a)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;trpo-from-1st-order-to-2nd-order&quot;&gt;TRPO: From 1st Order to 2nd Order&lt;/h2&gt;

&lt;p&gt;We don’t have access to the objective function in analytic form. However, we can estimate the gradient as the expected return of our policy by sampling trajectories. The gradient ascent update is correct if we make an infinitesimally small change.&lt;/p&gt;

&lt;p&gt;A local approximation is only accurate closeby:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/trpo_local_approx.png&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    With higher function values f(x) to the right, the probability density p(x) will be pushed up by vectors with higher magnitude on the right.  Image source: John Schulman [2].
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Trust-region methods penalize large steps. Let’s call our surrogate objective \(L_{\pi_{old}}(\pi)\).
Let’s call our true objective \( \eta(\pi)\). We will add a penalty for moving \( \theta \) from \( \theta_{old} \) (KL Divergence between the old and new policy).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta(\pi) \geq L_{\pi_{old}}(\pi) - C \cdot \max\limits_s KL \Big[ \pi_{old}(\cdot \mid s), \pi(\cdot \mid s) \Big]&lt;/script&gt;

&lt;p&gt;Monotonic improvement will be guaranteed.Since the max is not good for Monte Carlo estimation, a practical approximation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta(\pi) \geq L_{\pi_{old}}(\pi) - C \cdot \mathbb{E}_{s\sim p_{old}} KL \Big[ \pi_{old}(\cdot \mid s), \pi(\cdot \mid s) \Big]&lt;/script&gt;

&lt;h3 id=&quot;second-order-optimization-method&quot;&gt;Second Order Optimization Method&lt;/h3&gt;

&lt;p&gt;What if we used a 2nd order optimization method instead of our vanilla 1st order optimization method?
We would compute the Hessian of the KL divergence, not of our objective&lt;/p&gt;

&lt;p&gt;Hessian computation is extremely expensive…( 100,000 x 100,000 )&lt;/p&gt;

&lt;h3 id=&quot;truncated-natural-gradient-policy-algorithm&quot;&gt;Truncated Natural Gradient Policy Algorithm&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;for iteration 1,2,..  do
    &lt;ul&gt;
      &lt;li&gt;Run policy for \(T\) timesteps or \(N\) trajectories&lt;/li&gt;
      &lt;li&gt;Estimate advantage function at all timesteps&lt;/li&gt;
      &lt;li&gt;Compute policy gradient \(g\)&lt;/li&gt;
      &lt;li&gt;Use Conjugate Gradient (with Hessian-vector products) to compute \( H^{-1}g \)&lt;/li&gt;
      &lt;li&gt;Update policy parameter \( \theta = \theta_{old} + \alpha H^{-1}g \)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;trust-region-policy-optimization-trpo&quot;&gt;Trust Region Policy Optimization (TRPO)&lt;/h3&gt;

&lt;p&gt;The Truncated Natural Policy Gradient algorithm was an unconstrained problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta(\pi) \geq L_{\pi_{old}}(\pi) - C \cdot \mathbb{E}_{s\sim p_{old}} KL \Big[ \pi_{old}(\cdot \mid s), \pi(\cdot \mid s) \Big]&lt;/script&gt;

&lt;p&gt;TRPO is a constrained problem (otherwise, algorithm is identical):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mbox{max } &amp; L_{\pi_{old}}(\pi) \\
\mbox{subject to } &amp; \mathbb{E}_{s\sim p_{old}} KL \Big[ \pi_{old}(\cdot \mid s), \pi(\cdot \mid s) \Big] \leq \delta
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Easier to set hyper parameter \(\delta\) rather than \(C\). \(\delta\) can remain fixed over whole learning process… Simply rescale step by some scalar.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;[1] Richard Sutton and Andrew Barto. Reinforcement Learning:
An Introduction. Chapter 13. &lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;http://incompleteideas.net/book/bookdraft2017nov5.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] John Schulman. Deep Reinforcement Learning: Policy Gradients and Q-Learning.Bay Area Deep Learning School, September 24, 2016.&lt;/p&gt;

&lt;p&gt;[3] Andrej Karpathy. Deep Reinforcement Learning: Pong from Pixels. May 31, 2016. &lt;a href=&quot;http://karpathy.github.io/2016/05/31/rl/&quot;&gt;http://karpathy.github.io/2016/05/31/rl/&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Including Trust-Region Variant (Levenberg-Marquardt)</summary></entry><entry><title type="html">Gauss-Newton Optimization in 10 Minutes</title><link href="http://johnwlambert.github.io/gauss-newton/" rel="alternate" type="text/html" title="Gauss-Newton Optimization in 10 Minutes" /><published>2018-03-31T07:00:00-04:00</published><updated>2018-03-31T07:00:00-04:00</updated><id>http://johnwlambert.github.io/gaussnewton</id><content type="html" xml:base="http://johnwlambert.github.io/gauss-newton/">&lt;h2 id=&quot;unconstrained-optimization&quot;&gt;Unconstrained Optimization&lt;/h2&gt;

&lt;h3 id=&quot;the-gauss-newton-method&quot;&gt;The Gauss-Newton Method&lt;/h3&gt;

&lt;p&gt;Suppose our residual is no longer affine, but rather nonlinear. We want to minimize \(\lVert r(x) \rVert^2\). Generally speaking, we cannot solve this problem, but rather can use good heuristics to find local minima.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Start from initial guess for your solution&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;(1) Linearize \(r(x)\) around current guess \(x^{(k)}\). This can be accomplished by using a Taylor Series and Calculus (Standard Gauss-Newton), or one can use a least-squares fit to the line.&lt;/li&gt;
  &lt;li&gt;(2) Solve least squares for linearized objective, get \(x^{(k+1)}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The linearized residual \(r(x)\) will resemble:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(x) \approx r(x^{(k)}) + Dr(x^{(k)}) (x-x^{(k)})&lt;/script&gt;

&lt;p&gt;where \(Dr\) is the Jacobian, meaning \( (Dr)_{ij} = \frac{\partial r_i}{\partial x_j}\)&lt;/p&gt;

&lt;p&gt;Distributing the rightmost product, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(x) \approx Dr(x^{(k)})x - \bigg(Dr(x^{(k)}) (x^{(k)}) - r(x^{(k)}) \bigg)&lt;/script&gt;

&lt;p&gt;With a single variable \(x\), we can re-write the above equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(x) \approx A^{(k)}x - b^{(k)}&lt;/script&gt;

&lt;h3 id=&quot;levenberg-marquardt-algorithm-trust-region-gauss-newton-method&quot;&gt;Levenberg-Marquardt Algorithm (Trust-Region Gauss-Newton Method)&lt;/h3&gt;

&lt;p&gt;In Levenberg-Marquardt, we have add a term to the objective function to emphasize that we should not move so far from \( \theta^{(k)} \) that we cannot trust the affine approximation. We often refer to this concept as remaining within a “trust region” (&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;TRPO&lt;/a&gt; is named after the same concept). Thus, we wish 
&lt;script type=&quot;math/tex&quot;&gt;|| \theta - \theta^{(k)} ||^2&lt;/script&gt;
 to be small. Our new objective is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||A^{(k)} \theta - b^{(k)}||^2 + \lambda^{(k)} || \theta − \theta^{(k)}||^2&lt;/script&gt;

&lt;p&gt;This objective can be written inside a single \(\ell_2\)-norm, instead using two separate \(\ell_2\)-norms:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \end{bmatrix} ||^2&lt;/script&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Suppose we have some input data \(x\) and labels \(y\). Our prediction function could be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{f} = x^T\theta_1 + \theta_2&lt;/script&gt;

&lt;p&gt;Suppose at inference time we use \(f(x) = \mbox{sign }\bigg(\hat{f}(x)\bigg)\), where \(\mbox{sign }(a) = +1\) for \(a \geq 0\) and −1 for \(a &amp;lt; 0\). At training time, we use its smooth (and differentiable) approximation, the hyperbolic tangent, tanh:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}&lt;/script&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The gradient of \(\mbox{tanh}\): \(\nabla_x \mbox{tanh } = 1-\mbox{tanh }(x)^2\). We call this \(\phi^{\prime}\) in code:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;phiprime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.^&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Suppose our objective function is the MSE loss, with a regularization term:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum\limits_{i=1}^{N} \Bigg(y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)\Bigg)^2 + \mu ||\theta_1||^2&lt;/script&gt;

&lt;p&gt;The residual for a single training example \(i\) is \(r_i\) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i - \phi\big(x_i^T\theta_1 + \theta_2\big)&lt;/script&gt;

&lt;p&gt;For a vector of training examples \(\mathbf{X}\) and labels \(\mathbf{Y}\), our nonlinear residual function is:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To linearize the residual, we compute its Jacobian \(Dr(\theta_1,\theta_2)\) via matrix calculus:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial r_i}{\partial \theta_1} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)x_i^T&lt;/script&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;jacobian_0_entr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phiprime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'*theta(1:400)+theta(end))* X(:,i)'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial r_i}{\partial \theta_2} = -\phi^{\prime}(x_i^T\theta_1 + \theta_2)&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;jacobian_1_entr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phiprime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The the full Jacobian evaluated at a certain point \(X_i\) is just these stacked individual entries:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Dr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Dr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jacobian_0_entr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jacobian_1_entr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let \(\theta= \begin{bmatrix} \theta_1^T &amp;amp; \theta_2 \end{bmatrix}^T \in \mathbb{R}^{401}\). The linearized residual follows the exact form outlined in the Gauss-Newton section above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(\theta) \approx A^{(k)}\theta - b^{(k)}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b^{(k)} = A^{(k)} \theta^{(k)} - r\bigg(\theta^{(k)}\bigg)&lt;/script&gt;

&lt;p&gt;In code, this term is computed as:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_k_temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;% computed above&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b_k_temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We solve a least-squares problem in every iteration, with a 3-part objective function (penalizing the residual, large step sizes, and also large \(\theta_1\)-norm weights):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
|| \begin{bmatrix} A^{(k)} \\ \sqrt{\lambda^{(k)}} I_{401} \\ \begin{bmatrix} \sqrt{\mu} I_{400 } &amp; 0\end{bmatrix} \end{bmatrix} \theta - \begin{bmatrix} b^{(k)} \\ \sqrt{\lambda^{(k)}} \theta^{(k)} \\ 0 \end{bmatrix} ||^2. %]]&gt;&lt;/script&gt;

&lt;p&gt;We represent the left term by&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_k_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and the right term by&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;b_k = [b_k_temp; sqrt(lambda(itr))*theta; zeros(length(theta)-1,1)];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We solve for the next iterate of \(\theta\) with the pseudo-inverse:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;theta_temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The full algorithm might resemble:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%regularization coefficient&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%initial lambda for Levenberg-Marquardt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;401&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%initial value for theta (last entry is theta_2)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%calculate Jacobian at the current iteration (see code above)&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;%linearize the objective function (see code above)&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;% stopping condition ...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Including Trust-Region Variant (Levenberg-Marquardt)</summary></entry></feed>