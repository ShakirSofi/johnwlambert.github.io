<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://johnwlambert.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://johnwlambert.github.io/" rel="alternate" type="text/html" /><updated>2019-01-22T18:47:48-05:00</updated><id>http://johnwlambert.github.io/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">Graph Cuts</title><link href="http://johnwlambert.github.io/2019/01/16/graph-cuts.html" rel="alternate" type="text/html" title="Graph Cuts" /><published>2019-01-16T00:00:00-05:00</published><updated>2019-01-16T00:00:00-05:00</updated><id>http://johnwlambert.github.io/2019/01/16/graph-cuts</id><content type="html" xml:base="http://johnwlambert.github.io/2019/01/16/graph-cuts.html">&lt;p&gt;http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Factor Graphs for SLAM and SfM</title><link href="http://johnwlambert.github.io/factor-graphs/" rel="alternate" type="text/html" title="Factor Graphs for SLAM and SfM" /><published>2018-12-28T06:00:00-05:00</published><updated>2018-12-28T06:00:00-05:00</updated><id>http://johnwlambert.github.io/factor-graphs-slam</id><content type="html" xml:base="http://johnwlambert.github.io/factor-graphs/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whyliegroups&quot;&gt;Factor Graphs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#liegroups&quot;&gt;Factor Densities&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#son&quot;&gt;MAP Inference on Factor Graphs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so2&quot;&gt;Factor Graph Variable Elimanation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;whyliegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;factor-graphs-for-robot-vision&quot;&gt;Factor Graphs for Robot Vision&lt;/h2&gt;

&lt;p&gt;A factor graph is a probabilistic graphical model, theory which represents the marriage of graph theory with probability.&lt;/p&gt;

&lt;p&gt;To use factor graphs for computer vision in robotics, we will need another tool: numerical linear algebra. These problems can be reformulated as very large least-squares problems. In fact, the size of the matrix that would not need be to inverted to solve the system of equations makes these problems solvable only by iterative methods, rather than direct methods.&lt;/p&gt;

&lt;h2 id=&quot;factor-graphs-background&quot;&gt;Factor Graphs: Background&lt;/h2&gt;

&lt;p&gt;A factor graph is a probabilistic graphical model (in the same family with Markov Random Fields (MRFs) and Bayesian Networks). It is an undirected graph (meaning there are no parents or topological ordering).&lt;/p&gt;

&lt;p&gt;Bayesian Networks are directed graphs where edges in the graph are associated with conditional probability distributions (CPDs), assigning the probability of children in the graph taking on certain values based on the values of the parents.&lt;/p&gt;

&lt;p&gt;In undirected models like MRFs and Factor Graphs, instead of specifying CPDs, we specify (non-negative) potential functions (or factors) over sets of variables associated with cliques (complete subgraphs) C of the graph.  Like Conditional Prob. Distributions, a factor/potential can be represented as a table, but it is not normalized (does not sum to one).&lt;/p&gt;

&lt;p&gt;A factor graph is a bipartite undirected graph with variable nodes (circles) and factor nodes (squares). Edges are only between the variable nodes and the factor nodes.&lt;/p&gt;

&lt;p&gt;The variable nodes can take on certain values, and the likelihood of that event for a set of variables is expressed in the potential (factor node) attached to those variables.  Each factor node is associated with a single potential, whose scope is the set of variables that are neighbors in the factor graph.&lt;/p&gt;

&lt;p&gt;A small example might make this clearer. Suppose we have a group of four people: Alex, Bob, Catherine, David A=Alex’s hair color (red, green, blue)&lt;/p&gt;

&lt;p&gt;B=Bob’s hair color&lt;/p&gt;

&lt;p&gt;C=Catherine’s hair color&lt;/p&gt;

&lt;p&gt;D=David’s hair color&lt;/p&gt;

&lt;p&gt;Alex and Bob are friends, Bob and Catherine are friends, Catherine and David are friends, David and Alex are friends&lt;/p&gt;

&lt;p&gt;Friends never have the same hair color!&lt;/p&gt;

&lt;p&gt;It turns out that this distribution p cannot be represented (perfectly) by any Bayesian network. But it is succinctly represented by a Factor Graph or MRF.
https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmsma7unw07s/Screen_Shot_20181002_at_11.48.05_PM.png&lt;/p&gt;

&lt;p&gt;The Factor Graph distribution is same as the MRF – this is just a different graph data structure.&lt;/p&gt;

&lt;h2 id=&quot;factor-densities&quot;&gt;Factor Densities&lt;/h2&gt;

&lt;p&gt;The most often used probability densities involve the multi-variate Gaussian distribution [1], whose density is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x \sim p(x) = \mathcal{N}(\mu, \Sigma) = 
\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}\mbox{exp}
\Big\{ -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \Big\}&lt;/script&gt;

&lt;p&gt;For the sake of brevity, we can write this as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x \sim p(x) = \mathcal{N}(\mu, \Sigma) = 
\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}\mbox{exp}
\Big\{ -\frac{1}{2} \|(x - \mu)^T \|_{\Sigma^{-1}}^2 \Big\}&lt;/script&gt;

&lt;h2 id=&quot;map-inference-on-factor-graphs&quot;&gt;MAP Inference on Factor Graphs&lt;/h2&gt;

&lt;p&gt;MAP inference in SLAM is exactly the process of determining those
values for the unknowns that maximally agree with the information
present in the uncertain measurements [1]&lt;/p&gt;

&lt;p&gt;We wish to solve for &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, the position of the robot at all timesteps.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
X^{MAP} &amp;= \mbox{arg }\underset{X}{\mbox{max }} \phi(X) &amp; \mbox{Maximum likelihood Defn.} \\
 &amp;= \mbox{arg }\underset{X}{\mbox{max }} \prod\limits_i \phi_i(X_i) &amp; \mbox{text} \\
  &amp;= \mbox{arg }\underset{X}{\mbox{max }} \prod\limits_i \mbox{exp } \bigg\{ -\frac{1}{2}\|h_i(X_i) - z_i\|_{\Sigma_i}^2 \bigg\} &amp; \mbox{Use Gaussian prior and likelihood factors} \\
    &amp;= \mbox{arg }\underset{X}{\mbox{max }} \mbox{log } \prod\limits_i  \mbox{exp } \bigg\{ -\frac{1}{2}\|h_i(X_i) - z_i\|_{\Sigma_i}^2 \bigg\} &amp; \mbox{Log is monotonically increasing} \\
     &amp;= \mbox{arg }\underset{X}{\mbox{max }} \sum\limits_i \mbox{log }   \mbox{exp } \bigg\{ -\frac{1}{2}\|h_i(X_i) - z_i\|_{\Sigma_i}^2 \bigg\} &amp; \mbox{Log of a product is a sum of logs.} \\
       &amp;= \mbox{arg }\underset{X}{\mbox{max }} \sum\limits_i  \bigg( -\frac{1}{2}\|h_i(X_i) - z_i\|_{\Sigma_i}^2 \bigg) &amp; \mbox{Simplify } f(f^{-1}(x))=x \\
        &amp;= \mbox{arg }\underset{X}{\mbox{min }} \sum\limits_i  \bigg( \frac{1}{2}\|h_i(X_i) - z_i\|_{\Sigma_i}^2 \bigg) &amp; \mbox{Max of } -f(x) \mbox{ is min of } f(x) \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We’ve reduce the problem to minimization of a sum of nonlinear least-squares.&lt;/p&gt;

&lt;p&gt;Along the way, we decided to maximize the log-likelihood instead of the likelihood because the argument that maximizes the log of a function also maximizes the function itself.&lt;/p&gt;

&lt;h2 id=&quot;factor-graph-variable-elimanation&quot;&gt;Factor Graph Variable Elimanation&lt;/h2&gt;

&lt;p&gt;If you’re wondering about the variable elimination part, we choose subsets of variables connected by factors and start combining them by taking the product of their factors and marginalizing out variables.&lt;/p&gt;

&lt;p&gt;In the table above, we have variables as the columns and factors as the rows. We can combines factors progressively to involve more and more variables.&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmtgs7d71g2p/Screen_Shot_20181003_at_2.05.56_PM.png&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frank Dellaert and Michael Kaess. &lt;em&gt;Factor Graphs for Robot Perception&lt;/em&gt;. Foundations and Trends in Robotics, Vol. 6, No. 1-2 (2017) 1–139. &lt;a href=&quot;https://www.ri.cmu.edu/wp-content/uploads/2018/05/Dellaert17fnt.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Stefano Ermon.&lt;/p&gt;</content><author><name></name></author><summary type="html">Gaussian densities, direct methods, iterative methods</summary></entry><entry><title type="html">Lie Groups and Rigid Body Kinematics</title><link href="http://johnwlambert.github.io/lie-groups/" rel="alternate" type="text/html" title="Lie Groups and Rigid Body Kinematics" /><published>2018-12-28T06:00:00-05:00</published><updated>2018-12-28T06:00:00-05:00</updated><id>http://johnwlambert.github.io/lie-groups</id><content type="html" xml:base="http://johnwlambert.github.io/lie-groups/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whyliegroups&quot;&gt;Why do we need Lie Groups?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#liegroups&quot;&gt;Lie Groups&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#son&quot;&gt;SO(N)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so2&quot;&gt;SO(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so3&quot;&gt;SO(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se2&quot;&gt;SE(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se3&quot;&gt;SE(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conjugation&quot;&gt;Conjugation in Group Theory&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lie-algebra&quot;&gt;The Lie Algebra&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;whyliegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-lie-groups&quot;&gt;Why do we need Lie Groups?&lt;/h2&gt;

&lt;p&gt;Rigid bodies have a state which consists of position and orientation. When sensors are placed on a rigid body (e.g. a robot), they provide measurements in the body frame. Suppose we wish to take a measurement &lt;script type=&quot;math/tex&quot;&gt;y_b&lt;/script&gt; from the body frame and move it to the world frame, &lt;script type=&quot;math/tex&quot;&gt;y_w&lt;/script&gt;. We can do this via left multiplication with a transformation matrix &lt;script type=&quot;math/tex&quot;&gt;{}^{w}T_{b}&lt;/script&gt;, a member of the matrix Lie groups, that transports the point from one space to another space:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_w = {}^{w}T_{b} y_b&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;liegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lie-groups&quot;&gt;Lie Groups&lt;/h2&gt;

&lt;p&gt;When we are working with pure rotations, we work with Special Orthogonal groups, &lt;script type=&quot;math/tex&quot;&gt;SO(\cdot)&lt;/script&gt;. When we are working with a rotation and a translation together, we work with Special Euclidean groups &lt;script type=&quot;math/tex&quot;&gt;SE(\cdot)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Lie Groups are unique because they are &lt;strong&gt;both a group and a manifold&lt;/strong&gt;. They are continuous manifolds in high-dimensional spaces, and have a group structure. I’ll describe them in more detail below.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;son&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;son&quot;&gt;SO(N)&lt;/h3&gt;

&lt;p&gt;Membership in the Special Orthogonal Group &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt; requires two matrix properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;R^TR = I&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{det}(R) = +1&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives us a very helpful property: &lt;script type=&quot;math/tex&quot;&gt;R^{-1} = R^T&lt;/script&gt;, so the matrix inverse is as simple as taking the transpose.  We will generally work with &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;N=2,3&lt;/script&gt;, meaning the matrices are rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{2 \times 2}&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;These rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; are not commutative.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;so2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so2&quot;&gt;SO(2)&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is a 1D manifold living in a 2D Euclidean space, e.g. moving around a circle.  We will be stuck with singularities if we use 2 numbers to parameterize it, which would mean kinematics break down at certain orientations.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is the space of orthogonal matrices that corresponds to rotations in the plane.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A simple example&lt;/strong&gt;:
Let’s move from the body frame &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; to a target frame &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_t = {}^tR_b(\theta) P_b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
5 \mbox{ cos} (\theta) \\
5 \mbox{ sin} (\theta)
\end{bmatrix} = \begin{bmatrix} cos(\theta) &amp; -sin(\theta) \\ sin(\theta) &amp; cos(\theta) \end{bmatrix} * \begin{bmatrix} 5 \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;As described in [3], another way to think of this is to consider that a robot can be rotated counterclockwise by some angle &lt;script type=&quot;math/tex&quot;&gt;\theta \in [0,2 \pi)&lt;/script&gt; by mapping every &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x, y) \rightarrow (x \mbox{ cos } \theta − y \mbox{ sin } \theta, x \mbox{ sin } \theta + y \mbox{ cos } \theta).&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;so3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so3&quot;&gt;SO(3)&lt;/h3&gt;

&lt;p&gt;There are several well-known parameterizations of &lt;script type=&quot;math/tex&quot;&gt;R \in SO(3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1.) &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt; full rotation matrix, 9 parameters – there must be 6 constraints&lt;/li&gt;
  &lt;li&gt;(2.) Euler angles, e.g. &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt;, so 3 parameters&lt;/li&gt;
  &lt;li&gt;(3.) Angle-Axis parameters &lt;script type=&quot;math/tex&quot;&gt;(\vec{a}, \phi)&lt;/script&gt;, which is 4 parameters and 1 constraint (unit length)&lt;/li&gt;
  &lt;li&gt;(4.) Quaternions (&lt;script type=&quot;math/tex&quot;&gt;q_0,q_1,q_2,q_3)&lt;/script&gt;, 4 parameters and 1 constraint (unit length)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are only 3 degrees of freedom in describing a rotation. But this object doesn’t live in 3D space. It is a 3D manifold, embedded in a 4-D Euclidean space.&lt;/p&gt;

&lt;p&gt;Parameterizations 1,3,4 are overconstrained, meaning they employ more parameters than we strictly need. With overparameterized representations, we have to do extra work to make sure we satisfy the constraints of the representation.&lt;/p&gt;

&lt;p&gt;As it turns out &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; cannot be parameterized by only 3 parameters in a non-singular way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Euler Angles&lt;/strong&gt;
One parameterization of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; is to imagine three successive rotations around different axes. The Euler angles encapsulate yaw-pitch-roll: first, a rotation about the z-axis (yaw, &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;). Then, a rotation about the pitch axis by &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (via right-hand rule), and finally we perform a roll by &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/euler_angles.jpg&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    The Euler angles.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The sequence of successive rotations is encapsulated in &lt;script type=&quot;math/tex&quot;&gt;{}^{w}R_b&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{w}R_b = R_R(\phi) R_P(\theta) R_Y (\psi)&lt;/script&gt;

&lt;p&gt;As outlined in [3], these successive rotations by &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt; are defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{yaw} = R_y (\psi) = \begin{bmatrix}
\mbox{cos} \psi &amp; -\mbox{sin} \psi  &amp; 0 \\
\mbox{sin} \psi &amp; \mbox{cos} \psi &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{pitch} = R_p (\theta) = \begin{bmatrix}
\mbox{cos} \theta &amp; 0 &amp; \mbox{sin} \theta \\
 0 &amp; 1 &amp; 0 \\
 -\mbox{sin} \theta &amp; 0 &amp; \mbox{cos} \theta \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{roll} = R_R (\phi) = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; \mbox{cos} \phi &amp; -\mbox{sin} \phi \\
0 &amp; \mbox{sin} \phi &amp;  \mbox{cos} \phi \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;You have probably noticed that each rotation matrix &lt;script type=&quot;math/tex&quot;&gt;\in \mathbf{R}^{3 \times 3}&lt;/script&gt; above is a simple extension of the 2D rotation matrix from &lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt;. For example, the yaw matrix &lt;script type=&quot;math/tex&quot;&gt;R_{yaw}&lt;/script&gt; performs a 2D rotation with respect to the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; coordinates while leaving the &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; coordinate unchanged [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se2&quot;&gt;SE(2)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;3 \times 3&lt;/script&gt; matrices, moving a point in homogenous coordinates to a new frame. It is important to remember that this represents a rotation followed by a translation (not the other way around).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T = \begin{bmatrix} x_w \\ y_w \\ 1 \end{bmatrix} = \begin{bmatrix}
 R_{2 \times 2}&amp; &amp; t_{2 \times 1}  \\
&amp; \ddots &amp; \vdots  \\
0 &amp; 0 &amp; 1
\end{bmatrix} * \begin{bmatrix} x_b \\ y_b \\ 1 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;By adding an extra dimension to the input points and transformation matrix &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; the translational part of the transformation is absorbed [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se3&quot;&gt;SE(3)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(3)&lt;/script&gt; is a 6-dimensional manifold. Its dimensions is exactly the number of degrees of freedom of a free-floating rigid body in space [3]. &lt;script type=&quot;math/tex&quot;&gt;SE(3)&lt;/script&gt; can be parameterized with a &lt;script type=&quot;math/tex&quot;&gt;4 \times 4&lt;/script&gt; matrix as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; R_{3 \times 3} &amp; &amp;  t_{3 \times 1} \\
&amp; &amp; &amp; \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;conjugation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conjugation-in-group-theory&quot;&gt;Conjugation in Group Theory&lt;/h2&gt;

&lt;p&gt;Surprisingly, movement in &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; can always be achieved by moving somewhere, making a rotation there, and then moving back.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SE(2) = (\cdot) SO(2) (\cdot)&lt;/script&gt;

&lt;p&gt;If we move to a point by vector movement &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, essentially we perform: &lt;script type=&quot;math/tex&quot;&gt;B-p&lt;/script&gt;, then go back with &lt;script type=&quot;math/tex&quot;&gt;p + \dots&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
B^{\prime} = \begin{bmatrix}
R &amp; t \\
0 &amp; 1
\end{bmatrix}_B = \begin{bmatrix}
I &amp; p \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
R &amp; 0 \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
I &amp; -p \\
0 &amp; 1
\end{bmatrix}_B %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;lie-algebra&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;connecting-spatial-coordinates-and-spatial-velocities-the-lie-algebra&quot;&gt;Connecting Spatial Coordinates and Spatial Velocities: The Lie Algebra&lt;/h2&gt;

&lt;p&gt;The Lie Algebra maps spatial coordinates to spatial velocity.&lt;/p&gt;

&lt;p&gt;In the case of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt;, the Lie Algebra is the space of skew-symmetric matrices&lt;/p&gt;

&lt;p&gt;Rigid Body Kinematics: we want a differential equation (ODE) that links &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{}^{^w}R_b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;in particular,
&lt;script type=&quot;math/tex&quot;&gt;{}^{^w} \dot{R}_r = f({}^{^w}R_b, \vec{\omega})&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\frac{ \partial }{\partial t}(R^R = I)
 \\
\dot{R}^TR + R^T \dot{R} = 0 \\
\dot{R}^TR = -R^T \dot{R} 
\end{aligned}&lt;/script&gt;

&lt;p&gt;we define &lt;script type=&quot;math/tex&quot;&gt;\Omega = R^T \dot{R}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^T = (R^T \dot{R} )^T = \dot{R}^TR  = -R^T \dot{R}  = -\Omega&lt;/script&gt;

&lt;p&gt;Skew-symmetric! $\Omega^T = -\Omega$&lt;/p&gt;

&lt;p&gt;In fact,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Omega = \begin{bmatrix}
0 &amp; -\omega_z &amp; \omega_y \\
\omega_z &amp; 0 &amp; -\omega_x \\
-\omega_y &amp; \omega_x &amp; 0
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega} = \begin{bmatrix} \omega_x \\ \omega_y \\ \omega_z \end{bmatrix}&lt;/script&gt; is the angular velocity&lt;/p&gt;

&lt;p&gt;Notation! the “hat” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\omega}^{\hat{}} =\Omega&lt;/script&gt;

&lt;p&gt;the “v” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^{v} = \bar{\omega}&lt;/script&gt;

&lt;p&gt;So we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\dot{R} = R \Omega \\
\Omega = \bar{\omega}^{\hat{}}
\end{aligned}&lt;/script&gt;

&lt;p&gt;\item In terms of frames, we have Poisson’s kinematic equation&lt;/p&gt;

&lt;p&gt;\item The intrinsic equation is (where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}_b&lt;/script&gt; in the body frame is $\Omega_b$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^w}\dot{R}_b =  {}^{^w}R_b \Omega_b&lt;/script&gt;

&lt;p&gt;The “strap-down” equation (extrinsic)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^b}\dot{R}_w =  - \Omega_b {}^{^b}R_w&lt;/script&gt;

&lt;p&gt;Take vector on aircraft, put it into the coordinate frame of the world&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{}_{^w}R_b = \begin{bmatrix} | &amp; | &amp; | \\ {}^{^w} \hat{x}_b &amp; {}^{^w}\hat{y}_b &amp; {}^{^w} \hat{z}_b \\ | &amp; | &amp; |  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega \vec{v}_b = \omega \times \vec{v}_b&lt;/script&gt;

&lt;p&gt;Can we use this for filtering? Can we use &lt;script type=&quot;math/tex&quot;&gt;\dot{R} = R \Omega&lt;/script&gt; directly in an EKF, UKF
We’d have to turn it into discrete-time 
Naively, you might do the first order Euler approximation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ R_{t + \delta_t} -R_t}{\delta t} \approx R_t \Omega_t&lt;/script&gt;

&lt;p&gt;This does not work!&lt;/p&gt;

&lt;p&gt;You won’t maintain orthogonality! The defining property of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; was orthogonality, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{t + \delta t} \not\in SO(3)&lt;/script&gt;

&lt;p&gt;and pretty soon&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
R_{t + \delta t}^T R_{t + \delta t} \neq I \\
\mbox{det }(R_{t+\delta t}) \neq +1
\end{aligned}&lt;/script&gt;

&lt;p&gt;The 3x3 matrix will not be recognizable of a rotation&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frank Dellaert. Lecture Presentations of MM 8803: Mobile Manipulation, taught at the Georgia Institute of Technology, Fall 2018.&lt;/p&gt;

&lt;p&gt;[2] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.&lt;/p&gt;

&lt;p&gt;[3] Steven M. LaValle. &lt;em&gt;Planning Algorithms&lt;/em&gt;. Cambridge University Press, 2006, New York, NY, USA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rigid Body Kinematics and Filtering&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kinematics with Euler Angles&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{R} = f(R, \vec{w}), (\dot{\phi}, \dot{\theta}, \dot{\psi}) = f(\phi, \theta, \psi, \vec{\omega})
\end{equation}&lt;/p&gt;

&lt;p&gt;\item&lt;/p&gt;

&lt;p&gt;\begin{equation}
\vec{\omega} = \begin{bmatrix} w_x \ w_y \ w_z \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; -\mbox{sin} \theta \ 0 &amp;amp; \mbox{cos} \phi &amp;amp; \mbox{sin} \phi \mbox{cos} \phi \ 0 &amp;amp; -\mbox{sin} \phi &amp;amp; \mbox{cos} \phi \mbox{cos} \theta \end{bmatrix} \begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;\item We want to invert this matrix and solve for $\begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}$&lt;/p&gt;</content><author><name></name></author><summary type="html">SO(2), SO(3), SE(2), SE(3), Lie algebras</summary></entry><entry><title type="html">Stereo and Disparity</title><link href="http://johnwlambert.github.io/stereo/" rel="alternate" type="text/html" title="Stereo and Disparity" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/stereo-and-disparity</id><content type="html" xml:base="http://johnwlambert.github.io/stereo/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;Stereo Matching&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Disparity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;stereo-vision-and-stereo-matching&quot;&gt;Stereo Vision and Stereo Matching&lt;/h2&gt;

&lt;p&gt;“Stereo matching” is the task of estimating a 3D model of a scene from two or more images. The task requires finding matching pixels in the two images and converting the 2D positions of these matches into 3D depths [1]&lt;/p&gt;

&lt;p&gt;Humans have stereo vision with a baseline of 60 mm.&lt;/p&gt;

&lt;h2 id=&quot;disparity&quot;&gt;Disparity&lt;/h2&gt;

&lt;p&gt;Consider a simple model of stereo vision: we have two cameras whose optic axes are parallel. Each camera points down the &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;-axis. A figure is shown below:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD
  &lt;img src=&quot;/assets/stereo_vision_setup.jpg&quot; width=&quot;50%&quot; /&gt;
=======
  &lt;img src=&quot;/assets/stereo_vision_setup.png&quot; width=&quot;50%&quot; /&gt;
&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; bbabd62b79acfce4d33cc330fc01a5e0b1e16d7f
  &lt;div class=&quot;figcaption&quot;&gt;
    Two cameras L and R are separated by a baseline b. Here the Y-axis is perpendicular to the page. f is our (horizontal) focal length.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In this figure, the world point &lt;script type=&quot;math/tex&quot;&gt;P=(x,z)&lt;/script&gt; is projected into the left image as &lt;script type=&quot;math/tex&quot;&gt;p_l&lt;/script&gt; and into the right image as &lt;script type=&quot;math/tex&quot;&gt;p_r&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By defining right triangles we can find two similar triangles: with vertices at &lt;script type=&quot;math/tex&quot;&gt;(0,0)-(0,z)-(x,z)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(0,0)-(0,f)-(f,x_l)&lt;/script&gt;. Since they share the same angle &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\mbox{tan}(\theta)= \frac{\mbox{opposite}}{\mbox{adjacent}}&lt;/script&gt; for both, meaning:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{z}{f} = \frac{x}{x_l}&lt;/script&gt;

&lt;p&gt;We notice another pair of similar triangles
&lt;script type=&quot;math/tex&quot;&gt;(b,0)-(b,z)-(x,z)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(b,0)-(b,f)-(b+x_r,f)&lt;/script&gt;, which by the same logic gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{z}{f} = \frac{x-b}{x_r}&lt;/script&gt;

&lt;p&gt;We’ll derive a closed form expression for depth in terms of disparity. We already know that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{z}{f} = \frac{x}{x_l}&lt;/script&gt;

&lt;p&gt;Multiply both sides by &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, and we get an expression for our depth from the observer:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = f(\frac{x}{x_l})&lt;/script&gt;

&lt;p&gt;We now want to find an expression for &lt;script type=&quot;math/tex&quot;&gt;\frac{x}{x_l}&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;x_l-x_r&lt;/script&gt;, the &lt;em&gt;disparity&lt;/em&gt; between the two images.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{x}{x_l} &amp;= \frac{x-b}{x_r} &amp; \text{By similar triangles} \\
x x_r &amp;= x_l (x-b) &amp; \text{Multiply diagonals of the fraction} \\
x x_r &amp;= x_lx - x_l b &amp; \text{Distribute terms} \\
x_r &amp;= \frac{x_lx}{x} - b(\frac{x_l}{x}) &amp; \text{Divide all terms by } x \\
x_r &amp;= x_l - b(\frac{x_l}{x}) &amp; \text{Simplify} \\
b\frac{x_l}{x} &amp;= x_l - x_r &amp; \text{Rearrange terms to opposite sides} \\
b (\frac{x_l}{x}) (\frac{x}{x_l}) &amp;= (x_l - x_r) (\frac{x}{x_l}) &amp; \text{Multiply both sides by fraction inverse} \\
b &amp;= (x_l - x_r) (\frac{x}{x_l}) &amp; \text{Simplify} \\
\frac{b}{x_l - x_r} &amp;= \frac{x}{x_l} &amp; \text{Divide both sides by } (x_l - x_r) \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now plug this back in&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = f(\frac{x}{x_l}) = f(\frac{b}{x_l - x_r})&lt;/script&gt;

&lt;p&gt;What is our takeaway? The amount of horizontal distance between the object in Image L and image R (&lt;em&gt;the disparity&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;) is inversely proportional to the distance &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; from the observer. This makes perfect sense. Far away objects (large distance from the observer) will move very little between the left and right image. Very closeby objects (small distance from the observer) will move quite a bit more. The focal length &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and the baseline &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; between the cameras are just constant scaling factors.&lt;/p&gt;

&lt;p&gt;We made two large assumptions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We know the focal length &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and the baseline &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. This requires prior knowledge or camera calibration.&lt;/li&gt;
  &lt;li&gt;We need to find point correspondences, e.g. find the corresponding &lt;script type=&quot;math/tex&quot;&gt;(x_r,y_r)&lt;/script&gt; for
each &lt;script type=&quot;math/tex&quot;&gt;(x_l,y_l)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-epipolar-line-plane-and-constraint&quot;&gt;The Epipolar Line, Plane, and Constraint&lt;/h2&gt;

&lt;p&gt;Unfortunately, just because we know&lt;/p&gt;

&lt;p&gt;how to compute for a given pixel in one image the range of possible locations the pixel might appear at in the other image, i.e., its epipolar lin&lt;/p&gt;

&lt;h2 id=&quot;sum-of-squared-differences&quot;&gt;Sum of Squared-Differences&lt;/h2&gt;

&lt;p&gt;The matching cost is the squared difference of intensity values at a given disparity.&lt;/p&gt;

&lt;h2 id=&quot;cost-volumes&quot;&gt;Cost Volumes&lt;/h2&gt;

&lt;p&gt;Appendix B.5 and a recent survey paper on MRF inference (Szeliski, Zabih, Scharstein et al. 2008)&lt;/p&gt;

&lt;h2 id=&quot;simple-example-on-kitti&quot;&gt;Simple Example on KITTI&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E = R \mbox{ } [t]_{\times}&lt;/script&gt;

&lt;h2 id=&quot;object-detection-in-stereo&quot;&gt;Object Detection in Stereo&lt;/h2&gt;

&lt;p&gt;Chen &lt;em&gt;et al.&lt;/em&gt;
3DOP (NIPS 2015) Urtasun [4]&lt;/p&gt;

&lt;h2 id=&quot;unsupervised-learning-of-depth&quot;&gt;Unsupervised Learning of Depth&lt;/h2&gt;

&lt;p&gt;From 2017 [5]&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;p_t&lt;/script&gt; denote homogeneous coordinates in the target view. &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; denotes the camera intrinsic matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_s \sim K \hat{T}_{t \rightarrow s} \hat{D}_t (p_t) K^{-1} p_t&lt;/script&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Richard Szeliski.&lt;/p&gt;

&lt;p&gt;[2] James Hays. &lt;a href=&quot;https://www.cc.gatech.edu/~hays/compvision/lectures/09.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Rajesh Rao. Lecture 16: Stereo and 3D Vision, University of Washington. &lt;a href=&quot;https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] X. Chen, K. Kundu, Y. Zhu, A. Berneshawi, H. Ma, S. Fidler, R. Urtasun. &lt;em&gt;3D Object Proposals for Accurate Object Class Detection.&lt;/em&gt;  Advances in Neural Information Processing Systems 28 (NIPS 2015). &lt;a href=&quot;https://papers.nips.cc/paper/5644-3d-object-proposals-for-accurate-object-class-detection&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[5]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html&quot;&gt;PDF&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">focal length, similar triangles, depth</summary></entry><entry><title type="html">Recall and Precision</title><link href="http://johnwlambert.github.io/recall-precision/" rel="alternate" type="text/html" title="Recall and Precision" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/precision-recall</id><content type="html" xml:base="http://johnwlambert.github.io/recall-precision/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#recall&quot;&gt;Recall&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#precision&quot;&gt;Precision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#map&quot;&gt;Mean Average Precision (mAP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-more-finely-grained-measures-of-accuracy&quot;&gt;The Need for More Finely-Grained Measures of Accuracy&lt;/h2&gt;

&lt;p&gt;We’ll suppose that we are performing binary classification: classifying objects into different classes.&lt;/p&gt;

&lt;p&gt;Mean Average Precision involves computing the area under a curve (an integral), and can actually be quite confusing.&lt;/p&gt;

&lt;h3 id=&quot;recall&quot;&gt;Recall&lt;/h3&gt;

&lt;p&gt;Recall measures how many objects you missed. You can have very high recall by classifying everythin as some class (at the expense of precision).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{Recall} = \frac{tp}{tp+fn}&lt;/script&gt;

&lt;h3 id=&quot;precision&quot;&gt;Precision&lt;/h3&gt;

&lt;p&gt;Precision measures your discriminative ability. If you claimed that all of the objects you saw were of a particular class, and you were usually wrong because they belonged to a different class, you would have low precision. Your judgments can’t be considered &lt;em&gt;precise&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{Precision} = \frac{tp}{tp+fp}&lt;/script&gt;

&lt;h2 id=&quot;mean-average-precision&quot;&gt;Mean Average Precision&lt;/h2&gt;

&lt;p&gt;When performing the task of object detection, we would like to be discriminative not just about two classes&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def voc_eval(detpath,
             annopath,
             imagesetfile,
             classname,
             cachedir,
             ovthresh=0.5,
             use_07_metric=False):
    &quot;&quot;&quot;rec, prec, ap = voc_eval(detpath,
                                annopath,
                                imagesetfile,
                                classname,
                                [ovthresh],
                                [use_07_metric])

    Top level function that does the PASCAL VOC evaluation.

    detpath: Path to detections
        detpath.format(classname) should produce the detection results file.
    annopath: Path to annotations
        annopath.format(imagename) should be the xml annotations file.
    imagesetfile: Text file containing the list of images, one image per line.
    classname: Category name (duh)
    cachedir: Directory for caching the annotations
    [ovthresh]: Overlap threshold (default = 0.5)
    [use_07_metric]: Whether to use VOC07's 11 point AP computation
        (default False)
    &quot;&quot;&quot;
    # assumes detections are in detpath.format(classname)
    # assumes annotations are in annopath.format(imagename)
    # assumes imagesetfile is a text file with each line an image name
    # cachedir caches the annotations in a pickle file

    # first load gt
    if not os.path.isdir(cachedir):
        os.mkdir(cachedir)
    cachefile = os.path.join(cachedir, 'annots.pkl')
    # read list of images
    with open(imagesetfile, 'r') as f:
        lines = f.readlines()
    imagenames = [x.strip() for x in lines]

    if not os.path.isfile(cachefile):
        # load annots
        recs = {}
        for i, imagename in enumerate(imagenames):
            recs[imagename] = parse_rec(annopath.format(imagename))
            if i % 100 == 0:
                print 'Reading annotation for {:d}/{:d}'.format(
                    i + 1, len(imagenames))
        # save
        print 'Saving cached annotations to {:s}'.format(cachefile)
        with open(cachefile, 'w') as f:
            cPickle.dump(recs, f)
    else:
        # load
        with open(cachefile, 'r') as f:
            recs = cPickle.load(f)

    # extract gt objects for this class
    class_recs = {}
    npos = 0
    for imagename in imagenames:
        R = [obj for obj in recs[imagename] if obj['name'] == classname]
        bbox = np.array([x['bbox'] for x in R])
        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)
        det = [False] * len(R)
        npos = npos + sum(~difficult)
        class_recs[imagename] = {'bbox': bbox,
                                 'difficult': difficult,
                                 'det': det}

    # read dets
    detfile = detpath.format(classname)
    with open(detfile, 'r') as f:
        lines = f.readlines()

    splitlines = [x.strip().split(' ') for x in lines]
    image_ids = [x[0] for x in splitlines]
    confidence = np.array([float(x[1]) for x in splitlines])
    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])

    # sort by confidence
    sorted_ind = np.argsort(-confidence)
    sorted_scores = np.sort(-confidence)
    BB = BB[sorted_ind, :]
    image_ids = [image_ids[x] for x in sorted_ind]

    # go down dets and mark TPs and FPs
    nd = len(image_ids)
    tp = np.zeros(nd)
    fp = np.zeros(nd)
    for d in range(nd):
        R = class_recs[image_ids[d]]
        bb = BB[d, :].astype(float)
        ovmax = -np.inf
        BBGT = R['bbox'].astype(float)

        if BBGT.size &amp;gt; 0:
            # compute overlaps
            # intersection
            ixmin = np.maximum(BBGT[:, 0], bb[0])
            iymin = np.maximum(BBGT[:, 1], bb[1])
            ixmax = np.minimum(BBGT[:, 2], bb[2])
            iymax = np.minimum(BBGT[:, 3], bb[3])
            iw = np.maximum(ixmax - ixmin + 1., 0.)
            ih = np.maximum(iymax - iymin + 1., 0.)
            inters = iw * ih

            # union
            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +
                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *
                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)

            overlaps = inters / uni
            ovmax = np.max(overlaps)
            jmax = np.argmax(overlaps)

        if ovmax &amp;gt; ovthresh:
            if not R['difficult'][jmax]:
                if not R['det'][jmax]:
                    tp[d] = 1.
                    R['det'][jmax] = 1
                else:
                    fp[d] = 1.
        else:
            fp[d] = 1.

    # compute precision recall
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, use_07_metric)

    return rec, prec, ap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Bharath Hariharan and Ross Girshick. Fast/er R-CNN. &lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py&quot;&gt;https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The Histogram Filter</title><link href="http://johnwlambert.github.io/histogram-filter/" rel="alternate" type="text/html" title="The Histogram Filter" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/histogram-filter</id><content type="html" xml:base="http://johnwlambert.github.io/histogram-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-histogram-filter&quot;&gt;The Histogram Filter&lt;/h2&gt;

&lt;p&gt;An alternative to continuous distributions are piecewise constant approximations, such as histograms. These are &lt;em&gt;nonparametric&lt;/em&gt; filters, since they do not utilize parameters like a mean and covariance &lt;script type=&quot;math/tex&quot;&gt;(\mu, \Sigma)&lt;/script&gt; to define a distribution. Thus, nonparametric filters do not rely on a &lt;em&gt;fixed functional form of the posterior&lt;/em&gt;, like the Gaussian does [1].&lt;/p&gt;

&lt;p&gt;The Histogram Filter is a type of nonparametric filters that discretizes the state space into a finite number of regions. The histogram assigns to each region a single cumulative probability.&lt;/p&gt;

&lt;h2 id=&quot;1-d-histogram-filter&quot;&gt;1-D Histogram Filter&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;for all \(k\) do:
    &lt;ul&gt;
      &lt;li&gt;\( \hat{p}&lt;em&gt;{k,t} = \sum\limits_i p(X_t = x_k \mid u_t, X&lt;/em&gt;{t-1} = x_i) p_{i,t-1} \)&lt;/li&gt;
      &lt;li&gt;\( p_{k,t} = \eta p(z_t \mid X_t = x_k) \hat{p}_{k,t}) \)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-d-histogram-filter-and-grid-localization&quot;&gt;2-D Histogram Filter and Grid Localization&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Sebastian Thrun, Wolfram Burgard, Dieter Fox. &lt;em&gt;Probabilistic Robotics&lt;/em&gt;. The MIT Press, Cambridge, MA, 2005.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Spectral Clustering</title><link href="http://johnwlambert.github.io/spectral-clustering/" rel="alternate" type="text/html" title="Spectral Clustering" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/spectral-clustering</id><content type="html" xml:base="http://johnwlambert.github.io/spectral-clustering/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;spectral-clustering&quot;&gt;Spectral Clustering&lt;/h2&gt;

&lt;p&gt;\subsection{Graph Partitioning}
\begin{itemize}
\item Graph-cut problem: partition graph such that (1) edges between groups have a very low weight, and (2) edges within a group have high weight
\item Could use \textbf{Ratio Cut} (by size of the component) or the \textbf{Normalized Cut} (or by volume of the component, aka the sum of degrees)
\item Penalize using very small components, or very large components
\item These are NP-hard combinatorial problems. But Spectral clustering offers a way to solve the relaxation version of these problems
\item NCut -&amp;gt; use random-walk laplacian
\item RatioCut -&amp;gt; use unnormalized laplacian
\item 2-partition: assign by vector values of Fiedler vectors $\in \mathbbm{R}$, which ones $\in \mathbbm{R}&lt;em&gt;+$, or in $\mathbbm{R}&lt;/em&gt;-$
\item Can apply k-means or standard clustering algorithm on the embedded points (transform graph clustering into a point clustering problem!). Take you into point cloud setting
\item K-means minimizes the distortion measure/energy
\begin{equation}
J = \sum\limits_j \sum\limits_k r_{ji}(y_j - \mu_i)^2 ??
\end{equation}
\item Centroid already minimizes the sum of squared distances
\item Can only reduce energy in every step (locally converge to minimum)
\item Can discover number of clusters that you need – look at gap between eigenvalues, where is there a large gap $|\lambda_k - \lambda_{k-1}|$. 
\item Eigenvalues drop fast, then stabilize
\item Spectral clustering can cluster spirals of points, where k-nearest neighbors in this space would epicly fail
\item Look at data at a very different way… Even though data comes from a Euclidean space, easier to understand in the spectral space
\item edge weight $\mbox{exp} (-diff(pixel_{i}, pixel_j)/t^2 )$
\item Get reasonable, but not perfect, results for 3d segmentation
\item Fast and efficient with decent results
\end{itemize}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. &lt;em&gt;Graph Laplacians, Laplacian Embeddings, and Spectral Clustering&lt;/em&gt;. Lectures of CS233: Geometric and Topological Data Analysis, taught at Stanford University in Spring 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Understanding Multivariate Gaussians and Covariance</title><link href="http://johnwlambert.github.io/learning-point-cloud-features/" rel="alternate" type="text/html" title="Understanding Multivariate Gaussians and Covariance" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/learning-point-cloud-features</id><content type="html" xml:base="http://johnwlambert.github.io/learning-point-cloud-features/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3d-semantic-segmentation-of-features&quot;&gt;3D Semantic Segmentation of Features&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import torch
import torch.nn as nn
import math
import torch.nn.functional as F

class PointNetBase(nn.Module):

	def __init__(self, opt):
		&quot;&quot;&quot;
		Multilayer perceptrons with shared weights are implemented as 
		convolutions. This is because we are mapping from K inputs to 64 
		outputs, so we can just consider each of the 64 K-dim filters as 
		describing the weight matrix for each point dimension (X,Y,Z,...) to
		each index of the 64 dimension embeddings
		&quot;&quot;&quot;
		# Call the super constructor
		super(PointNetBase, self).__init__()

		self.opt = opt
		# stddev=1e-3,
		# weight_decay=0.0,
		# activation_fn=tf.nn.relu,

		self.autoencoder_mlp = self.make_ae_layers([32,64,64,128,128]) 
		self.decoder_fc = self.make_fc_layers([256, 256, self.opt.n_pc_per_model * 3])

		# 4 part classes
		# dim is 128 + 3
		self.seg_branch = nn.Sequential(nn.Conv1d(128+3,4,1))

		self._initialize_weights()


	def make_ae_layers(self, cfg):
		&quot;&quot;&quot;
		[1,3] kernel, stride [1,1]
		then [1,1] kernel and stride[1,1] everywhere

		Point functions (MLP implemented as conv2d)
		&quot;&quot;&quot;
		layers = []
		in_channels = 3
		for v in cfg:
			conv1d = nn.Conv1d(in_channels, v, 1)
			layers += [conv1d, nn.BatchNorm1d(v), nn.ReLU()]
			in_channels = v
		return nn.Sequential(*layers)


	def make_fc_layers(self, cfg):
		&quot;&quot;&quot;
		MLP on global point cloud vector
		&quot;&quot;&quot;
		layers = []
		in_channels = 128
		for v_idx, v in enumerate(cfg):
			fc = nn.Linear(in_channels, v, 1)
			if v_idx == len(cfg) - 1:
				# on the final layer, no activation fn here
				layers += [fc]
			else:
				layers += [fc, nn.ReLU(True), nn.Dropout(p=self.opt.dropout_prob) ]
			in_channels = v
		return nn.Sequential(*layers)


	def _initialize_weights(self):
		for m in self.modules():
			if isinstance(m, nn.Conv2d):
				n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
				m.weight.data.normal_(0, math.sqrt(2. / n))
				if m.bias is not None:
					m.bias.data.zero_()
			elif isinstance(m, nn.BatchNorm2d):
				m.weight.data.fill_(1)
				m.bias.data.zero_()
			elif isinstance(m, nn.Linear):
				n = m.weight.size(1)
				m.weight.data.normal_(0, 0.01)
				m.bias.data.zero_()


	def forward(self, x):
		&quot;&quot;&quot;
		K = 3
		Take as input a B x K x N matrix of B batches of N points with K 
		dimensions

		Points come in as torch.Size([B=50, N=1024, K=3])
		We turn them into ([B=50 x K=3 x N=1024])
		&quot;&quot;&quot;
		cloned_input = x.clone()
		# Number of points put into the network
		x = torch.transpose(x, 1, 2)
		N = x.size(2)

		# Input is B x K x N
		# Run the transformed inputs through the autoencoder MLP
		x = self.autoencoder_mlp(x)
		# Output is B x 128 x N

		# Pool over the number of points. This results in the &quot;global feature&quot;
		# Output should be B x 128 x 1 --&amp;gt; B x 128 (after squeeze)
		global_feature = F.max_pool1d(x, N).squeeze(2)
		latent_codes = global_feature.clone()

		per_pt_logits = None
		if self.opt.use_parts:
			pdb.set_trace()
			# segmentation fc for scores over num_class
			per_pt_latent_codes = global_feature.clone()
			per_pt_latent_codes = per_pt_latent_codes.repeat(N)
			per_pt_input =  torch.cat( [per_pt_latent_codes, cloned_input] )
			per_pt_logits = self.seg_branch(per_pt_input)

		# output has size ([B=50, N=3072])
		x = self.decoder_fc(global_feature)
		return x, latent_codes, per_pt_logits
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3d-instance-segmentation-of-features&quot;&gt;3D Instance Segmentation of Features&lt;/h2&gt;

&lt;p&gt;The line between 3D object detection and 3D instance segmentation in point clouds is blurry because both representations are easily derived from the other.&lt;/p&gt;

&lt;p&gt;The Similarity Group Proposal Network (SGPN) [1] is the first network architecture designed to perform instance-level and semantic segmentation directly on point clouds.&lt;/p&gt;

&lt;p&gt;It is possible to re-formulate the instance segmentation problem as semantic segmentation of 3 “similarity” classes in the following way. The 3 classes for each pair of points &lt;script type=&quot;math/tex&quot;&gt;\{Pi, Pj\}&lt;/script&gt; are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_j&lt;/script&gt; belong to the same object instance&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_j&lt;/script&gt; share the same semantic class but do not belong to the same object instance&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_j&lt;/script&gt; do not share the same semantic class. Pairs of points should lie progressively further away from each other in feature space as their similarity class increases.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;single network to predict point grouping proposals
and a corresponding semantic class for each proposal, from
which we can directly extract instance segmentation results&lt;/p&gt;

&lt;p&gt;similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point&lt;/p&gt;

&lt;p&gt;a similarity matrix
yielding point-wise group proposals,&lt;/p&gt;

&lt;p&gt;uses PointNet/PointNet++ to extract a descriptive feature vector for each point in the point cloud&lt;/p&gt;

&lt;p&gt;This feature extraction network
produces a matrix F. SGPN then diverges into three
branches that each pass F through a single PointNet layer to
obtain sized Np × Nf feature matrices FSIM, FCF , FSEM,
which we respectively use to obtain a similarity matrix, a
confidence map and a semantic segmentation map. The ith
row in a Np×Nf feature matrix is a Nf -dimensional vector
that represents point Pi
in an embedded feature space&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = L_{sim} + L_{cf} + L_{sem}&lt;/script&gt;

&lt;p&gt;The matrix &lt;script type=&quot;math/tex&quot;&gt;S \in \mathbf{R}^{N_p \times N_p}&lt;/script&gt;, and element &lt;script type=&quot;math/tex&quot;&gt;S_{ij}&lt;/script&gt; classifies whether or
not points &lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_j&lt;/script&gt; belong to the same object instance.
Each row of &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; can be viewed as a proposed grouping of
points that form a candidate object instance.&lt;/p&gt;

&lt;p&gt;We obtain &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; by, for each pair of
points &lt;script type=&quot;math/tex&quot;&gt;\{Pi, Pj\}&lt;/script&gt;, simply subtracting their corresponding feature vectors &lt;script type=&quot;math/tex&quot;&gt;\{F_{sim_i}, F_{sim_j} \}&lt;/script&gt; and taking the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm such that &lt;script type=&quot;math/tex&quot;&gt;S_{ij} = \|F_{sim_i} − F_{sim_j} \|_2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metric Learning With 3 Classes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Operates on matrix &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; which has &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm entries&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{sim} = \sum\limits_{i}^{N_p} \sum\limits_{j}^{N_p}l(i, j)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
l(i, j) = \begin{cases}
\|F_{sim_i} − F_{sim_j} \|_2 &amp; C_{ij} = 1 \\
\alpha \max(0, K_1 − \|F_{sim_i} − F_{sim_j} \|_2) &amp; C_{ij} = 2 \\
\max(0, K_2 − \|F_{sim_i} − F_{sim_j} \|_2) &amp; C_{ij} = 3 \end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;such that &lt;script type=&quot;math/tex&quot;&gt;\alpha &gt; 1, K2 &gt; K1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Confidence Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;we expect the
ground-truth value in the confidence map CMi
to be the intersection
over union (IoU) between the set of points in the
predicted group Si and the ground truth group Gi
. O&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Merging Group Proposals&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The similarity matrix S produces Np group proposals,
many of which are noisy or represent the same object.&lt;/p&gt;

&lt;p&gt;discard proposals with predicted confidence less than
T hC or cardinality less than T hM2. We further prune
our proposals into clean, non-overlapping object instances
by applying Non-Maximum Suppression; groups with IoU
greater than T hM1 are merged together by selecting the
group with the maximum cardinality&lt;/p&gt;

&lt;p&gt;T hM1 is set to 0.6 and T hM2 is set to 200. T hC is
set to 0.1. Our network is implemented&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instance Segmentation Without the 3-part Multi-Task Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also compare instance segmentation performance
with the following method (which we call Seg-Cluster):
Perform semantic segmentation using our network and then
select all points as seeds. Starting from a seed point, BFS
is used to search neighboring points with the same label. If
a cluster with more than 200 points has been found, it is
viewed as a valid group. Our GroupMerging algorithm is
then used to merge these valid groups.&lt;/p&gt;

&lt;p&gt;naive method like Seg-Cluster tends to
properly separate regions far away for large objects like the
ceiling and floor. However for small object, Seg-Cluster
fails to segment instances with the same label if they are
close to each other&lt;/p&gt;

&lt;p&gt;The method of Armeni [2] was …&lt;/p&gt;

&lt;h2 id=&quot;chamfer-distance&quot;&gt;Chamfer Distance&lt;/h2&gt;

&lt;p&gt;The symmetric Chamfer distance is a pseudo-metric, not a metric. It is defined as&lt;/p&gt;

&lt;p&gt;The Chamfer distance is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{Chamfer}(\mathbf{A,B}) = \Big( \sum\limits_{x_B \in \mathbf{B}} \underset{x_A \in \mathbf{A}}{\mbox{min}} |X_B - X_A|^2 \Big) + \Big( \sum\limits_{x_A \in \mathbf{A}} \underset{x_B \in \mathbf{B}}{\mbox{min}} |X_B - X_A|^2 \Big)&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np 
import torch

# def tensorflow_chamfer_distance(pc1, pc2):
#     '''
#     Input:
#         pc1: float TF tensor in shape (B,N,C) the first point cloud
#         pc2: float TF tensor in shape (B,M,C) the second point cloud
#     Output:
#         dist1: float TF tensor in shape (B,N) distance from first to second
#         idx1: int32 TF tensor in shape (B,N) nearest neighbor from first to second
#         dist2: float TF tensor in shape (B,M) distance from second to first
#         idx2: int32 TF tensor in shape (B,M) nearest neighbor from second to first
#     '''
#     N = pc1.get_shape()[1].value
#     M = pc2.get_shape()[1].value
#     pc1_expand_tile = tf.tile(tf.expand_dims(pc1,2), [1,1,M,1])
#     pc2_expand_tile = tf.tile(tf.expand_dims(pc2,1), [1,N,1,1])
#     pc_diff = pc1_expand_tile - pc2_expand_tile # B,N,M,C
#     pc_dist = tf.reduce_sum(pc_diff ** 2, axis=-1) # B,N,M
#     dist1 = tf.reduce_min(pc_dist, axis=2) # B,N
#     idx1 = tf.argmin(pc_dist, axis=2) # B,N
#     dist2 = tf.reduce_min(pc_dist, axis=1) # B,M
#     idx2 = tf.argmin(pc_dist, axis=1) # B,M
#     return dist1, idx1, dist2, idx2


# def pytorch_chamfer_distance(pc1, pc2):
#     '''
#     Input:
#         pc1: float TF tensor in shape (B,N,C) the first point cloud
#         pc2: float TF tensor in shape (B,M,C) the second point cloud
#     Output:
#         dist1: float TF tensor in shape (B,N) distance from first to second
#         idx1: int32 TF tensor in shape (B,N) nearest neighbor from first to second
#         dist2: float TF tensor in shape (B,M) distance from second to first
#         idx2: int32 TF tensor in shape (B,M) nearest neighbor from second to first
#     '''
#     N = pc1.size(1)
#     M = pc2.size(1)

# 	pc1_expand_tile = pc1.view(-1, 1).repeat(1, 3).view(1,1,M,1)
# 	pc2_expand_tile = pc2.view(-1, 1).repeat(1, 3).view(1,N,1,1)



#      = tf.tile(tf.expand_dims(pc1,2), [1,1,M,1])
#     pc2_expand_tile = tf.tile(tf.expand_dims(pc2,1), [1,N,1,1])
#     pc_diff = pc1_expand_tile - pc2_expand_tile # B,N,M,C
#     pc_dist = tf.reduce_sum(pc_diff ** 2, axis=-1) # B,N,M
#     dist1 = tf.reduce_min(pc_dist, axis=2) # B,N
#     idx1 = tf.argmin(pc_dist, axis=2) # B,N
#     dist2 = tf.reduce_min(pc_dist, axis=1) # B,M
#     idx2 = tf.argmin(pc_dist, axis=1) # B,M
#     return dist1, idx1, dist2, idx2



def batch_pairwise_dist(x, y, cuda):
    # 32, 2500, 3
    bs, num_points, points_dim = x.size()
    xx = torch.bmm(x, x.transpose(2, 1))
    yy = torch.bmm(y, y.transpose(2, 1))
    zz = torch.bmm(x, y.transpose(2, 1))
    if cuda:
        diag_ind = torch.arange(0, num_points).type(torch.cuda.LongTensor)
    else:
        diag_ind = torch.arange(0, num_points).type(torch.LongTensor)
    rx = xx[:, diag_ind, diag_ind].unsqueeze(1).expand_as(xx)
    ry = yy[:, diag_ind, diag_ind].unsqueeze(1).expand_as(yy)
    P = (rx.transpose(2, 1) + ry - 2 * zz)
    return P


def batch_NN_loss(x, y, cuda):
    bs, num_points, points_dim = x.size()
    dist1 = batch_pairwise_dist(x, y, cuda)
    values1, indices1 = dist1.min(dim=2)

    dist2 = batch_pairwise_dist(y, x, cuda)
    values2, indices2 = dist2.min(dim=2)
    a = torch.div(torch.sum(values1,1), num_points)
    b = torch.div(torch.sum(values2,1), num_points)
    chamfer = torch.div(torch.sum(a), bs) + torch.div(torch.sum(b), bs)

    #print('batch size: %d, and chamfer dist = %f' % (bs, chamfer.data[0]) )

    return chamfer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;graph-neural-networks&quot;&gt;Graph Neural Networks&lt;/h2&gt;

&lt;p&gt;Dynamic Graph CNN (DGCNN) [3] …&lt;/p&gt;

&lt;h2 id=&quot;meanshift&quot;&gt;Meanshift&lt;/h2&gt;

&lt;p&gt;Comaniciu … PAMI 2002&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. In CVPR, 2018. &lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf&quot;&gt;http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3d semantic parsing of largescale indoor spaces. In CVPR, 2016.&lt;/p&gt;

&lt;p&gt;[3] Dynamic Graph CNN.&lt;/p&gt;

&lt;p&gt;https://www.mathworks.com/matlabcentral/fileexchange/6110-toolbox-fast-marching&lt;/p&gt;</content><author><name></name></author><summary type="html">PointNet, SPGN, SPLATNet, Dynamic Graph Learning</summary></entry><entry><title type="html">Dimensionality Reduction</title><link href="http://johnwlambert.github.io/dimensionality-reduction/" rel="alternate" type="text/html" title="Dimensionality Reduction" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/dimensionality-reduction</id><content type="html" xml:base="http://johnwlambert.github.io/dimensionality-reduction/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#need-for-dr&quot;&gt;Need for Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pca&quot;&gt;PCA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nonlinear-dr-methods&quot;&gt;Nonlinear Dimensionality Reduction Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#isomap&quot;&gt;ISOMAP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lle&quot;&gt;LLE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sne&quot;&gt;SNE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#t-sne&quot;&gt;t-SNE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;need-for-dr&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-dimensionality-reduction&quot;&gt;The Need for Dimensionality Reduction&lt;/h2&gt;

&lt;p&gt;When the dimensionality of data points is high, interpretation or visualization of the data can be quite challenging.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;pca&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;linear-methods&quot;&gt;Linear Methods&lt;/h3&gt;

&lt;h2 id=&quot;principal-components-analysis-pca&quot;&gt;Principal Components Analysis (PCA)&lt;/h2&gt;

&lt;p&gt;PCA is an algorithm for linear dimensionality reduction that can be surprisingly confusing. There are many different ways to explain the algorithm, and it is not new – Karl Pearson (University College, London) introduced the ideas in 1901 and Harold Hostelling (Columbia) developed the terminology and more of the math about principal components in 1933.&lt;/p&gt;

&lt;p&gt;To understand PCA, we must first consider data as points in a Euclidean space.  Suppose the lies in a high-dimensional subspace, but the real space of the data could be low-dimensional. We would like to identify the real, lower-dim subspace where lower-dim structure corresponds to linear subspaces in the high-dim space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PCA has one key hyperparameter: the dimensionality of the lower-dim space.&lt;/strong&gt; How many dimensions do we need to explain most of the variability in the data?&lt;/p&gt;

&lt;p&gt;The key idea is that we will project the points from high-D to low-D, and then when we project them back into high-D, the “reconstruction error” should be minimized.&lt;/p&gt;

&lt;p&gt;\item Pearson (1901) – find a single lower dimensional subspace that captures most of the variation in the data
\item What should be the dimensionality of the lower-dim space?&lt;/p&gt;

&lt;p&gt;Acts as a proxy for the real space&lt;/p&gt;

&lt;p&gt;Minimize errors introudced by projecting the data into this subspace&lt;/p&gt;

&lt;p&gt;Assumption: data has zero mean, ``centered data’’&lt;/p&gt;

&lt;p&gt;Move the origin to the middle of the data, the data will live in the hyperplane. Becomes subspace in new translated coordinate system&lt;/p&gt;

&lt;p&gt;As a consequence, variance becomes just the 2nd moment&lt;/p&gt;

&lt;p&gt;Data points in 2D -&amp;gt; project onto the best fit line (projection onto 1D), then assess how well the data looks now when you lay out the projected points onto the best fit line (this is the reconstruction)
Bring it back onto the best fit line (multiply by vector)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{||} = vv^Tx&lt;/script&gt;

&lt;p&gt;Minimize an error function: error equivalent to maximizing the variance of the projected data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E = \frac{1}{M} \sum\limits_I \sum\limits_M ... = &lt; \|x^{\prime \mu} - x_{||}^{\prime \mu}\|^2 &gt;_{\mu} %]]&gt;&lt;/script&gt;

&lt;p&gt;$I$ dimensions
WHY IS THIS?
Trying to find the directions of maximum variance
Knowing the covariance matrix tells us about the 2nd order moments, but not about the highest-order structure of the data (unless data is normal)&lt;/p&gt;

&lt;p&gt;If the covariance matrix is diagonal, then finding the direction of maximum variance is easy&lt;/p&gt;

&lt;p&gt;Rotate your coordinate system so that the covariance matrix becomes diagonal.&lt;/p&gt;

&lt;p&gt;This becomes an eigenvalue problem: the eigenvectors of the covariance matrix point in the directions of maximal variance?&lt;/p&gt;

&lt;p&gt;Solve all top-k possible subspaces&lt;/p&gt;

&lt;p&gt;Valid for all of them&lt;/p&gt;

&lt;p&gt;Check the error depending on many dimensions we used. A tradeoff
$\mathbf{V}^T \mathbf{V} = I$ because $\mathbf{V}$ is an orthonormal matrix
Projection $x_{||} = \mathbf{V}y = \mathbf{V}\mathbf{V}^Tx$&lt;/p&gt;

&lt;p&gt;Projection operator = $\mathbf{V}\mathbf{V}^T$&lt;/p&gt;

&lt;p&gt;But in general $P$ is of low rank and loses info&lt;/p&gt;

&lt;p&gt;Variance and reconstruction error&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
&lt;x^Tx&gt; - &lt;y_{||}^T y_{||}&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;Want to be able to maximize the variance in the projected subspace… (FIND OUT WHY THAT IS)&lt;/p&gt;

&lt;p&gt;Spectral analysis of the covariance matrix:
if covariance matrix is symmetric, it always has real eigenvalues and orthogonal eigenvectors&lt;/p&gt;

&lt;p&gt;The total variance fo the data is just the sum of the eigenvalues of the covariance matrix, all non-negative&lt;/p&gt;

&lt;p&gt;Have diagonalized the covariance matrix, aligned, which was what we set out to do&lt;/p&gt;

&lt;p&gt;This is an extremal trace problem?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_i \lambda_i \sum\limits_p (v_{ip}^{\prime})^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \mbox{tr }(V^{\prime T} \Lambda V^{\prime})&lt;/script&gt;

&lt;p&gt;To maximize the variance, need to put as mucch weight as possible on the large eigenvalues&lt;/p&gt;

&lt;p&gt;The reconstruction error is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=P+1}^I \lambda_i&lt;/script&gt;

&lt;p&gt;Aligns the axis with your data, so that for any $P$, can find $P$-dimensional subspace&lt;/p&gt;

&lt;h3 id=&quot;eigenvectors-solve-pca&quot;&gt;Eigenvectors Solve PCA&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Main Lesson from PCA&lt;/strong&gt;: For any P, the optimal $P$-dimensional subspace is the one spanned by the first $P$ eigenvectors of the covariance matrix&lt;/p&gt;

&lt;p&gt;The eigenvectors give us the frame that we need – align with it&lt;/p&gt;

&lt;h3 id=&quot;pca-examples&quot;&gt;PCA Examples&lt;/h3&gt;

&lt;p&gt;First principal component does not always have semantic meaning (combination of arts, recreation, transportation, health, and housing explain the ratings of places the most)&lt;/p&gt;

&lt;p&gt;This is simply what the analysis gives.&lt;/p&gt;

&lt;p&gt;The math is beautiful, but semantic meaning is not always clear. Sparse PCA tries to fix this (linear combinations of only a small number of variables)&lt;/p&gt;

&lt;p&gt;Geography of where an individual came from is reflected in their genetic material&lt;/p&gt;

&lt;p&gt;Machinery gives us a way to understand distortions of changes: a recipe encoded as a matrix&lt;/p&gt;

&lt;p&gt;PCA works very well here&lt;/p&gt;

&lt;p&gt;View matrices as points in high-dimensional space&lt;/p&gt;

&lt;p&gt;Recover grid structure for deformed sphere&lt;/p&gt;

&lt;p&gt;Get circular pattern of galloping horse from points&lt;/p&gt;

&lt;h3 id=&quot;svd-trick&quot;&gt;SVD Trick&lt;/h3&gt;

&lt;p&gt;What if we have fewer data points than dimensions &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
M&lt;I %]]&gt;&lt;/script&gt;? Think of images…&lt;/p&gt;

&lt;p&gt;By SVD, transpose your data&lt;/p&gt;

&lt;p&gt;Think of your data as rows, not columns&lt;/p&gt;

&lt;p&gt;Data becomes first pixel across 1000 images&lt;/p&gt;

&lt;p&gt;Covariance matrix &lt;script type=&quot;math/tex&quot;&gt;C_2&lt;/script&gt; of transposed problem&lt;/p&gt;

&lt;p&gt;Rows of transformed X (PCA’d X) are just eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;C_1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Corresponding eigenvalues are just those of &lt;script type=&quot;math/tex&quot;&gt;C_2&lt;/script&gt;, scaled by &lt;script type=&quot;math/tex&quot;&gt;I/M&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;ntroduced by Pearson (1901)
and Hotelling (1933) to
describe the variation in a set
of multivariate data in terms of
a set of uncorrelated variables.
PCA looks for a single lower
dimensional subspace that
captures most of the variation
in the data.
Specifically, we aim to
minimize the error introduced
by projecting the data into this
linear subspace.&lt;/p&gt;

&lt;p&gt;Use spectral analysis of the covariance matrix C of the
data
For any integer p, the error-minimizing p-dimensional
subspace is the one spanned by the first p eigenvectors
of the covariance matrix&lt;/p&gt;

&lt;p&gt;Eigenfaces (PCA on face images)&lt;/p&gt;

&lt;p&gt;M. Turk and A. Pentland, Face Recognition using Eigenfaces, CVPR 1991&lt;/p&gt;

&lt;p&gt;https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf&lt;/p&gt;

&lt;p&gt;http://theory.stanford.edu/~tim/s17/l/l8.pdf&lt;/p&gt;

&lt;h2 id=&quot;multi-dimensional-scaling-mds&quot;&gt;Multi-Dimensional Scaling (MDS)&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;nonlinear-dr-methods&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;non-linear-methods&quot;&gt;Non-Linear Methods&lt;/h2&gt;

&lt;p&gt;Many data sets contain essential nonlinear structures and unfortunately these structures are invisible to linear methods like PCA [1].&lt;/p&gt;

&lt;p&gt;For example, Euclidean distance between points cannot disentangle or capture the structure of manifolds like the “swiss roll.” Instead, we need geodesic distance: distance directly on the manifold.&lt;/p&gt;

&lt;p&gt;Furthermore, PCA cares about large distances. The goal is to maximize variance, and variance comes from things that are far apart. If you care about small distances, PCA is the wrong tool to use.&lt;/p&gt;

&lt;h2 id=&quot;creating-graphs-from-geometric-point-data&quot;&gt;Creating Graphs from Geometric Point Data&lt;/h2&gt;

&lt;p&gt;Rely upon neighborhood relations. A graph can be constructed via (1) k-nearest neighbors or (2) &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-balls.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;isomap&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;isometric-feature-mapping-isomap&quot;&gt;Isometric Feature Mapping (ISOMAP)&lt;/h3&gt;

&lt;p&gt;In the case of the swiss roll, Isometric Feature Mapping (ISOMAP) produces an unrolled, planar version of data that respects distances [5]. This is “isometric” unrolling.&lt;/p&gt;

&lt;p&gt;The ISOMAP algorithm involves three main steps, as outlined by Guibas [1]:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1.) Form a nearest-neighbor graph &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}&lt;/script&gt; on the original data points, weighing the edges by their original distances &lt;script type=&quot;math/tex&quot;&gt;d_x(i,j)&lt;/script&gt;. One can build the NN-graph either (A) with a fixed radius/threshold &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;, or by using a (B) fixed # of neighbors.&lt;/li&gt;
  &lt;li&gt;(2.) Estimate the geodesic distances &lt;script type=&quot;math/tex&quot;&gt;d_{\mathcal{G}}(i,j)&lt;/script&gt; between all pairs of points on the sampled manifold by computing their shortest path distances in the graph &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}&lt;/script&gt;. This can be done with classic graph algorithms for all-pairs shortest-path algorithm (APSP)s: Floyd/Warshall’s algorithm or Dijkstra’s algorithm. Initially, all pairs given distance &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;, except for neighbors, which are connected.&lt;/li&gt;
  &lt;li&gt;(3.) Construct an embedding of the data in &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional Euclidean space that best preserves the inter-point distances &lt;script type=&quot;math/tex&quot;&gt;d_{\mathcal{G}}(i,j)&lt;/script&gt;. This is performed via Multi-Dimensional Scaling (MDS).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ISOMAP actually comes with recovery guarantees that discovered structure equal to actual structure of the manifold, especially as the graph point density increases. MDS and ISOMAP both converge, but ISOMAP gets there more quickly.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;lle&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;locally-linear-embedding-lle&quot;&gt;Locally Linear Embedding (LLE)&lt;/h3&gt;

&lt;p&gt;LLE is a method that learns linear weights that locally reconstruct points in order to map points to a lower dimension [1,4]. The method requires solving two successive optimization problems and requires a connectivity graph. Almost all methods start with the nearest neihgbor graph, since it’s the only thing that we can trust.&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;first step of LLE&lt;/strong&gt;, we find weights that reconstruct each data point from its neighbors:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\underset{w}{\mbox{minimize }} &amp; \| x_i - \sum\limits_{j \in N(i)} w_{ij}x_j \|^2 \\
\mbox{subject to} &amp; \sum\limits_j w_{ij} = 1
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can use linear least squares with Lagrange multipliers to obtain optimal linear combinations.&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;second step of LLE&lt;/strong&gt;, We then &lt;strong&gt;fix these weights&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; while optimizing for &lt;script type=&quot;math/tex&quot;&gt;x_i^{\prime}&lt;/script&gt;. We try to find low-dimensional coordinates:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2
\end{array}&lt;/script&gt;

&lt;p&gt;This is a sparse eigenvalue problem that requires constraints in order to prevent degenerate solutions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1.) The coordinates &lt;script type=&quot;math/tex&quot;&gt;x_i^{\prime}&lt;/script&gt; can be translated by a constant
displacement without affecting the cost. We can remove this degree of freedom by requiring the coordinates to be centered on the origin.&lt;/li&gt;
  &lt;li&gt;(2.) We constrain the embedding vectors to have unit covariance.
The optimization problem becomes:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} &amp; \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2 \\
\mbox{subject to} &amp; \sum\limits_i x_i^{\prime} = 0 \\
&amp; \frac{1}{n} \sum\limits_i x_i^{\prime}x_i^{\prime T} = I
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;These weights &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; capture the local shape. As Roweis and Saul point out, “&lt;em&gt;LLE illustrates a general principle of manifold learning…that overlapping
local neighborhoods – collectively analyzed – can provide information about global
geometry&lt;/em&gt;”” [4].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;sne&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;stochastic-neighbor-embedding-sne&quot;&gt;Stochastic Neighbor Embedding (SNE)&lt;/h3&gt;

&lt;p&gt;The Stochastic Neighbor Embedding (SNE) converts high-dimensional points to low-dimensional points by preserving distances. The method takes a probabilistic point of view: high-dimensional Euclidean point distances are converted into conditional probabilities that represent similarities [2,3].&lt;/p&gt;

&lt;p&gt;Similarity of datapoints in &lt;strong&gt;high himension&lt;/strong&gt;: The conditional probability is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{j \mid i} = \frac{\mbox{exp }\big( - \frac{\|x_i - x_j\|^2}{2 \sigma_i^2}\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \frac{\|x_i - x_k\|^2}{2 \sigma_i^2} \big)}&lt;/script&gt;

&lt;p&gt;Similarity of datapoints in &lt;strong&gt;the low dimension&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{j \mid i} = \frac{\mbox{exp }\big( - \|y_i - y_j\|^2\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \|y_i - y_k\|^2 \big)}&lt;/script&gt;

&lt;p&gt;If similarities between &lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt; are correctly mapped to similarities between &lt;script type=&quot;math/tex&quot;&gt;y_i,y_j&lt;/script&gt; by SNE, then the conditional probabilities should be equal: &lt;script type=&quot;math/tex&quot;&gt;q_{j \mid i} = p_{j \mid i}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;SNE seeks minimize the following cost function using gradient descent, which measures the dissimilarity between the two distributions (Kullback-Leibler divergence):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \sum\limits_i KL(P_i || Q_i) = \sum\limits_i \sum\limits_j p_{j \mid i} \mbox{ log } \frac{p_{j \mid i}}{q_{j \mid i}}&lt;/script&gt;

&lt;p&gt;This is known as asymetric SNE. The gradient turns out to be analytically simple:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial C}{\partial y_i} = 2 \sum\limits_j (p_{j \mid i} - q_{j \mid i} - p_{i \mid j} - q_{i \mid j} )(y_i - y_j)&lt;/script&gt;

&lt;p&gt;However, the Kullback-Leibler divergence is not symmetric, so a formulation with a joint distribution can be made.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;t-sne&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;t-sne&quot;&gt;t-SNE&lt;/h3&gt;

&lt;p&gt;An improvement to SNE is t-Distributed Stochastic Neighbor Embedding (t-SNE). t-SNE employs a Gaussian in the high-dimension, but a t-Student distribution in low-dim. The t-Student distribution has longer tails than a Gaussian and is thus happier to have points far away than a Gaussian. The motivation for doing so is that in low-D, you have have less freedom than you would in the high-dimension to put many things closeby. This is because there is not much space around (crowded easily), so we penalize having points far away less.&lt;/p&gt;

&lt;p&gt;The joint distribution in the low-distribution is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{ij} = \frac{ (1+ \| y_i −y_j \|^2)^{−1} }{ \sum\limits_{k \neq l} (1+ \|y_k −y_l\|^2)^{−1} }&lt;/script&gt;

&lt;h3 id=&quot;mnist-examples&quot;&gt;MNIST examples&lt;/h3&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. &lt;em&gt;Multi-Dimensional Scaling, Non-Linear Dimensionality Reduction&lt;/em&gt;. Class lectures of CS233: Geometric and Topological Data Analysis, taught at Stanford University in 18 April 2018.&lt;/p&gt;

&lt;p&gt;[2] Geoffrey Hinton and Sam Roweis. &lt;em&gt;Stochastic Neighbor Embedding&lt;/em&gt;. Advances in Neural Information Processing Systems (NIPS) 2003, pages 857–864. &lt;a href=&quot;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&quot;&gt;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] L.J.P. van der Maaten and G.E. Hinton. &lt;em&gt;Visualizing High-Dimensional Data Using t-SNE&lt;/em&gt;. Journal of Machine Learning Research 9 (Nov):2579-2605, 2008.&lt;/p&gt;

&lt;p&gt;[4] Sam T. Roweis and Lawrence K. Saul. &lt;em&gt;Nonlinear Dimensionality Reduction by Locally Linear Embedding&lt;/em&gt;. Science Magazine, Vol. 290,  22 Dec. 2000.&lt;/p&gt;

&lt;p&gt;[5] J. B. Tenenbaum, V. de Silva and J. C. Langford. &lt;em&gt;A Global Geometric Framework for Nonlinear Dimensionality Reduction&lt;/em&gt;. Science 290 (5500): 2319-2323, 22 December 2000.&lt;/p&gt;

&lt;p&gt;[6] Laurenz Wiskott. &lt;em&gt;Principal Component Analysis&lt;/em&gt;. 11 March 2004. &lt;a href=&quot;https://pdfs.semanticscholar.org/d657/68e1dad46bbdb5cfb17eb19eb07cc0f5947c.pdf&quot;&gt;Online PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[7] Karl Pearson. &lt;em&gt;On Lines and Planes of Closest Fit to Systems of Points in Space&lt;/em&gt;. 1901. Philosophical Magazine. 2 (11): 559–572.&lt;/p&gt;

&lt;p&gt;[8] H Hotelling. &lt;em&gt;Analysis of a complex of statistical variables into principal components&lt;/em&gt;. 1933. Journal of Educational Psychology, 24, 417–441, and 498–520.
Hotelling, H (1936). “Relations between two sets of variates”. Biometrika. 28 (3/4): 321–377. doi:10.2307/2333955. JSTOR 2333955.&lt;/p&gt;</content><author><name></name></author><summary type="html">PCA, geodesic distances, ISOMAP, LLE, SNE, t-SNE</summary></entry><entry><title type="html">Understanding Multivariate Gaussians and Covariance</title><link href="http://johnwlambert.github.io/gauss-covariance/" rel="alternate" type="text/html" title="Understanding Multivariate Gaussians and Covariance" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://johnwlambert.github.io/gaussians-and-covariance</id><content type="html" xml:base="http://johnwlambert.github.io/gauss-covariance/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-multivariate-gaussian-random-variable&quot;&gt;What is a multivariate Gaussian random variable?&lt;/h2&gt;

&lt;p&gt;Gaussian R.V.s are parameterized by two quantities:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X \sim p(x)  = \mathcal{N}(\mu_x, \Sigma_x)&lt;/script&gt;

&lt;p&gt;Preceded by a term for normalization&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \Sigma_x) = 
\frac{1}{\sqrt{(2\pi)^n|\Sigma_x|}}\mbox{exp}
\Bigg\{ -\frac{1}{2} (x - \mu_x)^T \Sigma_x^{-1} (x - \mu_x) \Bigg\}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the dimension, i.e. &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbbm{R}^n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And of course in the scalar case, we see&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \sigma_x) =&lt;/script&gt;

&lt;p&gt;Level sets trace out ellipses that are centered at &lt;script type=&quot;math/tex&quot;&gt;\mu_x&lt;/script&gt;, have minor and major axes (in 2D), and ellipsoids in higher dimensions&lt;/p&gt;

&lt;p&gt;Mean:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[X] = \int_x x p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_x = \int_x x \mathcal{N}(\mu_x,\Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Covariance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(X- \mu_x)(X - \mu_x)^T] = \int_x (x-\mu_x)(x-\mu_x)^T p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_x = \int_x (x- \mu_x) (x-\mu_x)^T \mathcal{N}(\mu_x, \Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Gaussian distribution is a second-order distribution, which does not mean that the higher order moments are zero (not true in general)
Convert between a standard normal, and any multivariate Gaussian&lt;/p&gt;

&lt;h2 id=&quot;the-standard-normal-distribution&quot;&gt;The Standard Normal Distribution&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_s \sim \mathcal{N}(0, I)&lt;/script&gt;

&lt;p&gt;unit (identity) covariance, so each axis decouples, compute integral over each axis separately&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to generate any Gaussian R.V. from standard normal&lt;/strong&gt; $$X_s$?$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X = \Sigma_x^{1/2}X_s + \mu_x&lt;/script&gt;

&lt;p&gt;Can obtain by scaling by covariance matrix, and by translating by mean
Very good for simulating
With MATLAB, can generate scalar, unit variance, 0 mean Gaussian R.V. with \texttt{randn}
Call \texttt{randn} $n$ times to populate $X_s$, and them multiply, then add
And we can compute via Cholesky Decomposition (unique if positive definite)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma = \Sigma_x^{1/2}(\Sigma_x^{1/2})^T&lt;/script&gt;

&lt;h2 id=&quot;matrix-square-roots&quot;&gt;Matrix Square Roots&lt;/h2&gt;

&lt;p&gt;\item There are other possible matrix square roots
\item \textbf{How to transform any Gaussian R.V. to the standard normal} $X_s$?
\item Do so via rearrangement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
X_s = \Sigma_x^{-1/2}(X - \mu_x), &amp; X \sim \mathcal{N}(\mu_x, \Sigma_x)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Much easier to integrate over the form on the RHS, not LHS
Comes from method of derived distributions
Derived Distributions: Given &lt;script type=&quot;math/tex&quot;&gt;X \sim p(x)&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Y=f(X)&lt;/script&gt;, find &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt;
Here &lt;script type=&quot;math/tex&quot;&gt;X = X_s&lt;/script&gt;, and function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the linear distribution &lt;script type=&quot;math/tex&quot;&gt;AX + b&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;properties-of-multivariate-gaussians&quot;&gt;Properties of Multivariate Gaussians&lt;/h2&gt;

&lt;h2 id=&quot;how-can-we-understand-a-covariance-matrix&quot;&gt;How can we understand a covariance matrix?&lt;/h2&gt;

&lt;p&gt;Larger covariance means more uncertainty. Isocontours/error ellipses&lt;/p&gt;

&lt;p&gt;We set &lt;script type=&quot;math/tex&quot;&gt;P=0.95&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon = \frac{1-P}{2 \pi |\Sigma|^{1/2}} = \frac{1-0.95}{2 \pi |\Sigma|^{1/2}} = \frac{0.05}{2 \pi |\Sigma|^{1/2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}(\varepsilon) = \Big\{ x \mid p(x) \geq \varepsilon \Big\}&lt;/script&gt;

&lt;h2 id=&quot;covariance&quot;&gt;Covariance&lt;/h2&gt;

&lt;p&gt;Cavariance Matrix How can we determine the direction of maximal variance? The first
we can do is to determine the variances of the individual components. If the data points (or
vectors) are written as x = (x1, x2)T (T indicates transpose), then the variances of the first
and second component can be written as C11 := “x1x1# and C22 := “x2x2# (angle brackets
indicate averaging over all data points). If C11 is large compared to C22, then the direction of
maximal variance is close to (1, 0)T , while if C11 is small, the direction of maximal variance
is close to (0, 1)T . (Notice that variance doesn’t have a polarity, so that one could use the
inverse vector (−1, 0)T instead of (1, 0)T equally well for indicating the direction of maximal
variance.)
But what if C11 is of similar value as C22, like in the example of Figure 1? Then the
co-variance between the two components, C12 := “x1x2#, can give us additional information
(notice that C21 := “x2x1# is equal to C12). A large positive value of C12 indicates a strong
correlation between x1 and x2 and that the data cloud is extended along the (1, 1)T direction.
A negative value would indicate anti-correlation and an extension along the (−1, 1)T
direction. A small value of C12 would indicate no correlation and thus little structure of
the data, i.e. no prominent direction of maximal variance. The variances and covariances
are conveniently arranged in a matrix with components Cij , which is called covariance matrix
(assuming zero mean data). Figure 3 shows several data clouds and the corresponding
covariance matrices.&lt;/p&gt;

&lt;p&gt;0.2 0
0 1&lt;/p&gt;

&lt;p&gt;1 -0.5
-0.5 0.3&lt;/p&gt;

&lt;p&gt;1 0 
0 1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import numpy as np
import pdb
import matplotlib.pyplot as plt
import seaborn as sns


import scipy

sns.set_style({'font.family': 'Times New Roman'})

def plot_gauss_ellipse(mu, cov, color='g', rad=2, ):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Plots a 2D ellipse derived from a 2D Gaussian specified by mu &amp;amp; cov.
	
	USAGE:
		hs = plotGaussEllipses( mus, Cs, [rad] )
	
	Args:
	-	mus: Numpy array of shape (2,), representing mean
	-	Cs: Numpy array of shape (2,2), representing covariance matrix
	-	color: string representing Matplotlib color
	-	rad: [2] Number of std to create the ellipse to
	
	Returns:
	-	None
	
	color choices: ['b', 'g', 'r', 'c', 'm', 'y', 'k']
	&quot;&quot;&quot;
	cRow, ccol, ra, rb, phi = gauss2ellipse( mu, cov, rad)
	plotEllipse( cRow, ccol, ra, rb, phi, color)



def gauss2ellipse(mu, C, rad=2):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Creates an ellipse representing the 2D Gaussian distribution.
	
	Creates an ellipse representing the 2D Gaussian distribution with mean mu
	and covariance matrix C.  Returns 5 parameters that specify the ellipse.
	
	USAGE
	 [cRow, cCol, ra, rb, phi] = gauss2ellipse( mu, C, [rad] )
	
	Args:
	-	mu: 1x2 vector representing the center of the ellipse
	-	C: 2x2 cov matrix
	-	rad: [2] Number of std to create the ellipse to
	
	OUTPUTS
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis length (in pixels) of the ellipse
	-	rb: semi-minor axis length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	
	EXAMPLE
	#  [cRow, cCol, ra, rb, phi] = gauss2ellipse( [5 5], [1 0; .5 2] )
	#  plotEllipse( cRow, cCol, ra, rb, phi );
	&quot;&quot;&quot;
	# error check
	if mu.size != 2 or C.shape != (2,2):
		print('Works only for 2D Gaussians')
		quit()

	# decompose using SVD
	_,D,Rh = np.linalg.svd(C)
	R = Rh.T
	normstd = np.sqrt(D)

	# get angle of rotation (in row/column format)
	phi = np.arccos(R[0,0])

	if R[1,0] &amp;lt; 0:
		phi = 2*np.pi - phi
	phi = np.pi/2 - phi

	# get ellipse radii
	ra = rad * normstd[0]
	rb = rad * normstd[1]

	# center of ellipse
	cRow = mu[0]
	cCol = mu[1]

	return cRow, cCol, ra, rb, phi



def plotEllipse(cRow,cCol,ra,rb,phi,color='b',nPnts=100,lw=1,ls='-'):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Adds an ellipse to the current plot.
	
	USAGE:
	-	h,hc,hl = plotEllipse(cRow,cCol,ra,rb,phi,[color],[nPnts],[lw],[ls])
	
	Args:
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis radius length (in pixels) of the ellipse
	-	rb: semi-minor axis radius length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	-	color: ['b'] color for ellipse
	-	nPnts: [100] number of points used to draw each ellipse
	-	lw: [1] line width
	-	ls: ['-'] line style

	Returns:
	-	h : handle to ellipse
	-	hc: handle to ellipse center
	-	hl: handle to ellipse orient

	EXAMPLE:
		plotEllipse( 3, 2, 1, 5, pi/6, 'g');
	&quot;&quot;&quot;
	# plot ellipse (rotate a scaled circle):
	ts = np.linspace(-np.pi, np.pi, nPnts+1)
	cts = np.cos(ts)
	sts = np.sin(ts)

	x = ra * cts * np.cos(-phi) + rb * sts * np.sin(-phi) + cCol
	y = rb * sts * np.cos(-phi) - ra * cts * np.sin(-phi) + cRow
	h = plt.plot(x,y, color=color, linewidth=lw, linestyle=ls)

	# plot center point and line indicating orientation
	hc = plt.plot(cCol, cRow, 'k+', color=color, linewidth=lw, linestyle=ls)

	x = [cCol, cCol+np.cos(-phi)*ra]
	y = [cRow, cRow-np.sin(-phi)*ra]
	hl = plt.plot(x, y, color=color, linewidth=lw, linestyle=ls)

	return h,hc,hl


def gen_from_distribution():



	Sigma_sqrt = scipy.linalg.sqrtm(Sigma)
	Sigma_inv = np.linalg.inv(Sigma)
	tiled_mu = np.tile(mu,(1000,1)).T
	samples = np.matmul( Sigma_sqrt, np.random.randn(2,1000) ) + tiled_mu
	exterior_samples = np.zeros((1,2))
	interior_samples = np.zeros((1,2))

	for i in range(samples.shape[1]):
			X = samples[:,i]
			f_val = 0.5 * np.matmul( np.matmul( (X - mu).T, Sigma_inv), X - mu )
			if f_val &amp;lt; -np.log(0.05):
				interior_samples = np.vstack([interior_samples, np.reshape(X,(1,2)) ])
			else:
				exterior_samples = np.vstack([exterior_samples, np.reshape(X,(1,2)) ])

	plt.scatter(interior_samples[:,0], interior_samples[:,1], c= 'b')
	plt.scatter(exterior_samples[:,0], exterior_samples[:,1], c= 'b')

def plot_gauss_ellipse_v2():
	&quot;&quot;&quot;
	&quot;&quot;&quot;
	d = 2 # dimension of samples
	p = 0.95 # probability
	num_samples = 1000

	mu = np.array(0,0) # dimension (2,)

	# define covar matrices of dim (2,2)
	cov_mats[0] = np.array([[1,0],
							[0,1]])
	cov_mats[1] = np.array([[2,0],
							[0,2]])
	cov_mats[2] = np.array([[0.25, 0.3]
							[0.3, 1]])
	cov_mats[3] = np.array([[10., 5]
							[5., 5]])

	for covar_mat in cov_mats:


		# generate and plot 1000 samples
		generate_gaussian_samples()
		plt.plot()

		# plot the error ellipse
		r = np.sqrt(ellipse_const)
		n_pts = int((2*np.pi) / 0.01)+1
		theta = np.linspace(0,2*np.pi,n_pts)
		w1 = r * np.cos(theta)
		w2 = r * np.sin(theta)
		w = np.array([w1,w2]).reshape(2,1)

		# transferred back to x coordinates
		x = scipy.linalg.sqrtm(sigma).dot(w) + mu
		plt.plot()


def unit_test1():
	&quot;&quot;&quot;
	&quot;&quot;&quot;

	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2))
	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2)*2.)
	# plt.show()

	fig()

	plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.array([[5,0],[0,3]]) )
	plt.show()




if __name__ == '__main__':
	unit_test1()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">error ellipses, uncertainty</summary></entry></feed>