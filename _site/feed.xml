<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-27T22:43:55-08:00</updated><id>http://localhost:4000/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">Simultaneous Localization and Mapping (SLAM)</title><link href="http://localhost:4000/slam/" rel="alternate" type="text/html" title="Simultaneous Localization and Mapping (SLAM)" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/slam</id><content type="html" xml:base="http://localhost:4000/slam/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;localization&quot;&gt;Localization&lt;/h2&gt;

&lt;p&gt;SLAM methods [2, 10, 13, 14, 21, 30]&lt;/p&gt;

&lt;p&gt;M. Bosse, P. Newman, J. Leonard, M. Soika, W. Feiten, and S. Teller. Simultaneous localization and map building in large-scale cyclic envi- ronments using the atlas framework. IJRR, 23(12), 2004.&lt;/p&gt;

&lt;p&gt;T. Duckett, S. Marsland, and J. Shapiro. Learning globally consistent
maps by relaxation. ICRA 2000.&lt;/p&gt;

&lt;p&gt;J. Folkesson and H. I. Christensen. Robust SLAM. ISAV 2004.
[14] U. Frese, P. Larsson, and T. Duckett. A multigrid algorithm for
simultaneous localization and mapping. IEEE Transactions on Robotics,
2005.&lt;/p&gt;

&lt;p&gt;K. Konolige. Large-scale map-making. AAAI, 2004.&lt;/p&gt;

&lt;p&gt;S. Thrun and M. Montemerlo. The GraphSLAM algorithm with
applications to large-scale mapping of urban structures. IJRR, 25(5/6),
2005.&lt;/p&gt;</content><author><name></name></author><summary type="html">GraphSLAM, loop closures</summary></entry><entry><title type="html">Structure From Motion</title><link href="http://localhost:4000/sfm/" rel="alternate" type="text/html" title="Structure From Motion" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/sfm</id><content type="html" xml:base="http://localhost:4000/sfm/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-basic-structure-from-motion-sfm-pipeline&quot;&gt;A Basic Structure-from-Motion (SFM) Pipeline&lt;/h2&gt;

&lt;p&gt;As described in [1], there are generally four steps to the algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) Find interest points in each image&lt;/li&gt;
  &lt;li&gt;(2) Find candidate correspondences (match descriptors for each interest point)&lt;/li&gt;
  &lt;li&gt;(3) Perform geometric verification of correspondences (RANSAC + fundamental matrix)&lt;/li&gt;
  &lt;li&gt;(4) Solve for 3D points and camera that minimize reprojection error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;costfunctions&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cost-functionserror-modelling&quot;&gt;Cost Functions/Error Modelling&lt;/h2&gt;

&lt;p&gt;The choice of cost function quantifies the total prediction error of the model. It measures how well the model fits the observations and background knowledge.&lt;/p&gt;

&lt;h3 id=&quot;reprojection-error-for-a-single-keypoint-in-two-images&quot;&gt;Reprojection Error for a Single Keypoint in Two Images&lt;/h3&gt;

&lt;p&gt;Imagine we have matched two keypoints, &lt;script type=&quot;math/tex&quot;&gt;x_1,x_2&lt;/script&gt; in two different images &lt;script type=&quot;math/tex&quot;&gt;I_1,I_2&lt;/script&gt; via a SIFT-like feature matching pipeline. We are viewing the projection of the same 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; in both images. We now wish to identify the coordinates describing the location of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If image &lt;script type=&quot;math/tex&quot;&gt;I_1&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt;, and image &lt;script type=&quot;math/tex&quot;&gt;I_2&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt;, we can enforce that the 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is projected into the image into the right location in both images. This is called &lt;em&gt;triangulation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If the camera calibration determining &lt;script type=&quot;math/tex&quot;&gt;M_1,M_2&lt;/script&gt; are &lt;strong&gt;known&lt;/strong&gt;, then the optimization problem for a single matched keypoint in two images becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}} f(\mathbf{X}) = \| x_1 - Proj(\mathbf{X},M_1)\|^2 + \| x_2 - Proj(\mathbf{X},M_2)\|^2&lt;/script&gt;

&lt;h3 id=&quot;reprojection-error-for-many-keypoints-in-many-cameras&quot;&gt;Reprojection Error for Many Keypoints in Many Cameras&lt;/h3&gt;

&lt;p&gt;Imagine now that we have &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; different cameras, and each camera has a &lt;strong&gt;known&lt;/strong&gt; projection matrix &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt;. Suppose we match &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; different keypoints across the images, denoted &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{X}_j\}_{j=1}^n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The optimization problem becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;In this case &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is the observed location of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;‘th keypoint into the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th image, as discovered by a keypoint detector in the SIFT-like pipeline. We penalize solutions for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_1,\mathbf{X}_2,\dots&lt;/script&gt; in which &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is not very close to &lt;script type=&quot;math/tex&quot;&gt;Proj(\mathbf{X_j},M_i)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;bundleadjustment&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bundle-adjustment&quot;&gt;Bundle Adjustment&lt;/h2&gt;

&lt;p&gt;Imagine now that we are working with arbitrary images for which we have no calibration information. Thus, the projection matrices &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt; are &lt;strong&gt;unknown&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Bundle adjustment is the process of minimizing reprojection error over (1) multiple 3D points and (2) multiple cameras. Triggs &lt;em&gt;et al.&lt;/em&gt; define it as &lt;em&gt;“the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates”&lt;/em&gt; [2]. The optimization problem changes only by adding new, additional variables for which we solve.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots, M_1,M_2,\dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;According to [2], the name “Bundle Adjustment” refers to &lt;em&gt;bundles&lt;/em&gt; of light rays leaving each 3D point and converging on each camera center, &lt;em&gt;“which are ‘adjusted’ optimally with respect to both feature and camera positions”&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;gauss-newton&quot;&gt;Gauss-Newton&lt;/h2&gt;

&lt;p&gt;Gauss-Newton is generally preferred to Second Order methods like Newton’s Method for a simple reason: deriving and implementing calculations of the second derivatives of the projection model &lt;script type=&quot;math/tex&quot;&gt;Proj(X_j,M_i)&lt;/script&gt; is difficult and error-prone [2].&lt;/p&gt;

&lt;p&gt;For example, the seminal work in SfM, “Building Rome in a Day” [3] uses Trust-Region Gauss-Newton optimization (Levenberg-Marquardt) that chooses between a truncated and an exact step Levenberg-Marquardt algorithm.&lt;/p&gt;

&lt;!-- ## Network Graph  shows which features are seen in which images, --&gt;

&lt;h2 id=&quot;exploiting-sparsity&quot;&gt;Exploiting Sparsity&lt;/h2&gt;

&lt;p&gt;Those who use generic optimization routines to solve SfM problems will find the optimization slow. This would be unwise, however, since the problem sparsity can be exploited.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Ramanan, Deva. &lt;em&gt;Structure from Motion.&lt;/em&gt; &lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&quot;&gt;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Bill Triggs, Philip McLauchlan, Richard Hartley and Andrew Fitzgibbon. &lt;em&gt;Bundle Adjustment — A Modern Synthesis&lt;/em&gt;. &lt;a href=&quot;https://hal.inria.fr/inria-00548290/document&quot;&gt;https://hal.inria.fr/inria-00548290/document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, Richard Szeliski. &lt;em&gt;Building Rome in a Day&lt;/em&gt;. Communications of the ACM,Volume 54 Issue 10, October 2011. Pages 105-112. &lt;a href=&quot;https://grail.cs.washington.edu/rome/rome_paper.pdf&quot;&gt;https://grail.cs.washington.edu/rome/rome_paper.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Deriving bundle adjustment</summary></entry><entry><title type="html">Particle Filter</title><link href="http://localhost:4000/particle-filter/" rel="alternate" type="text/html" title="Particle Filter" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/particle-filter</id><content type="html" xml:base="http://localhost:4000/particle-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle Filter&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Particle Filter&lt;/strong&gt; is a filtering algorithm that, unlike the Kalman Filter or EKF, can represent multi-modal distributions. It was developed between 1995 and 1996 at Oxford. It is often called the &lt;strong&gt;“Unscented Kalman Filter” (UKF)&lt;/strong&gt; because its creator, Durant-White, thought “it didn’t stink” [ADD REFERENCE].&lt;/p&gt;

&lt;h2 id=&quot;whats-wrong-with-the-ekf&quot;&gt;What’s Wrong With the EKF?&lt;/h2&gt;

&lt;p&gt;The Particle Filter addresses a number of problems with the EKF:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 1&lt;/strong&gt; The initial conditions (I.C.s)!
&lt;em&gt;If your initial guess is wrong, the Kalman filter will tell you exactly the wrong thing. The linearization can be vastly different at different parts of the state space&lt;/em&gt;. For example, the EKF could diverge if the residual $| \mu_{0 \mid 0} - x |$ on the initial condition is large. In fact, if $ | \mu_{t \mid t} - x_t |$ large at any time, then the EKF could diverge. This has to do with severity of non-linearity. This is the most commonly found problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 2: Covariance&lt;/strong&gt;
&lt;em&gt;In the EKF, the “covariance matrix” does not literally represent the covariance.&lt;/em&gt; Unfortunately, the covariance matrix only literally captures the covariance in the Kalman Filter. In the EKF, it is just a matrix! We don’t know what it means! If we treat it as confidence, then it is reasonable enough. And commonly true, as long as $\mu$ is tracking $x$ pretty well. However, this estimate tends to be overconfident since we are not incorporating linearization errors! Instead, &lt;script type=&quot;math/tex&quot;&gt;\Sigma_{t \mid t}&lt;/script&gt; incorporates only the noise errors &lt;script type=&quot;math/tex&quot;&gt;Q_t, R_t&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;\Sigma_{t \mid t}&lt;/script&gt;  tends to be smaller than the true covariance matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 3: No Canary in the Goldmine&lt;/strong&gt;
An additional problem with the EKF is that we have no signal to know if we’re going awry.&lt;/p&gt;

&lt;h2 id=&quot;a-different-parameterization-particles&quot;&gt;A Different Parameterization: Particles&lt;/h2&gt;

&lt;p&gt;The UKF represents a different type of compromise than the EKF. In the Kalman Filter and its Extended variant, &lt;script type=&quot;math/tex&quot;&gt;\mu, \Sigma&lt;/script&gt; are the parameters that define the distribution &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu, \Sigma)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The UKF parameterizes the state distribution in a different way by using “Sigma points.” Consequently, the parameterization is called the (&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;-points) parameterization. Thus, we move from &lt;script type=&quot;math/tex&quot;&gt;(\mu, \Sigma)&lt;/script&gt; to a set of points with a weight associated with each, e.g. &lt;script type=&quot;math/tex&quot;&gt;\{ (x^0,w^0), \cdots, (x^{2n}, w^{2n}) \}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The “Unscented Transform” is a curve-fitting exercise that converts individual points to a mean and covariance, i.e. &lt;script type=&quot;math/tex&quot;&gt;UT(\mu, \Sigma) = \{ (x^i, w^i) \}_i&lt;/script&gt;. An advantage of the UKF is that it is very easy to propagate these individual points through nonlinearities like non-linear dynamics, whereas it is harder to push &lt;script type=&quot;math/tex&quot;&gt;\mu, \Sigma&lt;/script&gt; through the nonlinearities.&lt;/p&gt;

&lt;p&gt;\item Properties that we want the unscented transform to have 
\item UT($\cdot$):&lt;br /&gt;
$(\mu, \Sigma) = { (x^i, w^i) }&lt;em&gt;i$
\item $UT^{-1}(\cdot)$ &lt;br /&gt;
$ { (x^i, w^i) }_i = (\mu, \Sigma)$
\item We want the sample sigma points to share the same mean, covariance
\begin{equation}
\mu = \sum\limits&lt;/em&gt;{i=0}^{2n} w^ix^i
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma = \sum\limits_{i=0}^{2n} w^i (x^i - \mu)(x^i - \mu)^T
\end{equation}
They are redundant (an overparameterization of the Gaussian)
\item We have redundancy for a smoothing effect, since won’t be for a perfect Gaussian
\item Here is the transform $UT(\cdot)$:&lt;br /&gt;
$x^0 = \mu$ &lt;br /&gt;
$x^i = \mu + ( \sqrt{(n+\lambda) \Sigma } )&lt;em&gt;i, i = 1, \dots, n$&lt;br /&gt;
\item This is a matrix square root
\item The index $i$ is the $i$’th column in the matrix square root
\item We also have a mirror image set:&lt;br /&gt;
$x^i = \mu - ( \sqrt{(n+\lambda) \Sigma } )&lt;/em&gt;{i-n}, i =n+ 1, \dots, 2n$&lt;br /&gt;
\item Those were the points. The weights themselves:&lt;/p&gt;

&lt;p&gt;\begin{equation}
w^0 = \frac{\lambda}{n+\lambda}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
w^i = \frac{1}{2(n+\lambda)}, i \geq 1
\end{equation}
\item Each of these points plot points around the circle/ellipse
\item Break ellipse into major and minor axes. $x_1, x_2, x_3, x_4$ at the corners of the principal axes and the parameters.
\item  $n \times n$ matrix is $\sqrt{(n+\lambda) \Sigma}$
\end{itemize}
\subsection{Verify Inverse Unscented Transform}
\begin{itemize}
\item  We verify $UT^{-1}(\cdot)$ as claimed&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sum\limits_{i=0}^{2n} w^ix^i = \frac{\lambda}{n+\lambda}(\mu) + \sum\limits_{i=1}^n  \frac{1}{2(n+\lambda)} \Bigg(\mu + ( \sqrt{(n+\lambda) \Sigma } )&lt;em&gt;i \Bigg) + \sum\limits&lt;/em&gt;{i=n+1}^{2n} \frac{1}{2(n+\lambda)} \Bigg(\mu +  - ( \sqrt{(n+\lambda) \Sigma })_{i-n} \Bigg)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
= \frac{\lambda}{n+\lambda}(\mu) +  \frac{n}{2(n+\lambda)} \Bigg(\mu  \Bigg) +  \frac{n}{2(n+\lambda)} \Bigg(\mu \Bigg) - \mu
\end{equation}&lt;/p&gt;

&lt;p&gt;\item Everything also cancels in the $\Sigma$ calculation&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sum\limits_{i=1}^n \frac{ (\sqrt{n_\lambda})(\sqrt{n_\lambda})}{ (n+\lambda)} (\sqrt{\Sigma}_i) (\sqrt{\Sigma})_i^T = \Sigma
\end{equation}&lt;/p&gt;

&lt;p&gt;We notice that if $AA^T = B$, then we can decompose $A$ as&lt;/p&gt;

&lt;p&gt;\begin{equation}
A = \begin{bmatrix}
| &amp;amp; \cdots &amp;amp; | &lt;br /&gt;
a_1 \cdots a_n &lt;br /&gt;
| &amp;amp; \cdots &amp;amp; | 
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;then $AA^T$ is&lt;/p&gt;

&lt;p&gt;\begin{equation}
 = \begin{bmatrix}
| &amp;amp; \cdots &amp;amp; | &lt;br /&gt;
a_1 &amp;amp; \cdots  &amp;amp; a_n &lt;br /&gt;
| &amp;amp; \cdots &amp;amp; | 
\end{bmatrix}
\begin{bmatrix}
– &amp;amp; a_1^T &amp;amp; – &lt;br /&gt;
– &amp;amp; \cdots &amp;amp; – &lt;br /&gt;
– &amp;amp; a_n^T &amp;amp; – | 
\end{bmatrix}
\end{equation}
\item Inverting the transform is just as simple
\item We get $a_1 a_1^T + a_2 a_2^T + \cdots + a_n a_n^T$
\item&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sum\limits_{i=1}^n a_i a_i^T = AA^T = B
\end{equation}&lt;/p&gt;

&lt;p&gt;therefore&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sum\limits_{i=1}^n (\sqrt{\Sigma})_i (\sqrt{\Sigma})_i^T = (\sqrt{\Sigma}) (\sqrt{\Sigma})^T = \Sigma
\end{equation}&lt;/p&gt;

&lt;p&gt;\item You might prefer the Cholesky Factorization for numerical versions
\item Matrix Square Root! Can use SVD or Cholesky Factorization
\item There are SVD matrix square roots:
\item (i) $\sqrt{\Sigma} = U \Lambda^{1/2}$&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sqrt{\Sigma} (\sqrt{\Sigma})^T = U \Lambda^{1/2} (U \Lambda^{1/2})^T = \Sigma
\end{equation}&lt;/p&gt;

&lt;p&gt;\item Columns of $U$ matrix are principal directions of ellipse. SVD gives you this geometric intution of the points around the semi axes, etc.
\item 
(ii)
\begin{equation}
\sqrt{\Sigma} = U \Lambda^{1/2} U^T = \Sigma
\end{equation}
\item SVD computation takes $O(4n^3)$ time&lt;/p&gt;

&lt;p&gt;\item \textbf{Cholesky Decomposition}:
\item $M = LU$, where lower triangular times upper triangular
\item When $M=M^T$, $L = U^T$
\item Let $M=LL^T$
\item (iii) $L = \sqrt{\Sigma}$
\item People prefer cholesky in UKF because it has complexity $O( \frac{1}{6} n^3)$, just a constant savings.
\item We choose $\sqrt{\Sigma} \sqrt{\Sigma}^T = LL^T = \Sigma$
\item Zero out successive row elements of your vector, it is not along semi axes, but more along different axis-aligned directions as you zero  out rows&lt;/p&gt;

&lt;p&gt;\end{itemize}
\subsection{UKF (Sigma Point Filter)}
\begin{itemize}
\item 
\begin{equation}
(\mu, \Sigma) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow predict \rightarrow (\bar{x}^i, \bar{w}^i) \rightarrow UT^{-1}(\cdot) \rightarrow  (\bar{\mu}, \bar{\Sigma}) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow update \rightarrow 
\end{equation}
\item \textbf{Predict Step}\&lt;/p&gt;

&lt;p&gt;$UT(\cdot)$
\begin{equation}
\begin{aligned}
x_{t \mid t}^0 = \mu_{t \mid t} &lt;br /&gt;
x_{t \mid t}^i = \mu_{t \mid t} + (\sqrt{(n+\lambda) \Sigma_{t \mid t}})&lt;em&gt;i, i = 1, \dots, n &lt;br /&gt;
x&lt;/em&gt;{t \mid t}^i = \mu_{t \mid t} - (\cdots), i = n+1, \dots, 2n &lt;br /&gt;
\bar{x}&lt;em&gt;{t +1 \mid t}^i = f(x&lt;/em&gt;{t \mid t}^i, u_t)
\end{aligned}
\end{equation}
We predict through nonlinear dynamics&lt;/p&gt;

&lt;p&gt;\item Now we run $UT^{-1}$
\begin{equation}
\begin{aligned}
\mu_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i \bar{x}&lt;em&gt;{t+1 \mid t}^i &lt;br /&gt;
\Sigma&lt;/em&gt;{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i (\bar{x}&lt;em&gt;{t+1 \mid t}^i - \mu&lt;/em&gt;{t+1 \mid t})(\bar{x}&lt;em&gt;{t+1 \mid t}^i - \mu&lt;/em&gt;{t+1 \mid t})^T
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;\item We recall the Gaussian estimate!&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\mu_{t \mid t} = \mu + \Sigma_{XY} \Sigma_{YY}^{-1} (y - \hat{y}) &lt;br /&gt;
\Sigma_{t \mid t} = \Sigma - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;\item Now the UPDATE step:&lt;br /&gt;
We run $UT(\cdot)$&lt;br /&gt;
$x_{t +1 \mid t}^0$&lt;br /&gt;
$x_{t+1 \mid t}^{2n}$\&lt;/p&gt;

&lt;p&gt;Let’s build $\hat{y}&lt;em&gt;{t+1 \mid t}$ and $\Sigma&lt;/em&gt;{t+1 \mid t}^{XY}$, $\Sigma_{t+1 \mid t}^{YY}$
\item Now&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hat{y}&lt;em&gt;{t+1 \mid t} = \sum\limits&lt;/em&gt;{i=0}^{2n} w^i \hat{y}_{t+1 \mid t}^i
\end{equation}
which is the expected measurment
Now,&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\Sigma_{t+1 \mid t}^{YY} = \sum\limits_{i=0}^{2n} w^i (\hat{y}&lt;em&gt;{t+1 \mid t}^i - \hat{y}&lt;/em&gt;{t+1 \mid t}) (\hat{y}&lt;em&gt;{t+1 \mid t}^i - \hat{y}&lt;/em&gt;{t+1 \mid t})^T
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;Now&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\Sigma_{t+1 \mid t}^{XY} = \sum\limits_{i=0}^{2n} w^i (x_{t+1 \mid t}^i - \mu_{t+1 \mid t}) (\hat{y}&lt;em&gt;{t+1 \mid t}^i - \hat{y}&lt;/em&gt;{t+1 \mid t})^T
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;We are doing a fitting operation. That is why we have more sigma points than we need. Smooth out the anomalies due to any one point getting weird.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{t+1 \mid t+1} = \Sigma_{t+1 \mid t} - \Sigma_{t+1 \mid t}^{XY} \Sigma_{t+1 \mid t}^{YY}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mu_{t+1 \mid t+1 } = \mu_{t+1 \mid t } + \Sigma_{t+1 \mid t}^{XY} (\Sigma_{t+1 \mid t}^{YY})^{-1} (y_{t+1} - \hat{y}&lt;em&gt;{t+1 \mid t})
\end{equation}
where $y&lt;/em&gt;{t+1}$ is the actual measurement.&lt;/p&gt;

&lt;p&gt;\item What is $\lambda$? Problem specific.  Consider SVD square root, so that sigma points will be along principal axes.
\item Suppose we have $x^0$ at the center of the ellipse, and $x^1, \dots, x^4$ lie at each corner of the principal semi-axes
\item $\lambda$ is the ``confidence-value’’ of the error ellipse
\item If $n+\lambda = 1$, then $x^i = \mu \pm \sqrt{\Sigma}_i$ and each column represents one standard deviation
\item Standard deviation in each direction. Bigger $\lambda$ is, then the bigger is the ellipse (And vice versa: smaller $\lambda$ gives smaller ellipse)
\item The size of the ellipse matters because this is what we take as the region about which we create our linearization
\item UKF is a lineaerization, takes average slope over a neighborhood. But it is not from the Taylor Series Expansion
\item 
\item Why does size of ellipse matter? Blurring over a bigger neighborhood. (neighborhood about which we fit the Gaussian is determined by  $\lambda$
\item The smaller $\lambda$ is, the closer it will be to an EKF, which fits about a single-point (linearizing it there)
\item Interesting value of $\lambda$: $\lambda=2$. For a quadratic non-linearity, then the inverse unscented transform fits the mean and the covariance of the Gaussian, and also the Kurtosis (the 4th moment) of the Gaussian (but only for a quadratic nonlinearity)
\item Fitting the Kurtosis is good! We can do it beacause the extra degrees of freedom of the sigma points overparameterize
\end{itemize}
\subsection{PRO version of UKF}
\begin{itemize}
\item Other Form of UKF:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\lambda = \alpha^2 ( n + k) - n
\end{equation}
this gives us two parameters to tune
\begin{equation}
x^i = \mu \pm \alpha ( \sqrt{(n+k) \Sigma})_i
\end{equation}
where the two parameters are $\alpha,k$
\item We now have to redefine the weights to be&lt;/p&gt;

&lt;p&gt;\begin{equation}
w_c^0 = \frac{\lambda}{n+\lambda} + (1 - \alpha^2 + \beta)
\end{equation}
where $\beta$ is another parameter                     &lt;br /&gt;
\item Hugh Durrant White, the original paper has this original form&lt;/p&gt;

&lt;p&gt;\item How does it work?
\end{itemize}
\section{May 10: Particle Filter}
\begin{itemize}
\item Algorithm is simple, but expensive to compute (with for loop)
\item In UKF, samples deterministally extracted
\item In Particle filter, no assumption about form of distribution, but need many form of them, and probabilistically extracted
\item \textbf{Main Idea:} Represent a distribution $p(x)$ with a collection of samples from $p(x)$: $x^i \sim p(x), i=1,\dots, N$ i.i.d.
\item What does it mean to ``represent’’ a distribution $p(x)$ with a bunch of particles ${ x_1, \dots, x_N }$?
\item We know that empirical moments are related to true distribution by the law of large number
\item Recall&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{array}{lll}
\mu = \mathbbm{E}[X] = \int_x p(x) dx \approx \frac{1}{N} \sum\limits_{i=1}^N x_i = \bar{\mu}, &amp;amp; x_i \sim p(x), &amp;amp; i=1,\dots, N, i.i.d
\end{array}
\end{equation}
Then 
\begin{equation}
\begin{array}{ll}
\Sigma = \mathbbm{E}[(X-\mu)(X-\mu)^T] = \dots \approx \frac{1}{N} \sum\limits_{i=1}^N (x_i - \bar{\mu})(x_i - \bar{\mu})^T, &amp;amp; x_i \sim p(x)
\end{array}
\end{equation}
Also,
\begin{equation}
\mathbbm{E}[f(x)] \approx \frac{1}{N} \sum\limits_{i=1}^N f(x_i)
\end{equation}
The Law of Large numbers states that
\begin{equation}
\frac{1}{N} \sum\limits_{i=1}^N f(x_i) \rightarrow \mathbbm{E}[f(X)]
\end{equation}
as $N \rightarrow \infty$
\item 
Problem: Given a set of particles 
\begin{equation}
{ x_{t+1 \mid t}^1, \dots, x_{t+1  \mid t}^N } \sim p(x_{t+1} \mid y_{1:t} )
\end{equation}
drawn from the above distribution
\item 
We recall the update step
\begin{equation}
p(x_t \mid y_{1:t} ) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) dx_t }
\end{equation}
We have particles from the top right expression, $x_{t \mid t-1}^i \sim p( x_t \mid y_{1:t-1})$
\item We want to use Bayes Rule so that we can transform our particles so that they approximate the posterior (this will be one step of the Bayesian Filter)
\item We will use :&lt;/p&gt;

&lt;p&gt;\item $q(x)$ the proposal distribution, the particles are actually from here&lt;/p&gt;

&lt;p&gt;\item We want them to be from $p(x)$ the target distribution, we wish they were from here
\item We use Particle Weights&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbbm{E}_p [ f(X)] = \int_x f(x) p(x) dx = \int_x f(x) p(x) \frac{q(x)}{q(x)} dx 
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbbm{E}_p [ f(X)] = \int_x f(x) w(x) q(x) dx  = \mathbbm{E}_q [ f(X) w(x) ]
\end{equation}&lt;/p&gt;

&lt;p&gt;\item Given ${x^1, \dots, x^N }$
\item New set: ${ (x^1,w^1), \dots, (x^N,w^N) }$ where $w^i = w(x^i)$
\item Now the expectation is&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbbm{E}&lt;em&gt;p[f(X)] \approx \frac{1}{N} \sum\limits&lt;/em&gt;{i=1}^N f(x^i) w^i
\end{equation}
and $w(x) = \frac{p(x)}{q(x)}$
\item If we knew $p,q$, then we would know the weights
\item But in the filtering setup, we don’t know the posterior, the distribution we are trying to hit! But in fact, we don’t need the target distribution!
\item 
Proposal: What we have
\begin{equation}
q(x) = p(x_t \mid y_{1:t-1})
\end{equation}
\item 
Target: What we want
\begin{equation}
p(x) = p(x_t \mid y_{1:t})
\end{equation}
\item 
So we get weights:&lt;/p&gt;

&lt;p&gt;\begin{equation}
w^i = w(x^i) = \frac{ p(x^i) }{ q(x^i) } = \frac{p(x_t^i \mid y_{1:t})^i}{p(x_t^i \mid y_{1:t-1}) }
\end{equation}
\item Now, by Bayes Rule, we can say that the PRIOR cancels out, which was the only thing we didn’t know???&lt;/p&gt;

&lt;p&gt;\begin{equation}
 = \frac{     \frac{p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} \cdots dx_t }     }{ p(x_t \mid y_{1:t-1}) }
\end{equation}&lt;/p&gt;

&lt;p&gt;When the prior cancels, we get&lt;/p&gt;

&lt;p&gt;\begin{equation}
w(x_t^i) = \frac{p(y_t \mid x_t^i ) }{ \int_{x_t} \cdots dx_t } 
\end{equation}&lt;/p&gt;

&lt;p&gt;We just care about the relative weights, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w} (x_t^i) = p(y_t \mid x_t^i)&lt;/script&gt;

&lt;p&gt;(unnormalized)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(x_t^i) = \frac{ \bar{w}(x_t^i) }{  \sum\limits_{i=1}^N \bar{w}(x_t^i) }&lt;/script&gt;

&lt;p&gt;Example: suppose we have measurement noise $v_t \sim \mathcal{N}(0, R_t)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t = g(x_t) + v_t&lt;/script&gt;

&lt;p&gt;Given $x_{t \mid t-1}^i \sim p(x_t \mid y_{1:t-1})$, then find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_{t \mid t}^i, w_{t \mid t}^i \}_i&lt;/script&gt;

&lt;p&gt;to represent $p(x_t \mid y_{1:t})$. The weights are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_{t \mid t-1}^i) \sim \mathcal{N}( g(x_{t \mid t-1}^i, R_t)&lt;/script&gt;

&lt;p&gt;And now&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = \eta \mbox{exp } \{ -\frac{1}{2} (y_t - g(x_{t \mid t-1}^i)^T R_t^{-1} (y_t - g(x_{t \mid t-1}^i) ) \}&lt;/script&gt;

&lt;p&gt;Renormalize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(x_t^i) = \frac{ \bar{w}(x_t^i) }{ \sum\limits_{i=1}^N \bar{w}(x_t^i)  }&lt;/script&gt;

&lt;p&gt;Then multiply the weights, and renormalize (could have started with weighted particles instead of just particles)&lt;/p&gt;

&lt;p&gt;Only the weight changes in the UPDATE step (but we keep two different time indices for weights). Time indices are redundant for the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; particles, which are the input&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_t^i, w_{t \mid t-1}^i) \}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_t^i) w_{t \mid t-1}^i&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t \mid t}^i  = \frac{ \bar{w}_{t \mid t}^i }{ \sum\limits_{i=1}^N \bar{w}_{t \mid t}^i  }&lt;/script&gt;

&lt;h2 id=&quot;predict-step&quot;&gt;Predict Step&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1} \mid y_{1:t}) = \int_x p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t&lt;/script&gt;

&lt;p&gt;the left side is the transition distribution (which we have)&lt;/p&gt;

&lt;p&gt;the right side is ${ (x_t^i, w_{t \mid t}^i) }$&lt;/p&gt;

&lt;p&gt;Let’s just “simulate” &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+1} \mid x_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Draw &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_{t+1} = f(x_t, u_t) + w_t&lt;/script&gt;, the process noise &lt;script type=&quot;math/tex&quot;&gt;w_t \sim \mathcal{N}(0, Q_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We sample 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
w_t \sim \mathcal{N}(0, Q_t), &amp; \rightarrow x_{t+1}^i = f(x_{t}^i, u_t) + w_t 
\end{array} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;1000 particles per dimension &lt;script type=&quot;math/tex&quot;&gt;(1000^{D}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;=dimension&lt;/p&gt;

&lt;p&gt;Exponential explosion of particles required to keep the same resolution&lt;/p&gt;

&lt;p&gt;Full Filter: Given&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_{t }^i, w_{t \mid t}^i \}_i&lt;/script&gt;

&lt;p&gt;Predict:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )&lt;/script&gt;

&lt;p&gt;Update:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t+1 \mid t+1}^i = p(y_{t+1} \mid x_{t+1} = x_{t+1}^i) w_{t \mid t}^i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1 \mid t+1 }^i  = \frac{ \bar{w}_{t+1 \mid t+1}^i }{ \sum\limits_{i=1}^N \bar{w}_{t+1 \mid t+1}^i  }&lt;/script&gt;

&lt;h2 id=&quot;sample-degeneracy&quot;&gt;Sample Degeneracy&lt;/h2&gt;

&lt;p&gt;Unfortunately, it is easy to end up with degenerate samples in the UKF, meaning the samples won’t do anything for you because they are all spread apart. TODO: Only one takes the weight, why?&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;solution&lt;/strong&gt;: resample points in a very clever way, via importance (re-)sampling. In a &lt;em&gt;survival of the fittest&lt;/em&gt;, those samples with high weight end up lots of children, and those with low weight disappear. I&lt;/p&gt;

&lt;p&gt;\item We sample a new set of particles at time $t$, get new $x_t^i$ s.t.
\item This is sequential important re-sampling.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr( x_t^i = x_t^j) = w_t^j&lt;/script&gt;

&lt;p&gt;The noisy prediction will disperse them. The new weights are uniform, i.e. &lt;script type=&quot;math/tex&quot;&gt;w_t^i = \frac{1}{N}&lt;/script&gt;
\item Particle impoverishment; all particles clumped at one point because not enough noise to disperse them
\item If the resampling happens too frequently, in comparison with the dispersal via noise, then we get sample impoverishment!&lt;/p&gt;

&lt;p&gt;We will get that all $x_t^i \approx x_t^j$, and $w_t^i \approx \frac{1}{N}, \forall i$&lt;/p&gt;

&lt;p&gt;So no diversity in the distribution anymore&lt;/p&gt;

&lt;p&gt;To fix it, resample less often&lt;/p&gt;

&lt;p&gt;Or you could just throw some random particles into your bag&lt;/p&gt;

&lt;p&gt;Check how different weights are at every time. If too different, trigger a resample (variance without subtracting mean is &lt;script type=&quot;math/tex&quot;&gt;\sum\limits_{i=1}^N (w_t^i)^2 \geq \mbox{thresh}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If maximally different, then you would get &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, maximally different&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low Variance Resampling:&lt;/strong&gt; in previous formulation, you could have gotten all copies of the same particle
One number line is &lt;script type=&quot;math/tex&quot;&gt;r \sim v[0, \frac{1}{N}]&lt;/script&gt;
Then map each of these to the right interval above in the weighted bins. Determinsitic resampling
Make &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; copies of it in every uniform bin&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Swept under Rug&lt;/strong&gt;
All filters have tried to compute the Bayesian posterior (either exactly or approximately)
The particle filter does not do this (look at prediction step)
PF tracks approximates the posterior of the trajectory instead&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{0:t} \mid y_{1:t})&lt;/script&gt;

&lt;p&gt;Because of the prediction step, where we say draw sample&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t  = x_t^i )&lt;/script&gt;

&lt;p&gt;What we really wanted was&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid y_{1:t} ) = \int_{x_t} p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t&lt;/script&gt;

&lt;p&gt;\item Instead, the Particle Filter finds&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1}, x_t \mid y_{1:t}) = p(x_{t+1} \mid x_t = x_t^i) p(x_t = x_t^i \mid y_{1:t})&lt;/script&gt;

&lt;p&gt;Track distribution of the whole TRAJECTORY, given all of the measurements.
Important for SLAM, because in SLAM we often want to estimate the history of the trajectory of the robot
\end{itemize}&lt;/p&gt;</content><author><name></name></author><summary type="html">multi-modal distributions, sigma point transform, matrix square roots ...</summary></entry><entry><title type="html">Robot Localization</title><link href="http://localhost:4000/robot-localization/" rel="alternate" type="text/html" title="Robot Localization" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/localization</id><content type="html" xml:base="http://localhost:4000/robot-localization/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;localization&quot;&gt;Localization&lt;/h2&gt;

&lt;p&gt;Navigation in urban environments requires clever solutions because sensors like GPS can’t provide centimeter-level accuracy.  Precise localization requires fusing signals from sensors like GPS, IMU, wheel odometry, and LIDAR data in concert with pre-built maps [1].&lt;/p&gt;

&lt;p&gt;I’ll review some methods from 2007-2018.&lt;/p&gt;

&lt;h2 id=&quot;points-planes-poles-gaussian-bars-over-2d-grids-feature-representation-method&quot;&gt;Points Planes Poles Gaussian Bars over 2D Grids (Feature representation method)&lt;/h2&gt;

&lt;p&gt;particle filter method for correlating LIDAR measurements [1]&lt;/p&gt;

&lt;h2 id=&quot;2d-grids&quot;&gt;2D grids&lt;/h2&gt;

&lt;h2 id=&quot;using-deep-learning&quot;&gt;Using Deep Learning&lt;/h2&gt;

&lt;p&gt;[1] J. Levinson, M. Montemerlo, and S. Thrun. Map-based 929 precision vehicle localization in urban environments. In 930
Robotics: Science and Systems, volume 4, page 1. Citeseer,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;1, 2, 5 931&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[2] R. Kummerle, D. Hahnel, D. Dolgov, S. Thrun, and W. Bur- 925 gard. Autonomous driving in a multi-level parking structure. 926 In IEEE International Conference on Robotics and Automa- 927 tion (ICRA), pages 3395–3400, May 2009. 1&lt;/p&gt;

&lt;p&gt;[3] J. Levinson and S. Thrun. Robust vehicle localization in 932 urban environments using probabilistic maps. In IEEE In- 933 ternational Conference on Robotics and Automation (ICRA), 934 pages 4372–4378, May 2010. 1, 2, 5, 7&lt;/p&gt;

&lt;p&gt;[4] R. W. Wolcott and R. M. Eustice. Fast LIDAR localization using multiresolution gaussian mixture maps. In IEEE In- ternational Conference on Robotics and Automation (ICRA), pages 2814–2821, May 2015. 1, 2&lt;/p&gt;

&lt;p&gt;[5] R. W. Wolcott and R. M. Eustice. Robust LIDAR local- ization using multiresolution gaussian mixture maps for au- tonomous driving. The International Journal of Robotics Re- search, 36(3):292–319, 2017. 1, 2&lt;/p&gt;

&lt;p&gt;[6] H. Kim, B. Liu, C. Y. Goh, S. Lee, and H. Myung. Robust 920 vehicle localization using entropy-weighted particle filter- 921 based data fusion of vertical and road intensity information 922 for a large scale urban area. IEEE Robotics and Automation 923 Letters, 2(3):1518–1524, July 2017.&lt;/p&gt;

&lt;p&gt;[7] R. Dub, M. G. Gollub, H. Sommer, I. Gilitschenski, R. Sieg- wart, C. Cadena, and J. Nieto. Incremental-segment-based localization in 3-D point clouds. IEEE Robotics and Au- tomation Letters, 3(3):1832–1839, July 2018. 1&lt;/p&gt;

&lt;p&gt;[8] G. Wan, X. Yang, R. Cai, H. Li, Y. Zhou, H. Wang, and S. Song. Robust and precise vehicle localization based on multi-sensor fusion in diverse city scenes. In IEEE Inter- national Conference on Robotics and Automation (ICRA), pages 4670–4677, May 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html">ICP, grid histograms, ...</summary></entry><entry><title type="html">The Kalman Filter</title><link href="http://localhost:4000/kalman-filter/" rel="alternate" type="text/html" title="The Kalman Filter" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/kalman-filter</id><content type="html" xml:base="http://localhost:4000/kalman-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-kalman-filter&quot;&gt;What is the Kalman Filter?&lt;/h2&gt;

&lt;p&gt;The Kalman Filter is nothing more than the Bayes’ Filter for Gaussian distributed random variables. The Bayes’ Filter is described in a &lt;a href=&quot;/bayes-filter/&quot;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is a very surprising result that we can write out the integrals analytically for the Bayes’ Filter when working with a special family of distributions: Gaussian distributed random variables (r.v.’s).  As we recall from the Bayes’ Filter, we have three quantities that we’ll need to be able to evaluate:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) &lt;script type=&quot;math/tex&quot;&gt;\int_{x_{t-1}} p(x_t \mid x_{t-1}) p(x_{t-1} \mid y_{1:t-1}) dx_{t-1}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(2) &lt;script type=&quot;math/tex&quot;&gt;p(x_t \mid x_{t-1})  = f(x_t, x_{t-1} )&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(3) &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t) = g(y_t, x_t)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Expressions (2) and (3) must be finitely parameterizable. It will take a fair amount of work to derive the analytical formulas of the Bayes’ Filter for Gaussian r.v.’s (the Kalman Filter).  We’ll first review properties of multivariate Gaussians, then the Woodbury matrix inversion lemmas, intuition behind covariance matrices, and then derive the Kalman Filter.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-multivariate-gaussians&quot;&gt;Properties of Multivariate Gaussians&lt;/h2&gt;

&lt;h2 id=&quot;woodbury-matrix-inversion-lemmas&quot;&gt;Woodbury Matrix Inversion Lemmas&lt;/h2&gt;

&lt;h2 id=&quot;how-can-we-understand-a-covariance-matrix&quot;&gt;How can we understand a covariance matrix?&lt;/h2&gt;

&lt;p&gt;Larger covariance means more uncertainty. Isocontours/error ellipses&lt;/p&gt;

&lt;h2 id=&quot;kalman-filter-derivation&quot;&gt;Kalman Filter Derivation&lt;/h2&gt;

&lt;h2 id=&quot;gaussian-white-noise&quot;&gt;Gaussian White Noise&lt;/h2&gt;

&lt;p&gt;In practice, noise is usually not Gaussian and is not white (usually colored).&lt;/p&gt;

&lt;p&gt;\item Fact: For a Linear-Gaussian dynamical system (one who’s dynamics are expressed with Gaussian White Noise
\begin{equation}
\begin{array}{ll}
x_{t+1} = Ax_t + B u_t + w_t, &amp;amp; w_t \sim \mathcal{N}(0,Q) &lt;br /&gt;
\end{array}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{array}{ll}
y_t = Cx_t + Du_t + v_t, &amp;amp; v_t \sim \mathcal{N}(O, R)
\end{array}
\end{equation}
where $w_t, v_t$ are zero-mean white noise&lt;/p&gt;</content><author><name></name></author><summary type="html">Multivariate Gaussians, ...</summary></entry><entry><title type="html">Bayes Filter</title><link href="http://localhost:4000/bayes-filter/" rel="alternate" type="text/html" title="Bayes Filter" /><published>2018-12-27T03:00:00-08:00</published><updated>2018-12-27T03:00:00-08:00</updated><id>http://localhost:4000/bayes-filter</id><content type="html" xml:base="http://localhost:4000/bayes-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#state-estimation&quot;&gt;What is State Estimation?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dt-lds&quot;&gt;Discrete-time Linear Dynamical Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#probability-review&quot;&gt;Probability Review: The Chain Rule, Marginalization, &amp;amp; Bayes Rule&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-estimation&quot;&gt;Recursive Bayesian Estimation + Conditional Independence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pgm-bayes-filter&quot;&gt;Graphical Model: The Structure of Variables in the Bayes Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-filter-deriv&quot;&gt;Derivation of the Bayes Filter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;state-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-state-estimation&quot;&gt;What is State Estimation?&lt;/h2&gt;

&lt;p&gt;State estimation is the study of reproducing the state of a robot (e.g. its orientation, location) from noisy measurements. Unfortunately, we can’t obtain the state &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; directly. Instead, we can obtain get a measurement that is all tangled up with noise.&lt;/p&gt;

&lt;p&gt;A more formal definition: Given a history of measurements &lt;script type=&quot;math/tex&quot;&gt;y_1, \dots, y_t)&lt;/script&gt;, and system inputs &lt;script type=&quot;math/tex&quot;&gt;(u_1, \dots, u_t)&lt;/script&gt;, find an estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{x}_t&lt;/script&gt; of the state &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with small error &lt;script type=&quot;math/tex&quot;&gt;\|\hat{x}_t - x_t \|&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;What do we have to work with?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) We are given the measurements &lt;script type=&quot;math/tex&quot;&gt;y_t&lt;/script&gt;, whose distribution is modeled  as &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t, u_t)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;(2) We assume that we know the state transition distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+1} \mid x_t, u_t)&lt;/script&gt;, i.e. the robot dynamics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal of state estimation is to find the posterior distribution of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; given all of the prior measurements and inputs:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t}, u_{1:t})&lt;/script&gt;

&lt;h2 id=&quot;why-do-we-need-a-distribution-instead-of-a-single-state-estimate&quot;&gt;Why do we need a distribution instead of a single state estimate?&lt;/h2&gt;

&lt;p&gt;The goal of state estimation is not to obtain a single &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt;. Rather, we desire a distribution over &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;’s. You might ask, &lt;em&gt;why?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The mean of a distribution may not be representative of the distribution whatsoever. For example, consider a bimodal distribution with a mean around 0, but consisting of two camel humps. There is more information in the Bayesian estimate that we can use for control. In the case of a Kalman Filter, we will express the state distribution as a Gaussian, which is parameterized compactly by a mean and covariance.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;dt-lds&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;discrete-time-linear-dynamical-systems-dt-lds&quot;&gt;Discrete-time Linear Dynamical Systems (DT LDS)&lt;/h2&gt;

&lt;p&gt;Filtering and estimation is much more easily described in discrete time than in continuous time. We use Linear Dynamical Systems as a key tool in state estimation.&lt;/p&gt;

&lt;p&gt;Suppose we have a system with state &lt;script type=&quot;math/tex&quot;&gt;x \in R^n&lt;/script&gt;, which changes over timesteps &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; [1,2]. The matrix &lt;script type=&quot;math/tex&quot;&gt;A(t) \in R^{n \times n}&lt;/script&gt; is called the dynamics matrix. Suppose we provide an input &lt;script type=&quot;math/tex&quot;&gt;u \in R^m&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  &lt;script type=&quot;math/tex&quot;&gt;B(t)&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;R^{n \times m}&lt;/script&gt; input matrix. The vector &lt;script type=&quot;math/tex&quot;&gt;c(t) \in R^n&lt;/script&gt; is called the offset. We can express the system as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(t + 1) = A(t)x(t) + B(t)u(t) + c(t)&lt;/script&gt;

&lt;p&gt;We will use the shorthand notation for simplicity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t + 1} = A_t x_t + B_t u_t + c_t&lt;/script&gt;

&lt;p&gt;As Boyd and Vandenberghe point out [2], the LDS is a “special case of a Markov system where the next state is a linear function of the current state.” Suppose we also have an output of the system &lt;script type=&quot;math/tex&quot;&gt;y(t) \in R^{p}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;C(t) \in R^{p \times n}&lt;/script&gt; is the output or sensor matrix. This equation can be modeled as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t) = C(t)x(t) + D(t)u(t)&lt;/script&gt;

&lt;p&gt;or in shorthand, as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t = C_t x_t + D_t u_t&lt;/script&gt;

&lt;p&gt;where  &lt;script type=&quot;math/tex&quot;&gt;t \in Z = \{0, \pm 1, \pm 2, \dots \}&lt;/script&gt; and vector signals &lt;script type=&quot;math/tex&quot;&gt;x,u,y&lt;/script&gt; are sequences. It is not hard to see that the DT LDS is a first-order vector recursion [1].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;probability-review&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;probability-review-the-chain-rule-marginalization--bayes-rule&quot;&gt;Probability Review: The Chain Rule, Marginalization, &amp;amp; Bayes Rule&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Chain Rule&lt;/strong&gt; states that if &lt;script type=&quot;math/tex&quot;&gt;y_1,...y_t&lt;/script&gt; are events, and &lt;script type=&quot;math/tex&quot;&gt;p(y_i) &gt; 0&lt;/script&gt;, then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_1 \cap y_2 \cap \dots \cap y_t) = p(y_1) p(y_2 \mid y_1)···p(y_t \mid y_1 \cap \dots \cap y_{t-1})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Marginalization&lt;/strong&gt; is a method of variable elimination. If &lt;script type=&quot;math/tex&quot;&gt;X_1,X_2&lt;/script&gt; are independent:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_{x_2} p(x_1,x_2) dx_2 = \int\limits_{x_2} p(x_1)p(x_2)dx = p(x_1) \int\limits_{x_2} p(x_2)dx_2 = p(x_1)&lt;/script&gt;

&lt;p&gt;We recall &lt;strong&gt;Bayes’ Rule&lt;/strong&gt; states that if &lt;script type=&quot;math/tex&quot;&gt;x,y&lt;/script&gt; are events, and &lt;script type=&quot;math/tex&quot;&gt;p(x) &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(y) &gt; 0&lt;/script&gt;, then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y) = \frac{p(y \mid x)p(x)}{p(y)} = \frac{p(y,x)}{p(y)}&lt;/script&gt;

&lt;p&gt;In the domain of state estimation, we can assign the following meaning to these distributions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; is our prior.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y \mid x)&lt;/script&gt; is our measurement likelihood.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; is the normalization term.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y)&lt;/script&gt; is our posterior (what we can’t see, given what we can see).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One problem is that we may not know &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; directly. Fortunately, it turns out &lt;em&gt;we don’t need to&lt;/em&gt;.
By marginalization of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from the joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(x,y)&lt;/script&gt;, we can write the normalization constant &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; in the denominator differently:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y)  = \frac{p(y \mid x) p(x)}{\int\limits_x p(y\mid x)p(x) dx}&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;bayes-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;recursive-bayesian-estimation--conditional-independence&quot;&gt;Recursive Bayesian Estimation + Conditional Independence&lt;/h2&gt;

&lt;p&gt;In estimation, the state &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is static. In filtering, the state &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is dynamic. We will address the &lt;strong&gt;Bayesian estimation&lt;/strong&gt; case first, which can be modeled graphically as Naive Bayes, and &lt;strong&gt;later we’ll address the Bayesian filtering&lt;/strong&gt; case.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/naive_bayes.jpg&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    Suppose we have $$Y_1$$ (wet umbrellas), and $$Y_2$$ (wet-ground), which share common parent $$X$$ (it's raining). Get conditional independence

Suppose we have sequence of measurements $$(Y_1,Y_2,\dots, Y_t)$$, revealed
$$X$$ is a static state. Assume $$Y_1,\dots, Y_t$$ are conditionally independent given $$X$$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;We’ll now derive the recursion, which iterates upon the previous timesteps. The key insight is that &lt;strong&gt;we can factor Bayes Rule via conditional independence.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have observed a measurement &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt; timesteps. In the &lt;strong&gt;previous time step&lt;/strong&gt; we had posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t-1})  = \frac{p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx}&lt;/script&gt;

&lt;p&gt;We want to compute:
&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y_{1:t})  = \frac{p(y_{1:t} \mid x) p(x)}{\int\limits_x p(y_{1:t}\mid x)p(x) dx}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;By the Chain Rule:
&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t, x, y_{1:t-1}) }{\int\limits_x p(y_{1:t}, x) dx} =  \frac{ p(y_t \mid x, y_{1:t-1}) p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t}\mid x)p(x) dx}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We assume that conditioned upon &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, all &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; are conditionally independent. Thus, we can discard evidence: &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x, y_{1:t-1}) = p(y_t \mid x)&lt;/script&gt;. Simplifying, we see:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t \mid x) p(y_{1:t-1} \mid x) p(x)}{\int\limits_x  p(y_t \mid x) p(y_{1:t-1} \mid x) p(x) dx}&lt;/script&gt;

&lt;p&gt;Rewrite it in terms of the posterior from the previous time step. We see the previous time step’s posterior also included &lt;script type=&quot;math/tex&quot;&gt;p(y_{1:t-1} \mid x) p(x)&lt;/script&gt; (the unnormalized previous time step posterior)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t \mid x) \frac{p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx} }{ \frac{\int\limits_x p(y_t \mid x) p(y_{1:t-1} \mid x) p(x) dx}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx} }&lt;/script&gt;

&lt;p&gt;Since constant w.r.t integral, can combine integrals, and see previous posterior in the denominator. Get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t}) = \frac{p(y_t \mid x) p(x \mid y_{1:t-1})}{ \int\limits_x p(y_t \mid x) p(x \mid y_{1:t-1})dx} = f\Bigg(y_t, p(x \mid y_{1:t-1}) \Bigg)&lt;/script&gt;

&lt;p&gt;In the recursive Bayes Filter, the prior is just the posterior from the previous time step. Thus, we have the following loop: &lt;strong&gt;Measure-&amp;gt;Estimate-&amp;gt;Measure-&amp;gt;Estimate…&lt;/strong&gt;. We only got this from conditional independence of measurements, given the state. The graphical model is Naive Bayes.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;pgm-bayes-filter&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;graphical-model-the-structure-of-variables-in-the-bayes-filter&quot;&gt;Graphical Model: The Structure of Variables in the Bayes Filter&lt;/h2&gt;

&lt;p&gt;Now consider a dynamic state &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;, instead of a static &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. This system can be modeled as a Hidden Markov Model.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/bayesian_filter_hmm.jpg&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    As we move forward in time, the state evolves from $$X_0 \rightarrow X_1 \rightarrow \dots \rightarrow X_t$$
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The HMM structure gives us two gifts: (1) the Markov Property, and (2) Conditional Independence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Markov Property (Markovianity)&lt;/strong&gt;: discard information regarding the state&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid x_{1:t-1}, y_{1:t-1}) = p(x_t \mid x_{t-1})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Conditional Independence of Measurements&lt;/strong&gt;: discard information regarding the measurement&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t \mid x_{1:t}, y_{1:t-1}) = p(y_t \mid x_t)&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;bayes-filter-deriv&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;derivation-of-the-bayes-filter&quot;&gt;Derivation of the Bayes Filter&lt;/h2&gt;

&lt;p&gt;Suppose we start with our posterior from the previous timestep, which incorporates all measurements &lt;script type=&quot;math/tex&quot;&gt;y_1,\dots, y_{t-1}&lt;/script&gt;: Via marginalization, we can rewrite the expression as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_{t-1}} p(x_{t}, x_{t-1}  \mid y_{1:t-1})dx_{t-1}&lt;/script&gt;

&lt;p&gt;By the chain rule, we can factor:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_{t-1}} p(x_{t},   \mid x_{t-1}, y_{1:t-1})  p( x_{t-1}  \mid y_{1:t-1})   dx_{t-1}&lt;/script&gt;

&lt;p&gt;By Markovianity, we can simplify the expression above, giving us &lt;strong&gt;the Predict Step&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_t-1} p(x_{t} \mid x_{t-1}) p(x_{t-1} \mid y_{1:t-1})dx_{t-1}&lt;/script&gt;

&lt;p&gt;In what we call the update step, we simply express the posterior using Bayes’ Rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{p(y_{1:t} \mid x_{t}) p(x_{t})}{p(y_{1:t})} =  \frac{p(y_{1:t} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_{1:t} \mid x_{t}) p(x_{t}) dx_{t}}&lt;/script&gt;

&lt;p&gt;We can now factor the numerator with the chain rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t, y_{1:t-1}) p(y_{1:t-1} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_t \mid x_t, y_{1:t-1}) p(y_{1:t-1} \mid x_{t}) p(x_{t})dx_{t}}&lt;/script&gt;

&lt;p&gt;By the conditional independence of measurements &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, we know &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t, y_{1:t-1}) = p(y_t \mid x_t)&lt;/script&gt;, so we can simplify:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(y_{1:t-1} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_t \mid x_t) p(y_{1:t-1} \mid x_{t}) p(x_{t})dx_{t}}&lt;/script&gt;

&lt;p&gt;Interestingly enough, we can see above in the right hand side two terms from Bayes’ Rule. We’ll be able to collapse them into a single term. Using these two terms, the left side of Bayes’ Rule would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) = \frac{p(y_{1:t-1} \mid x_{t}) p(x_{t})}{ \int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t }&lt;/script&gt;

&lt;p&gt;Multiplying by the denominator, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) \int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t = p(y_{1:t-1} \mid x_{t}) p(x_{t})&lt;/script&gt;

&lt;p&gt;Since by marginalization &lt;script type=&quot;math/tex&quot;&gt;\int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t = p(y_{1:t-1})&lt;/script&gt;, we can simplify the line above to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) p(y_{1:t-1}) = p(y_{1:t-1} \mid x_{t}) p(x_{t})&lt;/script&gt;

&lt;p&gt;We now plug this substitution into the numerator and in the denominator of the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1})   }{\int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;p(y_{1:t-1})&lt;/script&gt; does not depend upon &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we can pull it out of the integral, and the term cancels in the top and bottom:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1})   }{ p(y_{1:t-1})\int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1})  dx_{t}} = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1})  dx_{t}}&lt;/script&gt;

&lt;p&gt;This is the closed form expression for the &lt;strong&gt;Update Step&lt;/strong&gt; of the Bayes’ Filter.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{p(y_{t} \mid x_{t})p(x_{t} \mid y_{1:t-1})}{\int\limits_{x_{t}} p(y_{t} \mid x_{t}) p(x_{t} \mid y_{1:t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;The algorithm consists of repeatedly applying two steps: (1) the &lt;strong&gt;predict step&lt;/strong&gt;, where we move forward the time step, and (2) the &lt;strong&gt;update step&lt;/strong&gt;, where we incorporate the measurement.  They appear in an arbitrary order and constitute a cycle, but you have to start somewhere. At the end of the update step, we have the best estimate.&lt;/p&gt;

&lt;p&gt;It turns out that we can write out these integrals analytically for a very special family of distributions: Gaussian distributed random variables. This will be the Kalman Filter, which is covered in the next blog post in the &lt;em&gt;State Estimation&lt;/em&gt; module.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stephen Boyd and Reza Mahalati. &lt;a href=&quot;http://ee263.stanford.edu/lectures/overview.pdf&quot;&gt;http://ee263.stanford.edu/lectures/overview.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Stephen Boyd and Lieven Vandenberghe. &lt;em&gt;Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares&lt;/em&gt;. Cambridge University Press, 2018. &lt;a href=&quot;https://web.stanford.edu/~boyd/vmls/vmls.pdf&quot;&gt;https://web.stanford.edu/~boyd/vmls/vmls.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Mac Schwager. Lecture Presentations of AA 273: State Estimation, taught at Stanford University in April-June 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html">linear dynamical systems, bayes rule, bayesian filtering, estimation ...</summary></entry><entry><title type="html">Iterative Closest Point</title><link href="http://localhost:4000/icp/" rel="alternate" type="text/html" title="Iterative Closest Point" /><published>2018-11-29T03:00:00-08:00</published><updated>2018-11-29T03:00:00-08:00</updated><id>http://localhost:4000/icp</id><content type="html" xml:base="http://localhost:4000/icp/">&lt;h2 id=&quot;the-3d-registration-problem&quot;&gt;The 3D Registration Problem&lt;/h2&gt;
&lt;p&gt;Given two shapes A and B which partially overlap&lt;/p&gt;

&lt;p&gt;Goal: using only rigid transforms, register B against A by minimizing a measure of distance between A and B&lt;/p&gt;

&lt;p&gt;Assume A and B are positioned close to each other&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min\limits_T \delta(A, T(B))&lt;/script&gt;

&lt;h2 id=&quot;degrees-of-freedom-transform-estimation&quot;&gt;Degrees of Freedom: Transform estimation&lt;/h2&gt;

&lt;p&gt;A rigid motion has 6 degrees of freedom (3 for translation and 3 for rotation). We typically estimate the motion using many more pairs of corresponding points, so the problem is overdetermined (which is good, given noise, outliers, etc – use least squares approaches).&lt;/p&gt;

&lt;h2 id=&quot;key-challenges-of-the-hard-optimization-problem&quot;&gt;Key Challenges of the Hard Optimization Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(1) We must estimate correspondences. This gives rise to combinatorial searches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(2) We must estimate the aligning transform. Transforms may be non-linear.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fortunately, the low-dimensionality of some transforms helps.&lt;/p&gt;

&lt;h2 id=&quot;optimal-transformation-for-point-clouds&quot;&gt;Optimal Transformation for Point Clouds&lt;/h2&gt;

&lt;p&gt;When given correspondences, the problem is formulated as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Given two sets points:&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\{a_i \}_{i=1}^n, \{ b_i \}_{i=1}^n&lt;/script&gt; &lt;em&gt;in&lt;/em&gt;   . &lt;em&gt;Find the rigid transform&lt;/em&gt; \(\mathbf{R}, t\) &lt;em&gt;that minimizes&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{\mathbf{R}, t}{\mbox{minimize }} \sum\limits_{i=1}^N \| \mathbf{R}x_i + t - y_i \|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n \|Ra_i − t − b_i \|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{a} = \frac{1}{|A|} \sum\limits_i a_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{b} = \frac{1}{|B|} \sum\limits_i b_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_i^{\prime} = a_i − \bar{a}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_i^{\prime} = b_i − \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_i = a_i^{\prime} + \bar{a},  b_i = b_i^{\prime} + \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum\limits_{i=1}^n \|R(a_i^{\prime} + \bar{a}) − t − (b_i^{\prime} + \bar{b})\|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum\limits_{i=1}^n \|Ra_i^{\prime} − b_i^{\prime} + (R\bar{a} − \bar{b} − t)\|_2^2&lt;/script&gt;

&lt;p&gt;Let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = R\bar{a} − \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n \|Ra_i − t − b_i \|_2^2 = \sum\limits_{i=1}^n \|Ra_i^{\prime} − b_i^{\prime}\|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;tr(RN)&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def find_rigid_alignment(A, B):
	&quot;&quot;&quot;
	2-D or 3-D registration with known correspondences.
	Registration occurs in the zero centered coordinate system, and then
	must be transported back.

		Args:
		-	A: Numpy array of shape (N,D) -- Reference Point Cloud (target)
		-	B: Numpy array of shape (N,D) -- Point Cloud to Align (source)

		Returns:
		-	R: optimal rotation
		-	t: optimal translation
	&quot;&quot;&quot;
	num_pts = A.shape[0]
	dim = A.shape[1]

	a_mean = np.mean(A, axis=0)
	b_mean = np.mean(B, axis=0)

	# Zero-center the point clouds
	A -= a_mean
	B -= b_mean

	N = np.zeros((dim, dim))
	for i in range(num_pts):
		N += A[i].reshape(dim,1).dot( B[i].reshape(1,dim) )
	N = A.T.dot(B)

	U, D, V_T = np.linalg.svd(N)
	S = np.eye(dim)
	det = np.linalg.det(U) * np.linalg.det(V_T.T)
	
	# Check for reflection case
	if not np.isclose(det,1.):
		S[dim-1,dim-1] = -1

	R = U.dot(S).dot(V_T)
	t = R.dot( b_mean.reshape(dim,1) ) - a_mean.reshape(dim,1)
	return R, -t.squeeze()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similar code named the &lt;em&gt;Umeyama Transform&lt;/em&gt; after[4] ships with &lt;a href=&quot;https://eigen.tuxfamily.org/dox/group__Geometry__Module.html#gab3f5a82a24490b936f8694cf8fef8e60&quot;&gt;Eigen&lt;/a&gt; as &lt;a href=&quot;https://eigen.tuxfamily.org/dox/Umeyama_8h_source.html&quot;&gt;Umeyama&lt;/a&gt;. The Open3D library utilizes the Umeyama method also (source code &lt;a href=&quot;https://github.com/IntelVCL/Open3D/blob/master/src/Core/Registration/TransformationEstimation.cpp#L47&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;local-methods&quot;&gt;Local Methods&lt;/h2&gt;

&lt;h3 id=&quot;iterated-closest-pair-icp-2&quot;&gt;Iterated Closest Pair (ICP) [2]&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Align the A points to their closest B neighbors, then repeat.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Converges, if starting positions are “close enough”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variants&quot;&gt;Variants&lt;/h2&gt;

&lt;h3 id=&quot;exhaustive-search&quot;&gt;Exhaustive Search&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Compare (ideally) all alignments&lt;/li&gt;
  &lt;li&gt;Sample the space of possible initial alignments.&lt;/li&gt;
  &lt;li&gt;Correspondence is determined by the alignment at which models are closest.&lt;/li&gt;
  &lt;li&gt;Provides optimal result&lt;/li&gt;
  &lt;li&gt;Can be unnecessarily slow&lt;/li&gt;
  &lt;li&gt;Does not generalize well to non-rigid deformations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. Alignments and Correspondences, Geometric Features. CS 233: Geometric and Topological Data Analysis, 9 May 2018.&lt;/p&gt;

&lt;p&gt;[2] Besl, McKay 1992.&lt;/p&gt;

&lt;p&gt;[3] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns”, PAMI 1991. &lt;a href=&quot;http://web.stanford.edu/class/cs273/refs/umeyama.pdf&quot;&gt;http://web.stanford.edu/class/cs273/refs/umeyama.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Jeff Phillips. Lecture 24: Iterative Closest Point and Earth Mover’s Distance. CPS296.2 Geometric Optimization. 10 April 2007. &lt;a href=&quot;https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture24.pdf&quot;&gt;https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture24.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Constrained Optimization, Lagrangians, Duality, and Interior Point Methods</summary></entry><entry><title type="html">Piazza Answers</title><link href="http://localhost:4000/2018/11/28/piazza-answers.html" rel="alternate" type="text/html" title="Piazza Answers" /><published>2018-11-28T00:00:00-08:00</published><updated>2018-11-28T00:00:00-08:00</updated><id>http://localhost:4000/2018/11/28/piazza-answers</id><content type="html" xml:base="http://localhost:4000/2018/11/28/piazza-answers.html">&lt;h2 id=&quot;visualizing-cnns-via-deconvolution&quot;&gt;Visualizing CNNs via deconvolution&lt;/h2&gt;

&lt;p&gt;It concerns slide 4 in this presentation.
http://places.csail.mit.edu/slide_iclr2015.pdf&lt;/p&gt;

&lt;p&gt;I’m having a hard time understanding the deconvolution part of the slide.&lt;/p&gt;

&lt;p&gt;Great question. You can see a presentation from Matt Zeiler on the method here: https://www.youtube.com/watch?v=ghEmQSxT6tw. He summarizes his method from about minutes 8:40-20:00 in the presentation.&lt;/p&gt;

&lt;p&gt;Suppose there is some layer you want to visualize. Zeiler feeds in 50,000 ImageNet images through a learned convnet, and gets the activation at that layer for all of the images. Then they feed in the highest activations into their deconvolutional network.&lt;/p&gt;

&lt;p&gt;Their inverse network needs to make max pooling and convolution reversible. So they use unpooling and deconvolution to go backwards. This is how they can visualize individual layers.&lt;/p&gt;

&lt;h2 id=&quot;backprop-per-layer-equations&quot;&gt;Backprop per-layer equations&lt;/h2&gt;

&lt;p&gt;I would expect the quiz to include content from the lecture slides and what was discussed in lecture. Professor Hays didn’t go into detail how to perform backprop through every single layer, so I wouldn’t expect a detailed derivation for each layer. You can find the slides on backprop here.&lt;/p&gt;

&lt;p&gt;https://www.cc.gatech.edu/~hays/compvision/lectures/20.pdf&lt;/p&gt;

&lt;p&gt;If you’re interested in digging deeper into the equations, some basic intuition for derivatives can be found &lt;a href=&quot;http://cs231n.github.io/optimization-2/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture04.pdf&quot;&gt;here&lt;/a&gt;. The chain rules binds together the derivatives of each layer. For two simple examples, the (sub)gradient of a max of two arguments is 1 for the larger argument, 0 for the smaller argument. The derivative for the addition of two arguments is 1 with respect to each argument.&lt;/p&gt;

&lt;h2 id=&quot;fc-vs-conv2d-maxpool-linear&quot;&gt;FC vs. CONV2D MAXPOOL LINEAR&lt;/h2&gt;

&lt;p&gt;what are the reasons for not using fully connected layers and using conv2d+maxpool+linear layer combinations ? Is it only because the number in fully connected layers are very large for image processing to prohibits from learning the weights fast enough ? #vvm&lt;/p&gt;

&lt;p&gt;John Lambert
 John Lambert 18 hours ago We use convolutions and hierarchies of processing steps since we showed earlier in the course that this is the most effective way to work with image (gridded data).&lt;/p&gt;

&lt;p&gt;We don’t use fully-connected layers at every step because there would be way to many learnable parameters to learn without overfitting, and the memory size would be enormous. The fully-connected layers act as the classifier on top of the automatically-learned features.&lt;/p&gt;

&lt;p&gt;From CS 231N at Stanford:
http://cs231n.github.io/convolutional-networks/&lt;/p&gt;

&lt;p&gt;Regular Neural Nets. …Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.&lt;/p&gt;

&lt;p&gt;Regular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32&lt;em&gt;32&lt;/em&gt;3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200&lt;em&gt;200&lt;/em&gt;3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.&lt;/p&gt;

&lt;p&gt;3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depthhere refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension.&lt;/p&gt;

&lt;h2 id=&quot;why-does-batch-norm-help&quot;&gt;Why does Batch Norm help?&lt;/h2&gt;
&lt;p&gt;John: (adding this) I wouldn’t expect batch norm to help by 15% on the simple network. Maybe by about 4% on the SimpleNet. Are you confounding the influence of normalization, more layers, data augmentation, and dropout with the influence of batch norm?&lt;/p&gt;

&lt;p&gt;If you train your network to learn a mapping from X-&amp;gt;Y, and then the distribution of X changes, then you might need to retrain your network so that it can understand the changed distribution of X. (Suppose you go learned to classify black cats, and then suddenly you need to classify colored cats, example here).
https://www.youtube.com/watch?v=nUUqwaxLnWs&lt;/p&gt;

&lt;p&gt;Batch Norm speeds up learning. This is because it reduces the amount that the distribution of hidden values moves around. This is often called “reducing internal covariate shift”.  Since all the layers are linked, if the first layer changes, then every other layer was dependent on those values being similar to before (not suddenly huge or small).&lt;/p&gt;

&lt;p&gt;General ideas why Batch Norm helps:&lt;/p&gt;

&lt;p&gt;Improves gradient flow through the network (want variance=1 in your layers, avoid exponentially vanishing or exploding dynamics in both the forward and the backward pass)
Allows higher learning rates
Reduces the strong dependence on initialization
Acts as a form of regularization because it adds a bit of noise to training (uses statistics from random mini-batches to normalize) and it slightly reduces the need for dropout&lt;/p&gt;

&lt;p&gt;https://arxiv.org/pdf/1805.11604.pdf
Others say that BatchNorm makes the optimization landscape significantly smoother. They theorize that it reduces the Lipschitz constant of the loss function, meaning the loss changes at a smaller rate and the magnitudes of the gradients are smaller too.
https://en.wikipedia.org/wiki/Lipschitz_continuity&lt;/p&gt;

&lt;p&gt;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
In the AlexNet paper, the authors mentioned that local response normalization aids generalization, meaning that the network can accurately understand new examples.&lt;/p&gt;

&lt;h2 id=&quot;where-to-add-dropout&quot;&gt;Where to add Dropout?&lt;/h2&gt;
&lt;p&gt;Dropout was used in older convolutional network architectures like AlexNet and VGG. Dropout should go in between the fully connected layers (also known as 1x1 convolutions). Dropout doesn’t seem to help in the other convolutional layers.  It’s not exactly clear why.&lt;/p&gt;

&lt;p&gt;One hypothesis is that you only need to avoid overfitting in the layers with huge amounts of parameters (generally fully-connected layers), and convolutional layers usually have fewer parameters (just a few shared kernels) so there’s less need to avoid overfitting there. Of course, you could have the same number of parameters in both if you had very deep filter banks of kernels, but usually the max filter depth in VGG is only 512 and 384 in AlexNet.&lt;/p&gt;

&lt;p&gt;ResNet, a more modern convnet architecture, does not use dropout but rather uses BatchNorm.&lt;/p&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;p&gt;Could somebody post answers for Lecture-4 slides 12,13 and Lecture-5 slides 6?&lt;/p&gt;

&lt;p&gt;Lecture 4 Slide 12:&lt;/p&gt;

&lt;p&gt;(2) this is forward difference derivative approximation.
https://en.wikipedia.org/wiki/Finite_difference#Forward,_backward,_and_central_differences&lt;/p&gt;

&lt;p&gt;Lecture 4 Slide 13&lt;/p&gt;

&lt;p&gt;a) y-derivatives of image with Sobel operator&lt;/p&gt;

&lt;p&gt;b) derivative of Gaussian computed with Sobel operator&lt;/p&gt;

&lt;p&gt;c) image shifted with translated identity filter&lt;/p&gt;

&lt;p&gt;For explanations of Lecture 5 Slide 6,FOURIER MAGNITUDE IMAGES
 believe the answers are the following:&lt;/p&gt;

&lt;p&gt;1 and 3 are distinctive because they are not natural images.&lt;/p&gt;

&lt;p&gt;1D – Gaussian stays as a circle in the Fourier space.&lt;/p&gt;

&lt;p&gt;3A – Sobel has two separated ellipses.&lt;/p&gt;

&lt;p&gt;2,4,5 are all similar because natural images have fairly similar Fourier magnitude images&lt;/p&gt;

&lt;p&gt;2B – flower image has an even distribution of frequencies, so we see an even circular distribution in all directions in Fourier space.&lt;/p&gt;

&lt;p&gt;4E – because we have only lines along the x-axis, so we see a line only on the y-axis in the Fourier amplitude image.&lt;/p&gt;

&lt;p&gt;5C – because strong x,y-axis-aligned lines in the natural image related to x,y-axis-aligned lines in the Fourier magnitude images
For Lecture 4 Slide 12, I have:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;0 -1  0&lt;/p&gt;

&lt;p&gt;-1  4 -1&lt;/p&gt;

&lt;p&gt;0 -1  0&lt;/p&gt;

&lt;p&gt;2.&lt;/p&gt;

&lt;p&gt;0 -1 1&lt;/p&gt;

&lt;p&gt;For Lecture 4 Slide 13, I have:&lt;/p&gt;

&lt;p&gt;a) G = D * B&lt;/p&gt;

&lt;p&gt;b) A = B * C&lt;/p&gt;

&lt;p&gt;c) F = D * E&lt;/p&gt;

&lt;p&gt;d) I = D * D&lt;/p&gt;

&lt;p&gt;For Lecture 5 Slide 6, I have:&lt;/p&gt;

&lt;p&gt;1 - D&lt;/p&gt;

&lt;p&gt;2 - B&lt;/p&gt;

&lt;p&gt;3 - A&lt;/p&gt;

&lt;p&gt;4 - E&lt;/p&gt;

&lt;p&gt;5 - C&lt;/p&gt;

&lt;p&gt;I’m not sure if I can explain why all of those are the way they are, but I hope this helps!&lt;/p&gt;

&lt;h2 id=&quot;factor-graphs&quot;&gt;Factor Graphs&lt;/h2&gt;

&lt;p&gt;I’m trying to understand the few slides on factor graph variable elimination, does anyone have an intuitive explanation or good resources to explain what’s going on?&lt;/p&gt;

&lt;p&gt;A factor graph is a probabilistic graphical model (in the same family with Markov Random Fields (MRFs) and Bayesian Networks). It is an undirected graph (meaning there are no parents or topological ordering).&lt;/p&gt;

&lt;p&gt;Bayesian Networks are directed graphs where edges in the graph are associated with conditional probability distributions (CPDs), assigning the probability of children in the graph taking on certain values based on the values of the parents.&lt;/p&gt;

&lt;p&gt;In undirected models like MRFs and Factor Graphs, instead of specifying CPDs, we specify (non-negative) potential functions (or factors) over sets of variables associated with cliques (complete subgraphs) C of the graph.  Like Conditional Prob. Distributions, a factor/potential can be represented as a table, but it is not normalized (does not sum to one).&lt;/p&gt;

&lt;p&gt;A factor graph is a bipartite undirected graph with variable nodes (circles) and factor nodes (squares). Edges are only between the variable nodes and the factor nodes.&lt;/p&gt;

&lt;p&gt;The variable nodes can take on certain values, and the likelihood of that event for a set of variables is expressed in the potential (factor node) attached to those variables.  Each factor node is associated with a single potential, whose scope is the set of variables that are neighbors in the factor graph.&lt;/p&gt;

&lt;p&gt;A small example might make this clearer. Suppose we have a group of four people: Alex, Bob, Catherine, David A=Alex’s hair color (red, green, blue)&lt;/p&gt;

&lt;p&gt;B=Bob’s hair color&lt;/p&gt;

&lt;p&gt;C=Catherine’s hair color&lt;/p&gt;

&lt;p&gt;D=David’s hair color&lt;/p&gt;

&lt;p&gt;Alex and Bob are friends, Bob and Catherine are friends, Catherine and David are friends, David and Alex are friends&lt;/p&gt;

&lt;p&gt;Friends never have the same hair color!&lt;/p&gt;

&lt;p&gt;It turns out that this distribution p cannot be represented (perfectly) by any Bayesian network. But it is succinctly represented by a Factor Graph or MRF.
https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmsma7unw07s/Screen_Shot_20181002_at_11.48.05_PM.png&lt;/p&gt;

&lt;p&gt;The Factor Graph distribution is same as the MRF – this is just a different graph data structure&lt;/p&gt;

&lt;p&gt;If you’re wondering about the variable elimination part, we choose subsets of variables connected by factors and start combining them by taking the product of their factors and marginalizing out variables.&lt;/p&gt;

&lt;p&gt;In the table Prof. Dellaert showed, we have variables as the columns and factors as the rows. He combines factors progressively to involve more and more variables.&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jmtgs7d71g2p/Screen_Shot_20181003_at_2.05.56_PM.png&lt;/p&gt;

&lt;p&gt;$$ Normalizing SIFT&lt;/p&gt;

&lt;p&gt;You’re welcome to experiment with the choice of norm.  However, normally when we say that we’ll normalize every feature vector on its own, we mean normalizing descriptor &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;D_{normalized} = \frac{D}{\|D\|_2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We do this because feature descriptors are points in high-dimensional space, so we want them all to have the same length. That way the distance between them is based only upon the different angles the vectors point in, rather than considering their different length.&lt;/p&gt;

&lt;h2 id=&quot;more-efficient-calculation-of-sift-descriptor&quot;&gt;More efficient calculation of sift descriptor&lt;/h2&gt;

&lt;p&gt;I’m finding it slightly hard to implement an efficient algorithm for calculating the sift descriptor. The way I’m doing it now is basically like this;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate all the gradients and their angle&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over all interest points&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over the rows of the 4x4 cell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loop over the columns of the 4x4 cell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each cell index, compute the histogram and save it in that cell&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m feeling like there might be a nice numpy-way to get around my two inner loops (the ones over the cell). Is this possible? Any ideas or tips?&lt;/p&gt;

&lt;p&gt;For reference: it can compute the descriptor for around 1000 interest points in one second, don’t know if that’s sufficiently fast?&lt;/p&gt;

&lt;p&gt;One good way to reduce your 3 for-loops into 2 for-loops would be to do the following:&lt;/p&gt;

&lt;p&gt;Instead of:&lt;/p&gt;

&lt;p&gt;for interest point&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for row

        for col
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You could do:&lt;/p&gt;

&lt;p&gt;for interest point&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   for 4x4 patch in 16x16 window
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Creating those patches can be done by reshaping and swapping the axes. For example, if you had an array x like&lt;/p&gt;

&lt;p&gt;x = np.reshape(np.array(range(16)),(4,4))
It would look like&lt;/p&gt;

&lt;p&gt;array([[ 0,  1,  2,  3],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [ 4,  5,  6,  7],

   [ 8,  9, 10, 11],

   [12, 13, 14, 15]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You could break it into 2 parts along dimension 0:&lt;/p&gt;

&lt;p&gt;x.reshape(2,-1)
You’d get&lt;/p&gt;

&lt;p&gt;array([[ 0,  1,  2,  3,  4,  5,  6,  7],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [ 8,  9, 10, 11, 12, 13, 14, 15]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you kept breaking the innermost array into 2 arrays you’d get&lt;/p&gt;

&lt;p&gt;x.reshape(2,2,-1)
array([[[ 0,  1,  2,  3],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    [ 4,  5,  6,  7]],



   [[ 8,  9, 10, 11],

    [12, 13, 14, 15]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then finally you would get&lt;/p&gt;

&lt;p&gt;x.reshape(2,2,2,2)
array([[[[ 0,  1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     [ 2,  3]],

    [[ 4,  5],

     [ 6,  7]]],



   [[[ 8,  9],

     [10, 11]],

    [[12, 13],

     [14, 15]]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point you have 2 cubes that are 2x2x2. There are 3 ways you could look at this cube: there are 2 planes along the x-direction, or 2 planes along the y-direction, or 2 planes along the z-direction. If you swap the direction from which you look at the cube (swapaxes), you could now have&lt;/p&gt;

&lt;p&gt;array([[[[ 0,  1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     [ 4,  5]],

    [[ 2,  3],

     [ 6,  7]]],



   [[[ 8,  9],

     [12, 13]],

    [[10, 11],

     [14, 15]]]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which you’ll notice is effectively all of the original 2x2 patches with stride 2 from the original 4x4 matrix.&lt;/p&gt;

&lt;p&gt;Sad to say it, but the speedup will probably be completely unnoticeable since the for loop is only over 4 iterations, as opposed to something over 40,000 iterations&lt;/p&gt;

&lt;h2 id=&quot;visualizing-sift&quot;&gt;Visualizing SIFT&lt;/h2&gt;

&lt;p&gt;Your get_features() function should return a NumPy array of shape (k, feat_dim) representing k stacked feature vectors (row-vectors), where “feat_dim” is the feature_dimensionality (e.g. 128 for standard SIFT).&lt;/p&gt;

&lt;p&gt;Since this is a 2D matrix, we can treat it as a grayscale image. Each row in the image would correspond to the feature vector for one interest point. We would hope that each feature vector would unique, so the image shouldn’t be completely uniform in color (all identical features) or completely black (all zero values). That would be a clue that your features are degenerate.&lt;/p&gt;

&lt;p&gt;For example, you might see something like this if you were to call:&lt;/p&gt;

&lt;p&gt;import matplotlib.pyplot as plt; plt.imshow(image1_features); plt.show()&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm4fywn2xohb/features.png&lt;/p&gt;

&lt;h2 id=&quot;trilinear-interpolation&quot;&gt;Trilinear Interpolation&lt;/h2&gt;

&lt;p&gt;http://paulbourke.net/miscellaneous/interpolation/&lt;/p&gt;

&lt;p&gt;On slide 30 of Lecture 7, Professor Hays was discussing trilinear interpolation. Trilinear interpolation is the name given to the process of linearly interpolating points within a box (3D) given values at the vertices of the box. We can think of our histogram as a 3D spatial histogram with &lt;script type=&quot;math/tex&quot;&gt;N_{\theta} \times N_x \times N_y&lt;/script&gt;, bins usually &lt;script type=&quot;math/tex&quot;&gt;8 \times 4 \times 4&lt;/script&gt;.
https://www.cc.gatech.edu/~hays/compvision/lectures/07.pdf&lt;/p&gt;

&lt;p&gt;You aren’t required to implement the trilinear interpolation for this project, but you may if you wish. I would recommend getting a baseline working first where the x and y derivatives at each pixel &lt;script type=&quot;math/tex&quot;&gt;I_x, I_y&lt;/script&gt; form 1 orientation, and that orientation goes into a single bin.&lt;/p&gt;

&lt;p&gt;Then you could try the trilinear interpolation afterwards once that is working (without trilinear interpolation, you can still get »80% accuracy on Notre Dame).&lt;/p&gt;

&lt;h2 id=&quot;sobel-vs-gaussian&quot;&gt;Sobel vs. Gaussian&lt;/h2&gt;

&lt;p&gt;Hi, I’m trying to decide which is a better way to compute the gradient for the Harris corner detection, before I compute my cornerness function. I’m confused about the difference between both.&lt;/p&gt;

&lt;p&gt;If I run just Sobel on my image, that means I’m getting the derivative, and smoothing with Gaussian in one go, right? And if I want to use Gaussian, I find the derivatives of the pixels, and apply Gaussian separately on the image? Not sure if one way is better than the other, and why.&lt;/p&gt;

&lt;p&gt;You can also do both with one filter.&lt;/p&gt;

&lt;p&gt;Suppose we have the image  &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
&lt;br/&gt;&lt;br/&gt;I = \begin{bmatrix}&lt;br/&gt;&lt;br/&gt;a &amp; b &amp; c \\&lt;br/&gt;&lt;br/&gt;d &amp; e &amp; f \\&lt;br/&gt;&lt;br/&gt;g &amp; h &amp; i&lt;br/&gt;&lt;br/&gt;\end{bmatrix}&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;One way to think about the Sobel x-derivative filter is that rather than looking at only &lt;script type=&quot;math/tex&quot;&gt;\frac{rise}{run}=\frac{f-d}{2}&lt;/script&gt; (centered at pixel e), we also use the x-derivatives above it and below it, e.g. &lt;script type=&quot;math/tex&quot;&gt;\frac{c-a}{2}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{i-g}{2}&lt;/script&gt;. But we weight the x-derivative in the center the most (this is a form of smoothing) so we use an approximation like&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ 2 \cdot (f-d) + (i-g)+ (c-a) }{8}&lt;/script&gt;

&lt;p&gt;meaning our kernel resembles&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{1}{8}&lt;br/&gt;&lt;br/&gt;\begin{bmatrix}&lt;br/&gt;&lt;br/&gt;1 &amp; 0 &amp; -1 \\&lt;br/&gt;&lt;br/&gt;2 &amp; 0 &amp; -2 \\&lt;br/&gt;&lt;br/&gt;1 &amp; 0 &amp; -1&lt;br/&gt;&lt;br/&gt;\end{bmatrix}&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;It is actually derived with directional derivatives.
https://www.researchgate.net/publication/239398674_An_Isotropic_3_3_Image_Gradient_Operator&lt;/p&gt;

&lt;p&gt;This is not identical to first blurring the image with a Gaussian filter, and then computing derivatives. We blur the image first because it makes the gradients less noisy.&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm1ficspl6jw/smoothing_gradients.png&lt;/p&gt;

&lt;p&gt;https://d1b10bmlvqabco.cloudfront.net/attach/jl1qtqdkuye2rp/jl1r1s4npvog2/jm1fiqeigs2g/derivative_theorem.png&lt;/p&gt;

&lt;p&gt;And an elegant fact that can save a step in smoothed gradient computation is to simply blur with the x and y derivatives of a Gaussian filter, by the following property:&lt;/p&gt;

&lt;p&gt;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture5_edges_cs131_2016.pdf
Slides&lt;/p&gt;

&lt;h2 id=&quot;nearest-neighbor-distance-ratio-algorithm-418&quot;&gt;nearest neighbor distance ratio algorithm 4.18&lt;/h2&gt;
&lt;p&gt;1) in the formula 4.18, what is meant by target descriptor, here it is Da ?&lt;/p&gt;

&lt;p&gt;2) In the formula 4.18 what is meant by Db and Dc as being descriptors?&lt;/p&gt;

&lt;p&gt;3)  What makes Da to be target descriptor and Db and Dc nearest neighbors ?&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;4) In formular 4.18&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Da - Db&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;is norm? or euclidean distance ?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;5) how is value of the descriptor such as Da related to the euclidean distance to Db?&lt;/p&gt;

&lt;p&gt;6) Is there such a thing as x and y coordinates of the center for a specific descriptor that helps to calculate the distances between the&lt;/p&gt;

&lt;p&gt;descriptors ? if so how to calculate the center of each descriptor ?&lt;/p&gt;

&lt;p&gt;1) in the formula 4.18, what is meant by target descriptor, here it is Da ?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; is a high-dimensional point. In the case of SIFT descriptors, &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; would be the SIFT feature vector in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;2) In the formula 4.18 what is meant by Db and Dc as being descriptors?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_B, D_C&lt;/script&gt; are high-dimensional points. These are the closest points, as measured by the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm, from &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; in this high dimensional space.&lt;/p&gt;

&lt;p&gt;3)  What makes Da to be target descriptor and Db and Dc nearest neighbors ?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; is the feature vector corresponding to an (x,y) location, for which we are trying to find matches in another image. &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; could be from Image1, and &lt;script type=&quot;math/tex&quot;&gt;D_B, D_C&lt;/script&gt; might be feature vectors corresponding to points in Image2&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;4) In formula 4.18&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Da - Db&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;is norm? or euclidean distance ?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\|D_A-D_B\|&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm, which we often call the Euclidean norm.&lt;/p&gt;

&lt;p&gt;5) how is value of the descriptor such as Da related to the euclidean distance to Db?&lt;/p&gt;

&lt;p&gt;Euclidean distance from &lt;script type=&quot;math/tex&quot;&gt;D_B&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;D_A&lt;/script&gt; depends upon knowing the location of &lt;script type=&quot;math/tex&quot;&gt;D_B,D_A&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;6) Is there such a thing as x and y coordinates of the center for a specific descriptor that helps to calculate the distances between the&lt;/p&gt;

&lt;p&gt;descriptors ? if so how to calculate the center of each descriptor ?&lt;/p&gt;

&lt;p&gt;Each SIFT descriptor corresponds to an (x,y) point. We form the SIFT descriptor by looking at a 16x16 patch, centered at that (x,y) location. This could be considered a “center” of the descriptor, although I think using that terminology could be confusing since the (x,y)  “center” location in &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is not necessarily related at all to center of the &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt; space.  It may be possible to use the spatial information in &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; to verify the matching of points in &lt;script type=&quot;math/tex&quot;&gt;R^{128}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;arctan-vs-arctan2&quot;&gt;Arctan vs. Arctan2&lt;/h2&gt;

&lt;p&gt;I would use arctan2 instead of arctan. Because of the sign ambiguity, a function cannot determine with certainty in which quadrant the angle falls only by its tangent value. 
https://stackoverflow.com/questions/283406/what-is-the-difference-between-atan-and-atan2-in-c&lt;/p&gt;

&lt;p&gt;Numpy arctan2 returns an “Array of angles in radians, in the range [-pi, pi].” (see here).
https://docs.scipy.org/doc/numpy/reference/generated/numpy.arctan2.html&lt;/p&gt;

&lt;p&gt;As long as you bin all of the gradient orientations consistently, it turns out it doesn’t matter if the histograms are created from 8 uniform intervals from [0,360] or 8 uniform intervals from [-180,180]. Histograms are empirical samples of a distribution, and translating the range won’t affect that distribution.&lt;/p&gt;

&lt;p&gt;More on arctan2 from StackOverflow:&lt;/p&gt;

&lt;p&gt;From school mathematics we know that the tangent has the definition&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;code&gt;tan(α) = sin(α) / cos(α)&lt;/code&gt;&lt;/em&gt;
and we differentiate between four quadrants based on the angle that we supply to the functions. The sign of the sin, cos and tan have the following relationship (where we neglect the exact multiples of π/2):&lt;/p&gt;

&lt;h2 id=&quot;--quadrant----angle--------------sin---cos---tan&quot;&gt;&lt;em&gt;&lt;code&gt;  Quadrant    Angle              sin   cos   tan&lt;/code&gt;&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;I           0    &amp;lt; α &amp;lt; π/2      +     +     +
  II          π/2  &amp;lt; α &amp;lt; π        +     -     -
  III         π    &amp;lt; α &amp;lt; 3π/2     -     -     +
  IV          3π/2 &amp;lt; α &amp;lt; 2π       -     +     -&amp;lt;/code&amp;gt;&amp;lt;/em&amp;gt;
Given that the value of tan(α) is positive, we cannot distinguish, whether the angle was from the first or third quadrant and if it is negative, it could come from the second or fourth quadrant. So by convention, atan() returns an angle from the first or fourth quadrant (i.e. -π/2 &amp;lt;= atan() &amp;lt;= π/2), regardless of the original input to the tangent.&lt;/p&gt;

&lt;p&gt;In order to get back the full information, we must not use the result of the division sin(α) / cos(α) but we have to look at the values of the sine and cosine separately. And this is what atan2() does. It takes both, the sin(α) and cos(α) and resolves all four quadrants by adding π to the result of atan() whenever the cosine is negative.&lt;/p&gt;

&lt;h2 id=&quot;autocorrelation-matrix&quot;&gt;Autocorrelation Matrix&lt;/h2&gt;

&lt;p&gt;Nicolas, that &lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt; in the equation you’ve written is not multiplication – it is convolution. Szeliski states that he has “replaced the weighted summations with discrete convolutions with the weighting kernel &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;”.&lt;/p&gt;

&lt;p&gt;So before we had values &lt;script type=&quot;math/tex&quot;&gt;w(x,y)&lt;/script&gt; that could have been values from a Gaussian probability density function. Let &lt;script type=&quot;math/tex&quot;&gt;z = \begin{bmatrix} x \\ y \end{bmatrix}&lt;/script&gt; be the stacked 2D coordinate locations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(z) = \frac{1}{(2 \pi)^{n/2} |\Sigma|^{1/2}} \mbox{exp} \Bigg( - \frac{1}{2} (z − \mu)^T \Sigma^{-1} (z − \mu) \Bigg)&lt;/script&gt;

&lt;p&gt;For example, where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is the center pixel location and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is the location of each pixel in the local neighborhood. These were used as weights in summations over a local neighborhood,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
M = \sum\limits_{x,y} w(x,y) \begin{bmatrix}I_x^2 &amp; I_xI_y \\I_xI_y &amp; I_y^2\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;But the elegant convolution approach is used here because it is equivalent to summing a bunch of elementwise multiplications –  each point in a local neighborhood with its weight value (as we saw in Proj 1, and here filtering is equivalent to convolution since Gaussian filter is symmetric).&lt;/p&gt;

&lt;h2 id=&quot;image-derivatives&quot;&gt;Image Derivatives&lt;/h2&gt;

&lt;p&gt;Great question – there are a number of ways to do it, and they will all give different results.&lt;/p&gt;

&lt;p&gt;Prof. Hays discussed in lecture how convolving (not cross-correlation filtering) with the Sobel filter is a good way to approximate image derivatives (here we could treat a Gaussian filter as the image to find its derivatives).&lt;/p&gt;

&lt;p&gt;We’ve recommended a number of potentially useful OpenCV and SciPy functions that can do so in the project page. These will be very helpful!&lt;/p&gt;

&lt;p&gt;Another simple way to approximate the derivative is to calculate the 1st discrete difference along the given axis. For example, in order to compute horizontal discrete differences, shift the image by 1 pixel to the left and subtract the two&lt;/p&gt;

&lt;p&gt;For example, suppose you have a matrix b&lt;/p&gt;

&lt;p&gt;b = np.array([[  0,   1,   1,   2],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [  3,   5,   8,  13],

   [ 21,  34,  55,  89],

   [144, 233, 377, 610]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;b[:,1:] - b[:,:-1]
We would get:&lt;/p&gt;

&lt;p&gt;array([[  1,   0,   1],&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   [  2,   3,   5],

   [ 13,  21,  34],

   [ 89, 144, 233]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we could the pad the matrix with zeros on the right column to bring it back to the original size.&lt;/p&gt;

&lt;h2 id=&quot;harris&quot;&gt;Harris&lt;/h2&gt;

&lt;p&gt;A gaussian filter is expressed as &lt;script type=&quot;math/tex&quot;&gt;g(\sigma_1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The second moment matrix at each pixel is convolved as follows:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = g(\sigma_1) * \begin{bmatrix} I_x^2 (\sigma_D) &amp; I_xI_y (\sigma_D) \\ I_xI_y (\sigma_D) &amp; I_{y}^2 (\sigma_D) \end{bmatrix} %]]&gt;&lt;/script&gt;
Giving a cornerness function in the lecture slides:
&lt;script type=&quot;math/tex&quot;&gt;har = \mbox{ det }[\mu(\sigma_1,\sigma_D)] - \alpha[\mbox{trace }\Big(\mu(\sigma_1,\sigma_D)\Big)]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;or, when evaluated,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;har = g(I_x^2)g(I_y^2) - [g(I_xI_y)]^2 - \alpha [g(I_x^2) + g(I_y^2)]^2&lt;/script&gt;

&lt;p&gt;So in the lecture slides notation, we can write &lt;script type=&quot;math/tex&quot;&gt;\mu(\sigma_1,\sigma_D)&lt;/script&gt; more simply by bringing in the filtering (identical to convolution here) operation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = \begin{bmatrix} g(\sigma_1) * I_x^2 (\sigma_D) &amp; g(\sigma_1) * I_xI_y (\sigma_D) \\ g(\sigma_1) * I_xI_y (\sigma_D) &amp; g(\sigma_1) * I_{y}^2 (\sigma_D) \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;It may be easier to understand if we write the equation in the following syntax:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu(\sigma_1,\sigma_D) = \begin{bmatrix} g(\sigma_1) * \Big(g(\sigma_D) * I_x \Big)^2 &amp; g(\sigma_1) * \Bigg[ \Big(g(\sigma_D) * I_x \Big) \odot \Big(g(\sigma_D) * I_x \Big) \Bigg] \\g(\sigma_1) * \Bigg[ \Big(g(\sigma_D) * I_x \Big) \odot \Big(g(\sigma_D) * I_x \Big) \Bigg] &amp; g(\sigma_1) * \Bigg[g(\sigma_D) * I_y \Bigg]^2\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;I should have explained that above – &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt; is the Hadamard product (element wise multiplication).&lt;/p&gt;

&lt;p&gt;Question:
How is 1x1 convolution idential to a fully-connected layer&lt;/p&gt;

&lt;p&gt;5x5 kernel, with 5x5 input. Elementwise multiply. Same as matrix multiplication because…&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Epipolar Geometry</title><link href="http://localhost:4000/epipolar-geometry/" rel="alternate" type="text/html" title="Epipolar Geometry" /><published>2018-11-19T03:01:00-08:00</published><updated>2018-11-19T03:01:00-08:00</updated><id>http://localhost:4000/epipolar-geometry</id><content type="html" xml:base="http://localhost:4000/epipolar-geometry/">&lt;h2 id=&quot;why-know-epipolar-geometry&quot;&gt;Why know Epipolar Geometry?&lt;/h2&gt;
&lt;p&gt;Modern robotic computer vision tasks like Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM) would not be possible without feature matching. Tools from Epipolar Geometry are an easy way to discard outliers in feature matching and are widely used.&lt;/p&gt;

&lt;h2 id=&quot;the-fundamental-matrix&quot;&gt;The Fundamental Matrix&lt;/h2&gt;

&lt;p&gt;The Fundamental matrix provides a correspondence \(x^TFx^{\prime} = 0\), where \(x,x^{\prime}\) are 2D corresponding points in separate images. In other words,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} u^{\prime} &amp; v^{\prime} &amp; 1 \end{bmatrix} \begin{bmatrix} f_{11} &amp; f_{12} &amp; f_{13} \\ f_{21} &amp; f_{22} &amp; f_{23} \\ f_{31} &amp; f_{32} &amp; f_{33} \end{bmatrix} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 0 %]]&gt;&lt;/script&gt;

&lt;p&gt;Longuet-Higgins’ 8-Point Algorithm [1] provides the solution for estimating \(F\) if at least 8 point correspondences are provided. A system of linear equations is formed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Af = \begin{bmatrix} u_1 u_1^{\prime} &amp; u_1v_1^{\prime} &amp; u_1 &amp; v_1 u_1^{\prime} &amp; v_1 v_1^{\prime} &amp; v_1 &amp; u_1^{\prime} &amp; v_1^{\prime} &amp; 1 \\ \vdots &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots &amp; \vdots  \\   u_n u_n^{\prime} &amp; u_n v_n^{\prime} &amp; u_n &amp; v_n u_n^{\prime} &amp; v_n v_n^{\prime} &amp; v_n &amp; u_n^{\prime} &amp; v_n^{\prime} &amp; 1 \end{bmatrix} \begin{bmatrix} f_{11} \\ f_{12} \\ f_{13} \\ f_{21} \\ \vdots \\ f_{33} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The matrix-vector product above can be driven to zero by minimizing the norm, and avoiding the degenerate solution that \(x=0\) with a constraint that the solution lies upon the unit ball, e.g.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
  \underset{\|x\|=1}{\mbox{minimize}} &amp; \|A x \|_2^2 = x^TA^TAx = x^TBx
  \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;By the Courant-Fisher characterization, it is well known that if \(B\) is a \(n \times n\) symmetric matrix with eigenvalues \(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n \) and corresponding eigenvectors \(v_1, \dots, v_n\), then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_n = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} x^TBx&lt;/script&gt;

&lt;p&gt;meaning the eigenvector associated with the smallest eigenvalue \(\lambda_n\) of \(B\) is the solution \(x^{\star}\). The vector \(x^{\star}\) contains the 9 entries of the Fundamental matrix \(F^{\star}\).&lt;/p&gt;

&lt;p&gt;This is a specific instance of the extremal trace \([2]\) (or trace minimization on a unit sphere) problem, with \(k=1\), i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \begin{array}{ll}
    \mbox{minimize} &amp; \mathbf{\mbox{Tr}}(X^TBX) \\
    \mbox{subject to} &amp; X^TX=I_k
    \end{array}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(I_k\) denotes the \(k \times k\) identity matrix. The unit ball constraint avoids the trivial solution when all eigenvalues \(\lambda_i\) are zero instead of a single zero eigenvalue.&lt;/p&gt;

&lt;p&gt;In our case, since \(U,V\) are orthogonal matrices (with orthonormal columns), then \(U^TU=I\). Thus, the SVD of \(B\) yields 
\begin{equation}
A^TA = (U\Sigma V^T)^T (U\Sigma V^T) = V\Sigma U^TU \Sigma V^T = V \Sigma^2 V^T.
\end{equation}&lt;/p&gt;

&lt;p&gt;Since \(B=A^TA\), \(B\) is symmetric, and thus the columns of \(V=\begin{bmatrix}v_1 \dots v_n \end{bmatrix}\) are eigenvectors of \(B\). \(V\) can equivalently be computed with the SVD of \(A\) or \(B\), since \(V\) appears in both decompositions: \(A=U \Sigma V^T\) and \(B=V\Sigma^2V^T\).&lt;/p&gt;

&lt;h2 id=&quot;proof-the-svd-provides-the-solution&quot;&gt;Proof: the SVD provides the solution&lt;/h2&gt;
&lt;p&gt;The proof is almost always taken for granted, but we will provide it here for completeness. Because \(B\) is symmetric, there exists a set of \(n\) orthonormal eigenvectors, yielding an eigendecomposition \(B=V^T \Lambda V\). Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{llll}
    \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; x^TBx = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; x^TV^T \Lambda Vx = \mbox{arg } \underset{\|x\|=1}{\mbox{min }} &amp; (Vx)^T \Lambda (Vx)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since \(V\) is orthogonal, \(|Vx| = |x|\), thus minimizing \((Vx)^T \Lambda (Vx)\) is equivalent to minimizing \(x^T \Lambda x\). Since \(\Lambda\) is diagonal, \(x^TBx = \sum\limits_{i=1}^{n} \lambda_i x_i^2\) where \({\lambda_i}_{i=1}^n\) are the eigenvalues of \(B\). Let \(q_i=x_i^2\), meaning \(q_i\geq 0\) since it represents a squared quantity. Since \(|x|=1\), then \(\sqrt{\sum\limits_i x_i^2}=1\), \(\sum\limits_i x_i^2=1 \), \(\sum\limits_i q_i = 1\). Thus,&lt;/p&gt;

&lt;p&gt;\begin{equation}
 \underset{|x|=1}{\mbox{min }}  x^TBx= \underset{|x|=1}{\mbox{min }} \sum\limits_{i=1}^{n} \lambda_i x_i^2= \underset{q_i}{\mbox{min }} \sum\limits_i \lambda_i q_i = \underset{q_i}{\mbox{min }} \lambda_n \sum\limits_i q_i = \lambda_n
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\lambda_n\) is the smallest eigenvalue of \(B\). The last line follows since \(q_i \geq 0\) and \(\sum\limits_i q_i = 1\), therefore we have a convex combination of a set of numbers \( {\lambda_i}_{i=1}^n \) on the real line. By properties of a convex combination, the result must lie in between the smallest and largest number. Now that we know the minimum is \(\lambda_n\), we can obtain the argmin by the following observation:&lt;/p&gt;

&lt;p&gt;If \(v\) is an eigenvector of \(B\), then 
\begin{equation}
    Bv = \lambda_n v
\end{equation}
Left multiplication with \(v^T\) simplifies the right side because \(v^Tv=1\), by our constraint that \(|x|=1\). We find:
\begin{equation}
    v^T(Bv) = v^T (\lambda_n v) = v^Tv \lambda_n = \lambda_n
\end{equation}
Thus the eigenvector \(v\) associated with the eigenvalue \(\lambda_n\) is \(x^{\star}\). \( \square\)&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;[1] H. Longuet Higgins. A computer algorithm for reconstructing a scene from two projections. &lt;em&gt;Nature&lt;/em&gt;, 293, 1981.&lt;/p&gt;

&lt;p&gt;[2] S.  Boyd.   Low  rank  approximation  and  extremal  gain  problems,  2008.   URL &lt;a href=&quot;http://ee263.stanford.edu/notes/low_rank_approx.pdf&quot;&gt;http://ee263.stanford.edu/notes/low_rank_approx.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Fundamental matrix, solution via SVD.</summary></entry><entry><title type="html">Parallel Computing with MPI</title><link href="http://localhost:4000/mpi/" rel="alternate" type="text/html" title="Parallel Computing with MPI" /><published>2018-10-02T04:01:00-07:00</published><updated>2018-10-02T04:01:00-07:00</updated><id>http://localhost:4000/mpi</id><content type="html" xml:base="http://localhost:4000/mpi/">&lt;p&gt;mpiexec -x -np 2 xterm -e cuda-gdb ./myapp&lt;/p&gt;

&lt;p&gt;mpirun –np 2 nvprof –log-file profile.out.%p&lt;/p&gt;

&lt;p&gt;nvprof ./addMatrices -n 4000&lt;/p&gt;

&lt;p&gt;cuda-memcheck ./memcheck_demo&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 mpirun -np 4 nvprof –output-profile profile.%p.nvprof ./main [args]&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 makes MVAPICH2 CUDA aware.&lt;/p&gt;

&lt;p&gt;MV2_USE_CUDA=1 mpirun -np 1 nvprof –kernels gpu_GEMM –analysis-metrics
–output-profile GEMMmetrics.out.%p.nvprof ./main [args]&lt;/p&gt;

&lt;p&gt;nvvp &amp;amp;&lt;/p&gt;</content><author><name></name></author><summary type="html">Demystify backprop.</summary></entry></feed>