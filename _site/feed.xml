<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-29T02:18:59-05:00</updated><id>http://localhost:4000/</id><title type="html">John Lambert</title><subtitle>Ph.D. Candidate in Computer Vision.
</subtitle><entry><title type="html">Lie Groups and Rigid Body Kinematics</title><link href="http://localhost:4000/lie-groups/" rel="alternate" type="text/html" title="Lie Groups and Rigid Body Kinematics" /><published>2018-12-28T06:00:00-05:00</published><updated>2018-12-28T06:00:00-05:00</updated><id>http://localhost:4000/lie-groups</id><content type="html" xml:base="http://localhost:4000/lie-groups/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whyliegroups&quot;&gt;Why do we need Lie Groups?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#liegroups&quot;&gt;Lie Groups&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#son&quot;&gt;SO(N)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so2&quot;&gt;SO(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#so3&quot;&gt;SO(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se2&quot;&gt;SE(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#se3&quot;&gt;SE(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conjugation&quot;&gt;Conjugation in Group Theory&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lie-algebra&quot;&gt;The Lie Algebra&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;whyliegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-lie-groups&quot;&gt;Why do we need Lie Groups?&lt;/h2&gt;

&lt;p&gt;Rigid bodies have a state which consists of position and orientation. When sensors are placed on a rigid body (e.g. a robot), they provide measurements in the body frame. Suppose we wish to take a measurement &lt;script type=&quot;math/tex&quot;&gt;y_b&lt;/script&gt; from the body frame and move it to the world frame, &lt;script type=&quot;math/tex&quot;&gt;y_w&lt;/script&gt;. We can do this via left multiplication with a transformation matrix &lt;script type=&quot;math/tex&quot;&gt;{}^{w}T_{b}&lt;/script&gt;, a member of the matrix Lie groups, that transports the point from one space to another space:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_w = {}^{w}T_{b} y_b&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;liegroups&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lie-groups&quot;&gt;Lie Groups&lt;/h2&gt;

&lt;p&gt;When we are working with pure rotations, we work with Special Orthogonal groups, &lt;script type=&quot;math/tex&quot;&gt;SO(\cdot)&lt;/script&gt;. When we are working with a rotation and a translation together, we work with Special Euclidean groups &lt;script type=&quot;math/tex&quot;&gt;SE(\cdot)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Lie Groups are unique because they are &lt;strong&gt;both a group and a manifold&lt;/strong&gt;. They are continuous manifolds in high-dimensional spaces, and have a group structure. I’ll describe them in more detail below.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;son&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;son&quot;&gt;SO(N)&lt;/h3&gt;

&lt;p&gt;Membership in the Special Orthogonal Group &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt; requires two matrix properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;R^TR = I&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mbox{det}(R) = +1&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives us a very helpful property: &lt;script type=&quot;math/tex&quot;&gt;R^{-1} = R^T&lt;/script&gt;, so the matrix inverse is as simple as taking the transpose.  We will generally work with &lt;script type=&quot;math/tex&quot;&gt;SO(N)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;N=2,3&lt;/script&gt;, meaning the matrices are rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{2 \times 2}&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;These rotation matrices &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; are not commutative.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;so2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so2&quot;&gt;SO(2)&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is a 1D manifold living in a 2D Euclidean space, e.g. moving around a circle.  We will be stuck with singularities if we use 2 numbers to parameterize it, which would mean kinematics break down at certain orientations.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt; is the space of orthogonal matrices that corresponds to rotations in the plane.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A simple example&lt;/strong&gt;:
Let’s move from the body frame &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; to a target frame &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_t = {}^tR_b(\theta) P_o&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
5 \mbox{ cos} (\theta) \\
5 \mbox{ sin} (\theta)
\end{bmatrix} = \begin{bmatrix} cos(\theta) &amp; -sin(\theta) \\ sin(\theta) &amp; cos(\theta) \end{bmatrix} * \begin{bmatrix} 5 \\ 0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;As described in [3], another way to think of this is to consider that a robot can be rotated counterclockwise by some angle &lt;script type=&quot;math/tex&quot;&gt;\theta \in [0,2 \pi)&lt;/script&gt; by mapping every &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x, y) \rightarrow (x \mbox{ cos } \theta − y \mbox{ sin } \theta, x \mbox{ sin } \theta + y \mbox{ cos } \theta).&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;so3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;so3&quot;&gt;SO(3)&lt;/h3&gt;

&lt;p&gt;There are several well-known parameterizations of &lt;script type=&quot;math/tex&quot;&gt;R \in SO(3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1.) &lt;script type=&quot;math/tex&quot;&gt;R \in \mathbf{R}^{3 \times 3}&lt;/script&gt; full rotation matrix, 9 parameters – there must be 6 constraints&lt;/li&gt;
  &lt;li&gt;(2.) Euler angles, e.g. &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt;, so 3 parameters&lt;/li&gt;
  &lt;li&gt;(3.) Angle-Axis parameters &lt;script type=&quot;math/tex&quot;&gt;(\vec{a}, \phi)&lt;/script&gt;, which is 4 parameters and 1 constraint (unit length)&lt;/li&gt;
  &lt;li&gt;(4.) Quaternions (&lt;script type=&quot;math/tex&quot;&gt;q_0,q_1,q_2,q_3)&lt;/script&gt;, 4 parameters and 1 constraint (unit length)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are only 3 degrees of freedom in describing a rotation. But this object doesn’t live in 3D space. It is a 3D manifold, embedded in a 4-D Euclidean space.&lt;/p&gt;

&lt;p&gt;Parameterizations 1,3,4 are overconstrained, meaning they employ more parameters than we strictly need. With overparameterized representations, we have to do extra work to make sure we satisfy the constraints of the representation.&lt;/p&gt;

&lt;p&gt;As it turns out &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; cannot be parameterized by only 3 parameters in a non-singular way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Euler Angles&lt;/strong&gt;
One parameterization of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; is to imagine three successive rotations around different axes. The Euler angles encapsulate yaw-pitch-roll: first, a rotation about the z-axis (yaw, &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;). Then, a rotation about the pitch axis by &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (via right-hand rule), and finally we perform a roll by &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/euler_angles.jpg&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    The Euler angles.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The sequence of successive rotations is encapsulated in &lt;script type=&quot;math/tex&quot;&gt;{}^{w}R_b&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{w}R_b = R_R(\phi) R_P(\theta) R_Y (\psi)&lt;/script&gt;

&lt;p&gt;As outlined in [3], these successive rotations by &lt;script type=&quot;math/tex&quot;&gt;(\phi, \theta, \psi)&lt;/script&gt; are defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{yaw} = R_y (\psi) = \begin{bmatrix}
\mbox{cos} \psi &amp; -\mbox{sin} \psi  &amp; 0 \\
\mbox{sin} \psi &amp; \mbox{cos} \psi &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{pitch} = R_p (\theta) = \begin{bmatrix}
\mbox{cos} \theta &amp; 0 &amp; \mbox{sin} \theta \\
 0 &amp; 1 &amp; 0 \\
 -\mbox{sin} \theta &amp; 0 &amp; \mbox{cos} \theta \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R_{roll} = R_R (\phi) = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; \mbox{cos} \phi &amp; -\mbox{sin} \phi \\
0 &amp; \mbox{sin} \phi &amp;  \mbox{cos} \phi \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;You have probably noticed that each rotation matrix &lt;script type=&quot;math/tex&quot;&gt;\in \mathbf{R}^{3 \times 3}&lt;/script&gt; above is a simple extension of the 2D rotation matrix from &lt;script type=&quot;math/tex&quot;&gt;SO(2)&lt;/script&gt;. For example, the yaw matrix &lt;script type=&quot;math/tex&quot;&gt;R_{yaw}&lt;/script&gt; performs a 2D rotation with respect to the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; coordinates while leaving the &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; coordinate unchanged [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se2&quot;&gt;SE(2)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;3 \times 3&lt;/script&gt; matrices, moving a point in homogenous coordinates to a new frame. It is important to remember that this represents a rotation followed by a translation (not the other way around).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T = \begin{bmatrix} x_w \\ y_w \\ 1 \end{bmatrix} = \begin{bmatrix}
 R_{2 \times 2}&amp; &amp; t_{2 \times 1}  \\
&amp; \ddots &amp; \vdots  \\
0 &amp; 0 &amp; 1
\end{bmatrix} * \begin{bmatrix} x_b \\ y_b \\ 1 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;By adding an extra dimension to the input points and transformation matrix &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; the translational part of the transformation is absorbed [3].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;se3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;se3&quot;&gt;SE(3)&lt;/h3&gt;

&lt;p&gt;The real space &lt;script type=&quot;math/tex&quot;&gt;SE(3)&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;4 \times 4&lt;/script&gt; matrices, the real space resembles:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; R_{3 \times 3} &amp; &amp;  t_{3 \times 1} \\
&amp; &amp; &amp; \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;conjugation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conjugation-in-group-theory&quot;&gt;Conjugation in Group Theory&lt;/h2&gt;

&lt;p&gt;Surprisingly, movement in &lt;script type=&quot;math/tex&quot;&gt;SE(2)&lt;/script&gt; can always be achieved by moving somewhere, making a rotation there, and then moving back.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SE(2) = (\cdot) SO(2) (\cdot)&lt;/script&gt;

&lt;p&gt;If we move to a point by vector movement &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, essentially we perform: &lt;script type=&quot;math/tex&quot;&gt;B-p&lt;/script&gt;, then go back with &lt;script type=&quot;math/tex&quot;&gt;p + \dots&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
B^{\prime} = \begin{bmatrix}
R &amp; t \\
0 &amp; 1
\end{bmatrix}_B = \begin{bmatrix}
I &amp; p \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
R &amp; 0 \\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
I &amp; -p \\
0 &amp; 1
\end{bmatrix}_B %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;lie-algebra&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;connecting-spatial-coordinates-and-spatial-velocities-the-lie-algebra&quot;&gt;Connecting Spatial Coordinates and Spatial Velocities: The Lie Algebra&lt;/h2&gt;

&lt;p&gt;The Lie Algebra maps spatial coordinates to spatial velocity.&lt;/p&gt;

&lt;p&gt;In the case of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt;, the Lie Algebra is the space of skew-symmetric matrices&lt;/p&gt;

&lt;p&gt;Rigid Body Kinematics: we want a differential equation (ODE) that links &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{}^{^w}R_b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;in particular,
&lt;script type=&quot;math/tex&quot;&gt;{}^{^w} \dot{R}_r = f({}^{^w}R_b, \vec{\omega})&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\frac{ \partial }{\partial t}(R^R = I)
 \\
\dot{R}^TR + R^T \dot{R} = 0 \\
\dot{R}^TR = -R^T \dot{R} 
\end{aligned}&lt;/script&gt;

&lt;p&gt;we define &lt;script type=&quot;math/tex&quot;&gt;\Omega = R^T \dot{R}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^T = (R^T \dot{R} )^T = \dot{R}^TR  = -R^T \dot{R}  = -\Omega&lt;/script&gt;

&lt;p&gt;Skew-symmetric! $\Omega^T = -\Omega$&lt;/p&gt;

&lt;p&gt;In fact,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Omega = \begin{bmatrix}
0 &amp; -\omega_z &amp; \omega_y \\
\omega_z &amp; 0 &amp; -\omega_x \\
-\omega_y &amp; \omega_x &amp; 0
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega} = \begin{bmatrix} \omega_x \\ \omega_y \\ \omega_z \end{bmatrix}&lt;/script&gt; is the angular velocity&lt;/p&gt;

&lt;p&gt;Notation! the “hat” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\omega}^{\hat{}} =\Omega&lt;/script&gt;

&lt;p&gt;the “v” map&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega^{v} = \bar{\omega}&lt;/script&gt;

&lt;p&gt;So we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\dot{R} = R \Omega \\
\Omega = \bar{\omega}^{\hat{}}
\end{aligned}&lt;/script&gt;

&lt;p&gt;\item In terms of frames, we have Poisson’s kinematic equation&lt;/p&gt;

&lt;p&gt;\item The intrinsic equation is (where &lt;script type=&quot;math/tex&quot;&gt;\vec{\omega}_b&lt;/script&gt; in the body frame is $\Omega_b$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^w}\dot{R}_b =  {}^{^w}R_b \Omega_b&lt;/script&gt;

&lt;p&gt;The “strap-down” equation (extrinsic)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{}^{^b}\dot{R}_w =  - \Omega_b {}^{^b}R_w&lt;/script&gt;

&lt;p&gt;Take vector on aircraft, put it into the coordinate frame of the world&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{}_{^w}R_b = \begin{bmatrix} | &amp; | &amp; | \\ {}^{^w} \hat{x}_b &amp; {}^{^w}\hat{y}_b &amp; {}^{^w} \hat{z}_b \\ | &amp; | &amp; |  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Omega \vec{v}_b = \omega \times \vec{v}_b&lt;/script&gt;

&lt;p&gt;Can we use this for filtering? Can we use &lt;script type=&quot;math/tex&quot;&gt;\dot{R} = R \Omega&lt;/script&gt; directly in an EKF, UKF
We’d have to turn it into discrete-time 
Naively, you might do the first order Euler approximation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ R_{t + \delta_t} -R_t}{\delta t} \approx R_t \Omega_t&lt;/script&gt;

&lt;p&gt;This does not work!&lt;/p&gt;

&lt;p&gt;You won’t maintain orthogonality! The defining property of &lt;script type=&quot;math/tex&quot;&gt;SO(3)&lt;/script&gt; was orthogonality, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{t + \delta t} \not\in SO(3)&lt;/script&gt;

&lt;p&gt;and pretty soon&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
R_{t + \delta t}^T R_{t + \delta t} \neq I \\
\mbox{det }(R_{t+\delta t}) \neq +1
\end{aligned}&lt;/script&gt;

&lt;p&gt;The 3x3 matrix will not be recognizable of a rotation&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frank Dellaert. Lecture Presentations of MM 8803: Mobile Manipulation, taught at the Georgia Institute of Technology, Fall 2018.&lt;/p&gt;

&lt;p&gt;[2] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.&lt;/p&gt;

&lt;p&gt;[3] Steven M. LaValle. &lt;em&gt;Planning Algorithms&lt;/em&gt;. Cambridge University Press, 2006, New York, NY, USA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rigid Body Kinematics and Filtering&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kinematics with Euler Angles&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{R} = f(R, \vec{w}), (\dot{\phi}, \dot{\theta}, \dot{\psi}) = f(\phi, \theta, \psi, \vec{\omega})
\end{equation}&lt;/p&gt;

&lt;p&gt;\item&lt;/p&gt;

&lt;p&gt;\begin{equation}
\vec{\omega} = \begin{bmatrix} w_x \ w_y \ w_z \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; -\mbox{sin} \theta \ 0 &amp;amp; \mbox{cos} \phi &amp;amp; \mbox{sin} \phi \mbox{cos} \phi \ 0 &amp;amp; -\mbox{sin} \phi &amp;amp; \mbox{cos} \phi \mbox{cos} \theta \end{bmatrix} \begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;\item We want to invert this matrix and solve for $\begin{bmatrix} \dot{\phi} \ \dot{\theta} \ \dot{\psi} \end{bmatrix}$&lt;/p&gt;</content><author><name></name></author><summary type="html">SO(2), SO(3), SE(2), SE(3), Lie algebras</summary></entry><entry><title type="html">Understanding Multivariate Gaussians and Covariance</title><link href="http://localhost:4000/gauss-covariance/" rel="alternate" type="text/html" title="Understanding Multivariate Gaussians and Covariance" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/gaussians-and-covariance</id><content type="html" xml:base="http://localhost:4000/gauss-covariance/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-multivariate-gaussian-random-variable&quot;&gt;What is a multivariate Gaussian random variable?&lt;/h2&gt;

&lt;p&gt;Gaussian R.V.s are parameterized by two quantities:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X \sim p(x)  = \mathcal{N}(\mu_x, \Sigma_x)&lt;/script&gt;

&lt;p&gt;Preceded by a term for normalization&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \Sigma_x) = 
\frac{1}{\sqrt{(2\pi)^n|\Sigma_x|}}\mbox{exp}
\Bigg\{ -\frac{1}{2} (x - \mu_x)^T \Sigma_x^{-1} (x - \mu_x) \Bigg\}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the dimension, i.e. &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbbm{R}^n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And of course in the scalar case, we see&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\mu_x, \sigma_x) =&lt;/script&gt;

&lt;p&gt;Level sets trace out ellipses that are centered at &lt;script type=&quot;math/tex&quot;&gt;\mu_x&lt;/script&gt;, have minor and major axes (in 2D), and ellipsoids in higher dimensions&lt;/p&gt;

&lt;p&gt;Mean:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[X] = \int_x x p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_x = \int_x x \mathcal{N}(\mu_x,\Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Covariance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(X- \mu_x)(X - \mu_x)^T] = \int_x (x-\mu_x)(x-\mu_x)^T p(x) dx&lt;/script&gt;

&lt;p&gt;need to show&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_x = \int_x (x- \mu_x) (x-\mu_x)^T \mathcal{N}(\mu_x, \Sigma_x) dx&lt;/script&gt;

&lt;p&gt;Gaussian distribution is a second-order distribution, which does not mean that the higher order moments are zero (not true in general)
Convert between a standard normal, and any multivariate Gaussian&lt;/p&gt;

&lt;h2 id=&quot;the-standard-normal-distribution&quot;&gt;The Standard Normal Distribution&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_s \sim \mathcal{N}(0, I)&lt;/script&gt;

&lt;p&gt;unit (identity) covariance, so each axis decouples, compute integral over each axis separately&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to generate any Gaussian R.V. from standard normal&lt;/strong&gt; $$X_s$?$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X = \Sigma_x^{1/2}X_s + \mu_x&lt;/script&gt;

&lt;p&gt;Can obtain by scaling by covariance matrix, and by translating by mean
Very good for simulating
With MATLAB, can generate scalar, unit variance, 0 mean Gaussian R.V. with \texttt{randn}
Call \texttt{randn} $n$ times to populate $X_s$, and them multiply, then add
And we can compute via Cholesky Decomposition (unique if positive definite)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma = \Sigma_x^{1/2}(\Sigma_x^{1/2})^T&lt;/script&gt;

&lt;h2 id=&quot;matrix-square-roots&quot;&gt;Matrix Square Roots&lt;/h2&gt;

&lt;p&gt;\item There are other possible matrix square roots
\item \textbf{How to transform any Gaussian R.V. to the standard normal} $X_s$?
\item Do so via rearrangement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
X_s = \Sigma_x^{-1/2}(X - \mu_x), &amp; X \sim \mathcal{N}(\mu_x, \Sigma_x)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Much easier to integrate over the form on the RHS, not LHS
Comes from method of derived distributions
Derived Distributions: Given &lt;script type=&quot;math/tex&quot;&gt;X \sim p(x)&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Y=f(X)&lt;/script&gt;, find &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt;
Here &lt;script type=&quot;math/tex&quot;&gt;X = X_s&lt;/script&gt;, and function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the linear distribution &lt;script type=&quot;math/tex&quot;&gt;AX + b&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;properties-of-multivariate-gaussians&quot;&gt;Properties of Multivariate Gaussians&lt;/h2&gt;

&lt;h2 id=&quot;how-can-we-understand-a-covariance-matrix&quot;&gt;How can we understand a covariance matrix?&lt;/h2&gt;

&lt;p&gt;Larger covariance means more uncertainty. Isocontours/error ellipses&lt;/p&gt;

&lt;p&gt;We set &lt;script type=&quot;math/tex&quot;&gt;P=0.95&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon = \frac{1-P}{2 \pi |\Sigma|^{1/2}} = \frac{1-0.95}{2 \pi |\Sigma|^{1/2}} = \frac{0.05}{2 \pi |\Sigma|^{1/2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}(\varepsilon) = \Big\{ x \mid p(x) \geq \varepsilon \Big\}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import numpy as np
import pdb
import matplotlib.pyplot as plt
import seaborn as sns


import scipy

sns.set_style({'font.family': 'Times New Roman'})

def plot_gauss_ellipse(mu, cov, color='g', rad=2, ):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Plots a 2D ellipse derived from a 2D Gaussian specified by mu &amp;amp; cov.
	
	USAGE:
		hs = plotGaussEllipses( mus, Cs, [rad] )
	
	Args:
	-	mus: Numpy array of shape (2,), representing mean
	-	Cs: Numpy array of shape (2,2), representing covariance matrix
	-	color: string representing Matplotlib color
	-	rad: [2] Number of std to create the ellipse to
	
	Returns:
	-	None
	
	color choices: ['b', 'g', 'r', 'c', 'm', 'y', 'k']
	&quot;&quot;&quot;
	cRow, ccol, ra, rb, phi = gauss2ellipse( mu, cov, rad)
	plotEllipse( cRow, ccol, ra, rb, phi, color)



def gauss2ellipse(mu, C, rad=2):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Creates an ellipse representing the 2D Gaussian distribution.
	
	Creates an ellipse representing the 2D Gaussian distribution with mean mu
	and covariance matrix C.  Returns 5 parameters that specify the ellipse.
	
	USAGE
	 [cRow, cCol, ra, rb, phi] = gauss2ellipse( mu, C, [rad] )
	
	Args:
	-	mu: 1x2 vector representing the center of the ellipse
	-	C: 2x2 cov matrix
	-	rad: [2] Number of std to create the ellipse to
	
	OUTPUTS
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis length (in pixels) of the ellipse
	-	rb: semi-minor axis length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	
	EXAMPLE
	#  [cRow, cCol, ra, rb, phi] = gauss2ellipse( [5 5], [1 0; .5 2] )
	#  plotEllipse( cRow, cCol, ra, rb, phi );
	&quot;&quot;&quot;
	# error check
	if mu.size != 2 or C.shape != (2,2):
		print('Works only for 2D Gaussians')
		quit()

	# decompose using SVD
	_,D,Rh = np.linalg.svd(C)
	R = Rh.T
	normstd = np.sqrt(D)

	# get angle of rotation (in row/column format)
	phi = np.arccos(R[0,0])

	if R[1,0] &amp;lt; 0:
		phi = 2*np.pi - phi
	phi = np.pi/2 - phi

	# get ellipse radii
	ra = rad * normstd[0]
	rb = rad * normstd[1]

	# center of ellipse
	cRow = mu[0]
	cCol = mu[1]

	return cRow, cCol, ra, rb, phi



def plotEllipse(cRow,cCol,ra,rb,phi,color='b',nPnts=100,lw=1,ls='-'):
	&quot;&quot;&quot;
	Adapted from Piotr Dollar's https://github.com/pdollar/toolbox/
	Adds an ellipse to the current plot.
	
	USAGE:
	-	h,hc,hl = plotEllipse(cRow,cCol,ra,rb,phi,[color],[nPnts],[lw],[ls])
	
	Args:
	-	cRow: the row location of the center of the ellipse
	-	cCol: the column location of the center of the ellipse
	-	ra: semi-major axis radius length (in pixels) of the ellipse
	-	rb: semi-minor axis radius length (in pixels) of the ellipse
	-	phi: rotation angle (radians) of semimajor axis from x-axis
	-	color: ['b'] color for ellipse
	-	nPnts: [100] number of points used to draw each ellipse
	-	lw: [1] line width
	-	ls: ['-'] line style

	Returns:
	-	h : handle to ellipse
	-	hc: handle to ellipse center
	-	hl: handle to ellipse orient

	EXAMPLE:
		plotEllipse( 3, 2, 1, 5, pi/6, 'g');
	&quot;&quot;&quot;
	# plot ellipse (rotate a scaled circle):
	ts = np.linspace(-np.pi, np.pi, nPnts+1)
	cts = np.cos(ts)
	sts = np.sin(ts)

	x = ra * cts * np.cos(-phi) + rb * sts * np.sin(-phi) + cCol
	y = rb * sts * np.cos(-phi) - ra * cts * np.sin(-phi) + cRow
	h = plt.plot(x,y, color=color, linewidth=lw, linestyle=ls)

	# plot center point and line indicating orientation
	hc = plt.plot(cCol, cRow, 'k+', color=color, linewidth=lw, linestyle=ls)

	x = [cCol, cCol+np.cos(-phi)*ra]
	y = [cRow, cRow-np.sin(-phi)*ra]
	hl = plt.plot(x, y, color=color, linewidth=lw, linestyle=ls)

	return h,hc,hl


def gen_from_distribution():



	Sigma_sqrt = scipy.linalg.sqrtm(Sigma)
	Sigma_inv = np.linalg.inv(Sigma)
	tiled_mu = np.tile(mu,(1000,1)).T
	samples = np.matmul( Sigma_sqrt, np.random.randn(2,1000) ) + tiled_mu
	exterior_samples = np.zeros((1,2))
	interior_samples = np.zeros((1,2))

	for i in range(samples.shape[1]):
			X = samples[:,i]
			f_val = 0.5 * np.matmul( np.matmul( (X - mu).T, Sigma_inv), X - mu )
			if f_val &amp;lt; -np.log(0.05):
				interior_samples = np.vstack([interior_samples, np.reshape(X,(1,2)) ])
			else:
				exterior_samples = np.vstack([exterior_samples, np.reshape(X,(1,2)) ])

	plt.scatter(interior_samples[:,0], interior_samples[:,1], c= 'b')
	plt.scatter(exterior_samples[:,0], exterior_samples[:,1], c= 'b')

def plot_gauss_ellipse_v2():
	&quot;&quot;&quot;
	&quot;&quot;&quot;
	d = 2 # dimension of samples
	p = 0.95 # probability
	num_samples = 1000

	mu = np.array(0,0) # dimension (2,)

	# define covar matrices of dim (2,2)
	cov_mats[0] = np.array([[1,0],
							[0,1]])
	cov_mats[1] = np.array([[2,0],
							[0,2]])
	cov_mats[2] = np.array([[0.25, 0.3]
							[0.3, 1]])
	cov_mats[3] = np.array([[10., 5]
							[5., 5]])

	for covar_mat in cov_mats:


		# generate and plot 1000 samples
		generate_gaussian_samples()
		plt.plot()

		# plot the error ellipse
		r = np.sqrt(ellipse_const)
		n_pts = int((2*np.pi) / 0.01)+1
		theta = np.linspace(0,2*np.pi,n_pts)
		w1 = r * np.cos(theta)
		w2 = r * np.sin(theta)
		w = np.array([w1,w2]).reshape(2,1)

		# transferred back to x coordinates
		x = scipy.linalg.sqrtm(sigma).dot(w) + mu
		plt.plot()


def unit_test1():
	&quot;&quot;&quot;
	&quot;&quot;&quot;

	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2))
	# plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.eye(2)*2.)
	# plt.show()

	fig()

	plot_gauss_ellipse(mu=np.array([10., 10.]), cov=np.array([[5,0],[0,3]]) )
	plt.show()




if __name__ == '__main__':
	unit_test1()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">error ellipses, uncertainty</summary></entry><entry><title type="html">Conjugate Gradients</title><link href="http://localhost:4000/conjugate-gradients/" rel="alternate" type="text/html" title="Conjugate Gradients" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/conjugate-gradients</id><content type="html" xml:base="http://localhost:4000/conjugate-gradients/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conjugate-gradients&quot;&gt;Conjugate Gradients&lt;/h2&gt;</content><author><name></name></author><summary type="html">Krylov subspaces ...</summary></entry><entry><title type="html">Simultaneous Localization and Mapping (SLAM)</title><link href="http://localhost:4000/slam/" rel="alternate" type="text/html" title="Simultaneous Localization and Mapping (SLAM)" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/slam</id><content type="html" xml:base="http://localhost:4000/slam/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;slam&quot;&gt;SLAM&lt;/h2&gt;

&lt;h2 id=&quot;orbslam&quot;&gt;ORBSlam&lt;/h2&gt;

&lt;h2 id=&quot;graphslam&quot;&gt;GraphSLAM&lt;/h2&gt;

&lt;p&gt;SLAM methods [2, 10, 13, 14, 21, 30]&lt;/p&gt;

&lt;p&gt;M. Bosse, P. Newman, J. Leonard, M. Soika, W. Feiten, and S. Teller. Simultaneous localization and map building in large-scale cyclic envi- ronments using the atlas framework. IJRR, 23(12), 2004.&lt;/p&gt;

&lt;p&gt;T. Duckett, S. Marsland, and J. Shapiro. Learning globally consistent
maps by relaxation. ICRA 2000.&lt;/p&gt;

&lt;p&gt;J. Folkesson and H. I. Christensen. Robust SLAM. ISAV 2004.
[14] U. Frese, P. Larsson, and T. Duckett. A multigrid algorithm for
simultaneous localization and mapping. IEEE Transactions on Robotics,
2005.&lt;/p&gt;

&lt;p&gt;K. Konolige. Large-scale map-making. AAAI, 2004.&lt;/p&gt;

&lt;p&gt;S. Thrun and M. Montemerlo. The GraphSLAM algorithm with
applications to large-scale mapping of urban structures. IJRR, 25(5/6),
2005.&lt;/p&gt;

&lt;p&gt;. Folkesson and H. I. Christensen. Robust SLAM. ISAV 2004.&lt;/p&gt;</content><author><name></name></author><summary type="html">GraphSLAM, loop closures</summary></entry><entry><title type="html">Structure From Motion</title><link href="http://localhost:4000/sfm/" rel="alternate" type="text/html" title="Structure From Motion" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/sfm</id><content type="html" xml:base="http://localhost:4000/sfm/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-basic-structure-from-motion-sfm-pipeline&quot;&gt;A Basic Structure-from-Motion (SFM) Pipeline&lt;/h2&gt;

&lt;p&gt;As described in [1], there are generally four steps to the algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) Find interest points in each image&lt;/li&gt;
  &lt;li&gt;(2) Find candidate correspondences (match descriptors for each interest point)&lt;/li&gt;
  &lt;li&gt;(3) Perform geometric verification of correspondences (RANSAC + fundamental matrix)&lt;/li&gt;
  &lt;li&gt;(4) Solve for 3D points and camera that minimize reprojection error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;costfunctions&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cost-functionserror-modelling&quot;&gt;Cost Functions/Error Modelling&lt;/h2&gt;

&lt;p&gt;The choice of cost function quantifies the total prediction error of the model. It measures how well the model fits the observations and background knowledge.&lt;/p&gt;

&lt;h3 id=&quot;reprojection-error-for-a-single-keypoint-in-two-images&quot;&gt;Reprojection Error for a Single Keypoint in Two Images&lt;/h3&gt;

&lt;p&gt;Imagine we have matched two keypoints, &lt;script type=&quot;math/tex&quot;&gt;x_1,x_2&lt;/script&gt; in two different images &lt;script type=&quot;math/tex&quot;&gt;I_1,I_2&lt;/script&gt; via a SIFT-like feature matching pipeline. We are viewing the projection of the same 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; in both images. We now wish to identify the coordinates describing the location of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If image &lt;script type=&quot;math/tex&quot;&gt;I_1&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt;, and image &lt;script type=&quot;math/tex&quot;&gt;I_2&lt;/script&gt; was captured with projection matrix &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt;, we can enforce that the 3D point &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is projected into the image into the right location in both images. This is called &lt;em&gt;triangulation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If the camera calibration determining &lt;script type=&quot;math/tex&quot;&gt;M_1,M_2&lt;/script&gt; are &lt;strong&gt;known&lt;/strong&gt;, then the optimization problem for a single matched keypoint in two images becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}} f(\mathbf{X}) = \| x_1 - Proj(\mathbf{X},M_1)\|^2 + \| x_2 - Proj(\mathbf{X},M_2)\|^2&lt;/script&gt;

&lt;h3 id=&quot;reprojection-error-for-many-keypoints-in-many-cameras&quot;&gt;Reprojection Error for Many Keypoints in Many Cameras&lt;/h3&gt;

&lt;p&gt;Imagine now that we have &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; different cameras, and each camera has a &lt;strong&gt;known&lt;/strong&gt; projection matrix &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt;. Suppose we match &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; different keypoints across the images, denoted &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{X}_j\}_{j=1}^n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The optimization problem becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;In this case &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is the observed location of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;‘th keypoint into the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th image, as discovered by a keypoint detector in the SIFT-like pipeline. We penalize solutions for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_1,\mathbf{X}_2,\dots&lt;/script&gt; in which &lt;script type=&quot;math/tex&quot;&gt;x_{ij}&lt;/script&gt; is not very close to &lt;script type=&quot;math/tex&quot;&gt;Proj(\mathbf{X_j},M_i)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;bundleadjustment&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bundle-adjustment&quot;&gt;Bundle Adjustment&lt;/h2&gt;

&lt;p&gt;Imagine now that we are working with arbitrary images for which we have no calibration information. Thus, the projection matrices &lt;script type=&quot;math/tex&quot;&gt;\{M_i\}_{i=1}^m&lt;/script&gt; are &lt;strong&gt;unknown&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Bundle adjustment is the process of minimizing reprojection error over (1) multiple 3D points and (2) multiple cameras. Triggs &lt;em&gt;et al.&lt;/em&gt; define it as &lt;em&gt;“the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates”&lt;/em&gt; [2]. The optimization problem changes only by adding new, additional variables for which we solve.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{X}_1,\mathbf{X}_2, \dots, M_1,M_2,\dots} \sum\limits_{i=1}^m \sum\limits_{j=1}^n \| x_{ij} - Proj(\mathbf{X_j},M_i)\|^2&lt;/script&gt;

&lt;p&gt;According to [2], the name “Bundle Adjustment” refers to &lt;em&gt;bundles&lt;/em&gt; of light rays leaving each 3D point and converging on each camera center, &lt;em&gt;“which are ‘adjusted’ optimally with respect to both feature and camera positions”&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;gauss-newton&quot;&gt;Gauss-Newton&lt;/h2&gt;

&lt;p&gt;Gauss-Newton is generally preferred to Second Order methods like Newton’s Method for a simple reason: deriving and implementing calculations of the second derivatives of the projection model &lt;script type=&quot;math/tex&quot;&gt;Proj(X_j,M_i)&lt;/script&gt; is difficult and error-prone [2].&lt;/p&gt;

&lt;p&gt;For example, the seminal work in SfM, “Building Rome in a Day” [3] uses Trust-Region Gauss-Newton optimization (Levenberg-Marquardt) that chooses between a truncated and an exact step Levenberg-Marquardt algorithm.&lt;/p&gt;

&lt;!-- ## Network Graph  shows which features are seen in which images, --&gt;

&lt;h2 id=&quot;exploiting-sparsity&quot;&gt;Exploiting Sparsity&lt;/h2&gt;

&lt;p&gt;Those who use generic optimization routines to solve SfM problems will find the optimization slow. This would be unwise, however, since the problem sparsity can be exploited.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Ramanan, Deva. &lt;em&gt;Structure from Motion.&lt;/em&gt; &lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&quot;&gt;http://16720.courses.cs.cmu.edu/lec/sfm.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Bill Triggs, Philip McLauchlan, Richard Hartley and Andrew Fitzgibbon. &lt;em&gt;Bundle Adjustment — A Modern Synthesis&lt;/em&gt;. &lt;a href=&quot;https://hal.inria.fr/inria-00548290/document&quot;&gt;https://hal.inria.fr/inria-00548290/document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, Richard Szeliski. &lt;em&gt;Building Rome in a Day&lt;/em&gt;. Communications of the ACM,Volume 54 Issue 10, October 2011. Pages 105-112. &lt;a href=&quot;https://grail.cs.washington.edu/rome/rome_paper.pdf&quot;&gt;https://grail.cs.washington.edu/rome/rome_paper.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Deriving bundle adjustment</summary></entry><entry><title type="html">Particle Filter</title><link href="http://localhost:4000/particle-filter/" rel="alternate" type="text/html" title="Particle Filter" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/particle-filter</id><content type="html" xml:base="http://localhost:4000/particle-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle Filter&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Particle Filter&lt;/strong&gt; is a filtering algorithm that, unlike the Kalman Filter or EKF, can represent multi-modal distributions. This is because it contains no assumptions about the form of the state distribution. It was published in 1995 [2,3] by Simon Julier, Jeffrey Uhlmann, and Hugh Durrant-Whyte at Oxford. It is often called the &lt;strong&gt;“Unscented Kalman Filter” (UKF)&lt;/strong&gt; because the inventors thought “it didn’t stink” like the EKF.&lt;/p&gt;

&lt;p&gt;The main idea is to Represent a distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; with a collection of samples (particles) from &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;x^i \sim p(x), i=1,\dots, N&lt;/script&gt; i.i.d. We know that empirical moments are related to true distribution by the Law of Large Numbers. The algorithm is simple, but expensive to compute with a for loop.&lt;/p&gt;

&lt;h2 id=&quot;whats-wrong-with-the-ekf&quot;&gt;What’s Wrong With the EKF?&lt;/h2&gt;

&lt;p&gt;The Particle Filter addresses a number of problems with the EKF:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 1&lt;/strong&gt; The initial conditions (I.C.s)!
&lt;em&gt;If your initial guess is wrong, the Kalman filter will tell you exactly the wrong thing. The linearization can be vastly different at different parts of the state space&lt;/em&gt;. For example, the EKF could diverge if the residual $| \mu_{0 \mid 0} - x |$ on the initial condition is large. In fact, if $ | \mu_{t \mid t} - x_t |$ large at any time, then the EKF could diverge. This has to do with severity of non-linearity. This is the most commonly found problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 2: Covariance&lt;/strong&gt;
&lt;em&gt;In the EKF, the “covariance matrix” does not literally represent the covariance.&lt;/em&gt; Unfortunately, the covariance matrix only literally captures the covariance in the Kalman Filter. In the EKF, it is just a matrix! We don’t know what it means! If we treat it as confidence, then it is reasonable enough. And commonly true, as long as $\mu$ is tracking $x$ pretty well. However, this estimate tends to be overconfident since we are not incorporating linearization errors! Instead, &lt;script type=&quot;math/tex&quot;&gt;\Sigma_{t \mid t}&lt;/script&gt; incorporates only the noise errors &lt;script type=&quot;math/tex&quot;&gt;Q_t, R_t&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;\Sigma_{t \mid t}&lt;/script&gt;  tends to be smaller than the true covariance matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem No. 3: No Canary in the Goldmine&lt;/strong&gt;
An additional problem with the EKF is that we have no signal to know if we’re going awry.&lt;/p&gt;

&lt;h2 id=&quot;a-different-parameterization-particles&quot;&gt;A Different Parameterization: Particles&lt;/h2&gt;

&lt;p&gt;The UKF represents a different type of compromise than the EKF. In the Kalman Filter and its Extended variant, &lt;script type=&quot;math/tex&quot;&gt;\mu, \Sigma&lt;/script&gt; are the parameters that define the distribution &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu, \Sigma)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The UKF parameterizes the state distribution in a different way by using “Sigma points.” Consequently, the parameterization is called the (&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;-points) parameterization. Thus, we move from &lt;script type=&quot;math/tex&quot;&gt;(\mu, \Sigma)&lt;/script&gt; to a set of points with a weight associated with each, e.g. &lt;script type=&quot;math/tex&quot;&gt;\{ (x^0,w^0), \cdots, (x^{2n}, w^{2n}) \}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The “Unscented Transform” is a curve-fitting exercise that converts individual points to a mean and covariance, i.e. &lt;script type=&quot;math/tex&quot;&gt;UT(\mu, \Sigma) = \{ (x^i, w^i) \}_i&lt;/script&gt;. An advantage of the UKF is that it is very easy to propagate these individual points through nonlinearities like non-linear dynamics, whereas it is harder to push &lt;script type=&quot;math/tex&quot;&gt;\mu, \Sigma&lt;/script&gt; through the nonlinearities.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Properties that we want the unscented transform to have&lt;/em&gt;
\item UT(&lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;(\mu, \Sigma) = \{ (x^i, w^i) \}_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;UT^{-1}(\cdot)&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{ (x^i, w^i) \}_i = (\mu, \Sigma)&lt;/script&gt;
We want the sample sigma points to share the same mean, covariance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu = \sum\limits_{i=0}^{2n} w^ix^i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma = \sum\limits_{i=0}^{2n} w^i (x^i - \mu)(x^i - \mu)^T&lt;/script&gt;

&lt;p&gt;They are redundant (an overparameterization of the Gaussian)&lt;/p&gt;

&lt;p&gt;We have redundancy for a smoothing effect, since won’t be for a perfect Gaussian&lt;/p&gt;

&lt;p&gt;Here is the transform &lt;script type=&quot;math/tex&quot;&gt;UT(\cdot)&lt;/script&gt;:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x^0 = \mu&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x^i = \mu + ( \sqrt{(n+\lambda) \Sigma } )_i, i = 1, \dots, n&lt;/script&gt;\&lt;/p&gt;

&lt;p&gt;This is a matrix square root&lt;/p&gt;

&lt;p&gt;The index $i$ is the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th column in the matrix square root&lt;/p&gt;

&lt;p&gt;We also have a mirror image set:&lt;br /&gt;
$x^i = \mu - ( \sqrt{(n+\lambda) \Sigma } )_{i-n}, i =n+ 1, \dots, 2n$\&lt;/p&gt;

&lt;p&gt;Those were the points. The weights themselves:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^0 = \frac{\lambda}{n+\lambda}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^i = \frac{1}{2(n+\lambda)}, i \geq 1&lt;/script&gt;

&lt;p&gt;Each of these points plot points around the circle/ellipse
Break ellipse into major and minor axes. $x_1, x_2, x_3, x_4$ at the corners of the principal axes and the parameters.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix is &lt;script type=&quot;math/tex&quot;&gt;\sqrt{(n+\lambda) \Sigma}&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;verify-inverse-unscented-transform&quot;&gt;Verify Inverse Unscented Transform**&lt;/h2&gt;

&lt;p&gt;We verify &lt;script type=&quot;math/tex&quot;&gt;UT^{-1}(\cdot)&lt;/script&gt; as claimed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=0}^{2n} w^ix^i = \frac{\lambda}{n+\lambda}(\mu) + \sum\limits_{i=1}^n  \frac{1}{2(n+\lambda)} \Bigg(\mu + ( \sqrt{(n+\lambda) \Sigma } )_i \Bigg) + \sum\limits_{i=n+1}^{2n} \frac{1}{2(n+\lambda)} \Bigg(\mu +  - ( \sqrt{(n+\lambda) \Sigma })_{i-n} \Bigg)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{\lambda}{n+\lambda}(\mu) +  \frac{n}{2(n+\lambda)} \Bigg(\mu  \Bigg) +  \frac{n}{2(n+\lambda)} \Bigg(\mu \Bigg) - \mu&lt;/script&gt;

&lt;p&gt;Everything also cancels in the $\Sigma$ calculation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n \frac{ (\sqrt{n_\lambda})(\sqrt{n_\lambda})}{ (n+\lambda)} (\sqrt{\Sigma}_i) (\sqrt{\Sigma})_i^T = \Sigma&lt;/script&gt;

&lt;p&gt;We notice that if $AA^T = B$, then we can decompose $A$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A = \begin{bmatrix}
| &amp; \cdots &amp; | \\
a_1 \cdots a_n \\
| &amp; \cdots &amp; | 
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;then &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
= \begin{bmatrix}
| &amp; \cdots &amp; | \\
a_1 &amp; \cdots  &amp; a_n \\
| &amp; \cdots &amp; | 
\end{bmatrix}
\begin{bmatrix}
-- &amp; a_1^T &amp; -- \\
-- &amp; \cdots &amp; -- \\
-- &amp; a_n^T &amp; -- | 
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Inverting the transform is just as simple
We get &lt;script type=&quot;math/tex&quot;&gt;a_1 a_1^T + a_2 a_2^T + \cdots + a_n a_n^T&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n a_i a_i^T = AA^T = B&lt;/script&gt;

&lt;p&gt;therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n (\sqrt{\Sigma})_i (\sqrt{\Sigma})_i^T = (\sqrt{\Sigma}) (\sqrt{\Sigma})^T = \Sigma&lt;/script&gt;

&lt;h2 id=&quot;matrix-square-roots&quot;&gt;Matrix Square Roots&lt;/h2&gt;

&lt;p&gt;You might prefer the Cholesky Factorization for numerical versions
Matrix Square Root! Can use SVD or Cholesky Factorization
There are SVD matrix square roots:
(i) &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\Sigma} = U \Lambda^{1/2}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\Sigma} (\sqrt{\Sigma})^T = U \Lambda^{1/2} (U \Lambda^{1/2})^T = \Sigma&lt;/script&gt;

&lt;p&gt;Columns of $U$ matrix are principal directions of ellipse. SVD gives you this geometric intution of the points around the semi axes, etc.&lt;/p&gt;

&lt;p&gt;(ii)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\Sigma} = U \Lambda^{1/2} U^T = \Sigma&lt;/script&gt;

&lt;p&gt;SVD computation takes $O(4n^3)$ time&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cholesky Decomposition&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;M = LU&lt;/script&gt;, where lower triangular times upper triangular
When &lt;script type=&quot;math/tex&quot;&gt;M=M^T&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;L = U^T&lt;/script&gt;
Let &lt;script type=&quot;math/tex&quot;&gt;M=LL^T&lt;/script&gt;
(iii) &lt;script type=&quot;math/tex&quot;&gt;L = \sqrt{\Sigma}&lt;/script&gt;
People prefer cholesky in UKF because it has complexity &lt;script type=&quot;math/tex&quot;&gt;O( \frac{1}{6} n^3)&lt;/script&gt;, just a constant savings.&lt;/p&gt;

&lt;p&gt;We choose &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\Sigma} \sqrt{\Sigma}^T = LL^T = \Sigma&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Zero out successive row elements of your vector, it is not along semi axes, but more along different axis-aligned directions as you zero  out rows&lt;/p&gt;

&lt;h2 id=&quot;ukf-sigma-point-filter&quot;&gt;UKF (Sigma Point Filter)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\mu, \Sigma) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow predict \rightarrow (\bar{x}^i, \bar{w}^i) \rightarrow UT^{-1}(\cdot) \rightarrow  (\bar{\mu}, \bar{\Sigma}) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow update \rightarrow&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Predict Step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;UT(\cdot)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
x_{t \mid t}^0 = \mu_{t \mid t} \\
x_{t \mid t}^i = \mu_{t \mid t} + (\sqrt{(n+\lambda) \Sigma_{t \mid t}})_i, i = 1, \dots, n \\
x_{t \mid t}^i = \mu_{t \mid t} - (\cdots), i = n+1, \dots, 2n \\
\bar{x}_{t +1 \mid t}^i = f(x_{t \mid t}^i, u_t)
\end{aligned}&lt;/script&gt;

&lt;p&gt;We predict through nonlinear dynamics&lt;/p&gt;

&lt;p&gt;Now we run &lt;script type=&quot;math/tex&quot;&gt;UT^{-1}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\mu_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i \bar{x}_{t+1 \mid t}^i \\
\Sigma_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i (\bar{x}_{t+1 \mid t}^i - \mu_{t+1 \mid t})(\bar{x}_{t+1 \mid t}^i - \mu_{t+1 \mid t})^T
\end{aligned}&lt;/script&gt;

&lt;p&gt;We recall the Gaussian estimate!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\mu_{t \mid t} = \mu + \Sigma_{XY} \Sigma_{YY}^{-1} (y - \hat{y}) \\
\Sigma_{t \mid t} = \Sigma - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}
\end{aligned}&lt;/script&gt;

&lt;p&gt;\item Now the UPDATE step:&lt;br /&gt;
We run &lt;script type=&quot;math/tex&quot;&gt;UT(\cdot)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_{t +1 \mid t}^0&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_{t+1 \mid t}^{2n}&lt;/script&gt;\&lt;/p&gt;

&lt;p&gt;Let’s build $\hat{y}&lt;em&gt;{t+1 \mid t}$ and $\Sigma&lt;/em&gt;{t+1 \mid t}^{XY}$, $\Sigma_{t+1 \mid t}^{YY}$
\item Now&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i \hat{y}_{t+1 \mid t}^i&lt;/script&gt;

&lt;p&gt;which is the expected measurment
Now,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\Sigma_{t+1 \mid t}^{YY} = \sum\limits_{i=0}^{2n} w^i (\hat{y}_{t+1 \mid t}^i - \hat{y}_{t+1 \mid t}) (\hat{y}_{t+1 \mid t}^i - \hat{y}_{t+1 \mid t})^T
\end{aligned}&lt;/script&gt;

&lt;p&gt;Now&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\Sigma_{t+1 \mid t}^{XY} = \sum\limits_{i=0}^{2n} w^i (x_{t+1 \mid t}^i - \mu_{t+1 \mid t}) (\hat{y}&lt;em&gt;{t+1 \mid t}^i - \hat{y}&lt;/em&gt;{t+1 \mid t})^T
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;We are doing a fitting operation. That is why we have more sigma points than we need. Smooth out the anomalies due to any one point getting weird.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{t+1 \mid t+1} = \Sigma_{t+1 \mid t} - \Sigma_{t+1 \mid t}^{XY} \Sigma_{t+1 \mid t}^{YY}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mu_{t+1 \mid t+1 } = \mu_{t+1 \mid t } + \Sigma_{t+1 \mid t}^{XY} (\Sigma_{t+1 \mid t}^{YY})^{-1} (y_{t+1} - \hat{y}&lt;em&gt;{t+1 \mid t})
\end{equation}
where $y&lt;/em&gt;{t+1}$ is the actual measurement.&lt;/p&gt;

&lt;h2 id=&quot;choosing-lambda&quot;&gt;Choosing Lambda&lt;/h2&gt;

&lt;p&gt;What is &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;? Problem specific.  Consider SVD square root, so that sigma points will be along principal axes.&lt;/p&gt;

&lt;p&gt;Suppose we have &lt;script type=&quot;math/tex&quot;&gt;x^0&lt;/script&gt; at the center of the ellipse, and &lt;script type=&quot;math/tex&quot;&gt;x^1, \dots, x^4&lt;/script&gt; lie at each corner of the principal semi-axes&lt;/p&gt;

&lt;p&gt;$$\lambda$ is the “confidence-value” of the error ellipse&lt;/p&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;n+\lambda = 1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;x^i = \mu \pm \sqrt{\Sigma}_i&lt;/script&gt; and each column represents one standard deviation&lt;/p&gt;

&lt;p&gt;Standard deviation in each direction. Bigger &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is, then the bigger is the ellipse (And vice versa: smaller &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; gives smaller ellipse)&lt;/p&gt;

&lt;p&gt;The size of the ellipse matters because this is what we take as the region about which we create our linearization&lt;/p&gt;

&lt;p&gt;UKF is a linearization, takes average slope over a neighborhood. But it is not from the Taylor Series Expansion&lt;/p&gt;

&lt;p&gt;Why does size of ellipse matter? Blurring over a bigger neighborhood. (neighborhood about which we fit the Gaussian is determined by  &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The smaller &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is, the closer it will be to an EKF, which fits about a single-point (linearizing it there)&lt;/p&gt;

&lt;p&gt;Interesting value of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lambda=2&lt;/script&gt;. For a quadratic non-linearity, then the inverse unscented transform fits the mean and the covariance of the Gaussian, and also the Kurtosis (the 4th moment) of the Gaussian (but only for a quadratic nonlinearity)
\item Fitting the Kurtosis is good! We can do it beacause the extra degrees of freedom of the sigma points overparameterize
\end{itemize}
\subsection{PRO version of UKF}
\begin{itemize}
\item Other Form of UKF:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\lambda = \alpha^2 ( n + k) - n
\end{equation}
this gives us two parameters to tune&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^i = \mu \pm \alpha ( \sqrt{(n+k) \Sigma})_i&lt;/script&gt;

&lt;p&gt;where the two parameters are &lt;script type=&quot;math/tex&quot;&gt;\alpha,k&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We now have to redefine the weights to be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_c^0 = \frac{\lambda}{n+\lambda} + (1 - \alpha^2 + \beta)&lt;/script&gt;

&lt;p&gt;where $\beta$ is another parameter&lt;/p&gt;

&lt;p&gt;Hugh Durrant White, the original paper has this original form&lt;/p&gt;

&lt;p&gt;How does it work?&lt;/p&gt;

&lt;p&gt;Algorithm is simple, but expensive to compute (with for loop)&lt;/p&gt;

&lt;p&gt;In UKF, samples deterministally extracted&lt;/p&gt;

&lt;p&gt;In Particle filter, no assumption about form of distribution, but need many form of them, and probabilistically extracted&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Idea:&lt;/strong&gt; Represent a distribution $p(x)$ with a collection of samples from &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;x^i \sim p(x), i=1,\dots, N&lt;/script&gt; i.i.d.&lt;/p&gt;

&lt;p&gt;What does it mean to ``represent’’ a distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; with a bunch of particles &lt;script type=&quot;math/tex&quot;&gt;\{ x_1, \dots, x_N \}&lt;/script&gt;?&lt;/p&gt;

&lt;p&gt;We know that empirical moments are related to true distribution by the law of large number&lt;/p&gt;

&lt;p&gt;Recall&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{lll}
\mu = \mathbbm{E}[X] = \int_x p(x) dx \approx \frac{1}{N} \sum\limits_{i=1}^N x_i = \bar{\mu}, &amp; x_i \sim p(x), &amp; i=1,\dots, N, i.i.d
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
\Sigma = \mathbbm{E}[(X-\mu)(X-\mu)^T] = \dots \approx \frac{1}{N} \sum\limits_{i=1}^N (x_i - \bar{\mu})(x_i - \bar{\mu})^T, &amp; x_i \sim p(x)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Also,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbbm{E}[f(x)] \approx \frac{1}{N} \sum\limits_{i=1}^N f(x_i)&lt;/script&gt;

&lt;p&gt;The Law of Large numbers states that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{N} \sum\limits_{i=1}^N f(x_i) \rightarrow \mathbbm{E}[f(X)]&lt;/script&gt;

&lt;p&gt;as &lt;script type=&quot;math/tex&quot;&gt;N \rightarrow \infty&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Problem: Given a set of particles&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ x_{t+1 \mid t}^1, \dots, x_{t+1  \mid t}^N \} \sim p(x_{t+1} \mid y_{1:t} )&lt;/script&gt;

&lt;p&gt;drawn from the above distribution&lt;/p&gt;

&lt;p&gt;We recall the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t} ) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) dx_t }&lt;/script&gt;

&lt;p&gt;We have particles from the top right expression, &lt;script type=&quot;math/tex&quot;&gt;x_{t \mid t-1}^i \sim p( x_t \mid y_{1:t-1})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We want to use Bayes Rule so that we can transform our particles so that they approximate the posterior (this will be one step of the Bayesian Filter)&lt;/p&gt;

&lt;p&gt;We will use :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;q(x)&lt;/script&gt; the proposal distribution, the particles are actually from here&lt;/p&gt;

&lt;p&gt;We want them to be from $p(x)$ the target distribution, we wish they were from here&lt;/p&gt;

&lt;p&gt;We use Particle Weights&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbbm{E}_p [ f(X)] = \int_x f(x) p(x) dx = \int_x f(x) p(x) \frac{q(x)}{q(x)} dx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbbm{E}_p [ f(X)] = \int_x f(x) w(x) q(x) dx  = \mathbbm{E}_q [ f(X) w(x) ]&lt;/script&gt;

&lt;p&gt;Given ${x^1, \dots, x^N }$
New set: ${ (x^1,w^1), \dots, (x^N,w^N) }$ where $w^i = w(x^i)$
Now the expectation is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbbm{E}_p[f(X)] \approx \frac{1}{N} \sum\limits_{i=1}^N f(x^i) w^i&lt;/script&gt;

&lt;p&gt;and &lt;script type=&quot;math/tex&quot;&gt;w(x) = \frac{p(x)}{q(x)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If we knew &lt;script type=&quot;math/tex&quot;&gt;p,q&lt;/script&gt;, then we would know the weights
But in the filtering setup, we don’t know the posterior, the distribution we are trying to hit! But in fact, we don’t need the target distribution!&lt;/p&gt;

&lt;p&gt;Proposal: What we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(x) = p(x_t \mid y_{1:t-1})&lt;/script&gt;

&lt;p&gt;Target: What we want&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = p(x_t \mid y_{1:t})&lt;/script&gt;

&lt;p&gt;So we get weights:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^i = w(x^i) = \frac{ p(x^i) }{ q(x^i) } = \frac{p(x_t^i \mid y_{1:t})^i}{p(x_t^i \mid y_{1:t-1}) }&lt;/script&gt;

&lt;p&gt;Now, by Bayes Rule, we can say that the PRIOR cancels out, which was the only thing we didn’t know???&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{     \frac{p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} \cdots dx_t }     }{ p(x_t \mid y_{1:t-1}) }&lt;/script&gt;

&lt;p&gt;When the prior cancels, we get&lt;/p&gt;

&lt;p&gt;\begin{equation}
w(x_t^i) = \frac{p(y_t \mid x_t^i ) }{ \int_{x_t} \cdots dx_t } 
\end{equation}&lt;/p&gt;

&lt;p&gt;We just care about the relative weights, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w} (x_t^i) = p(y_t \mid x_t^i)&lt;/script&gt;

&lt;p&gt;(unnormalized)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(x_t^i) = \frac{ \bar{w}(x_t^i) }{  \sum\limits_{i=1}^N \bar{w}(x_t^i) }&lt;/script&gt;

&lt;p&gt;Example: suppose we have measurement noise $v_t \sim \mathcal{N}(0, R_t)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t = g(x_t) + v_t&lt;/script&gt;

&lt;p&gt;Given $x_{t \mid t-1}^i \sim p(x_t \mid y_{1:t-1})$, then find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_{t \mid t}^i, w_{t \mid t}^i \}_i&lt;/script&gt;

&lt;p&gt;to represent $p(x_t \mid y_{1:t})$. The weights are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_{t \mid t-1}^i) \sim \mathcal{N}( g(x_{t \mid t-1}^i, R_t)&lt;/script&gt;

&lt;p&gt;And now&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = \eta \mbox{exp } \{ -\frac{1}{2} (y_t - g(x_{t \mid t-1}^i)^T R_t^{-1} (y_t - g(x_{t \mid t-1}^i) ) \}&lt;/script&gt;

&lt;p&gt;Renormalize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(x_t^i) = \frac{ \bar{w}(x_t^i) }{ \sum\limits_{i=1}^N \bar{w}(x_t^i)  }&lt;/script&gt;

&lt;p&gt;Then multiply the weights, and renormalize (could have started with weighted particles instead of just particles)&lt;/p&gt;

&lt;p&gt;Only the weight changes in the UPDATE step (but we keep two different time indices for weights). Time indices are redundant for the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; particles, which are the input&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_t^i, w_{t \mid t-1}^i) \}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_t^i) w_{t \mid t-1}^i&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t \mid t}^i  = \frac{ \bar{w}_{t \mid t}^i }{ \sum\limits_{i=1}^N \bar{w}_{t \mid t}^i  }&lt;/script&gt;

&lt;h2 id=&quot;predict-step&quot;&gt;Predict Step&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1} \mid y_{1:t}) = \int_x p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t&lt;/script&gt;

&lt;p&gt;the left side is the transition distribution (which we have)&lt;/p&gt;

&lt;p&gt;the right side is ${ (x_t^i, w_{t \mid t}^i) }$&lt;/p&gt;

&lt;p&gt;Let’s just “simulate” &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+1} \mid x_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Draw &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_{t+1} = f(x_t, u_t) + w_t&lt;/script&gt;, the process noise &lt;script type=&quot;math/tex&quot;&gt;w_t \sim \mathcal{N}(0, Q_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We sample 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{array}{ll}
w_t \sim \mathcal{N}(0, Q_t), &amp; \rightarrow x_{t+1}^i = f(x_{t}^i, u_t) + w_t 
\end{array} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;1000 particles per dimension &lt;script type=&quot;math/tex&quot;&gt;(1000^{D}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;=dimension&lt;/p&gt;

&lt;p&gt;Exponential explosion of particles required to keep the same resolution&lt;/p&gt;

&lt;p&gt;Full Filter: Given&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ (x_{t }^i, w_{t \mid t}^i \}_i&lt;/script&gt;

&lt;p&gt;Predict:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )&lt;/script&gt;

&lt;p&gt;Update:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}_{t+1 \mid t+1}^i = p(y_{t+1} \mid x_{t+1} = x_{t+1}^i) w_{t \mid t}^i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1 \mid t+1 }^i  = \frac{ \bar{w}_{t+1 \mid t+1}^i }{ \sum\limits_{i=1}^N \bar{w}_{t+1 \mid t+1}^i  }&lt;/script&gt;

&lt;h2 id=&quot;sample-degeneracy&quot;&gt;Sample Degeneracy&lt;/h2&gt;

&lt;p&gt;Unfortunately, it is easy to end up with degenerate samples in the UKF, meaning the samples won’t do anything for you because they are all spread apart. TODO: Only one takes the weight, why?&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;solution&lt;/strong&gt;: resample points in a very clever way, via importance (re-)sampling. In a &lt;em&gt;survival of the fittest&lt;/em&gt;, those samples with high weight end up lots of children, and those with low weight disappear. I&lt;/p&gt;

&lt;p&gt;\item We sample a new set of particles at time $t$, get new $x_t^i$ s.t.
\item This is sequential important re-sampling.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr( x_t^i = x_t^j) = w_t^j&lt;/script&gt;

&lt;p&gt;The noisy prediction will disperse them. The new weights are uniform, i.e. &lt;script type=&quot;math/tex&quot;&gt;w_t^i = \frac{1}{N}&lt;/script&gt;
\item Particle impoverishment; all particles clumped at one point because not enough noise to disperse them
\item If the resampling happens too frequently, in comparison with the dispersal via noise, then we get sample impoverishment!&lt;/p&gt;

&lt;p&gt;We will get that all $x_t^i \approx x_t^j$, and $w_t^i \approx \frac{1}{N}, \forall i$&lt;/p&gt;

&lt;p&gt;So no diversity in the distribution anymore&lt;/p&gt;

&lt;p&gt;To fix it, resample less often&lt;/p&gt;

&lt;p&gt;Or you could just throw some random particles into your bag&lt;/p&gt;

&lt;p&gt;Check how different weights are at every time. If too different, trigger a resample (variance without subtracting mean is &lt;script type=&quot;math/tex&quot;&gt;\sum\limits_{i=1}^N (w_t^i)^2 \geq \mbox{thresh}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If maximally different, then you would get &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, maximally different&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low Variance Resampling:&lt;/strong&gt; in previous formulation, you could have gotten all copies of the same particle
One number line is &lt;script type=&quot;math/tex&quot;&gt;r \sim v[0, \frac{1}{N}]&lt;/script&gt;
Then map each of these to the right interval above in the weighted bins. Determinsitic resampling
Make &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; copies of it in every uniform bin&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Swept under Rug&lt;/strong&gt;
All filters have tried to compute the Bayesian posterior (either exactly or approximately)
The particle filter does not do this (look at prediction step)
PF tracks approximates the posterior of the trajectory instead&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{0:t} \mid y_{1:t})&lt;/script&gt;

&lt;p&gt;Because of the prediction step, where we say draw sample&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid x_t  = x_t^i )&lt;/script&gt;

&lt;p&gt;What we really wanted was&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1} \mid y_{1:t} ) = \int_{x_t} p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t&lt;/script&gt;

&lt;p&gt;\item Instead, the Particle Filter finds&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1}^i \sim p(x_{t+1}, x_t \mid y_{1:t}) = p(x_{t+1} \mid x_t = x_t^i) p(x_t = x_t^i \mid y_{1:t})&lt;/script&gt;

&lt;p&gt;Track distribution of the whole TRAJECTORY, given all of the measurements.
Important for SLAM, because in SLAM we often want to estimate the history of the trajectory of the robot
\end{itemize}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.&lt;/p&gt;

&lt;p&gt;[2] SJ Julier, JK Uhlmann, HF Durrant-Whyte. &lt;em&gt;A new approach for filtering nonlinear systems&lt;/em&gt;. Proceedings of the American Control Conference, June 1995, Volume 3, pages 1628-1632.&lt;/p&gt;

&lt;p&gt;[3] Simon Julier, Jeffrey Uhlmann, and Hugh F. Durrant-Whyte. &lt;em&gt;A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators&lt;/em&gt;. IEEE Transactions on Automatic Control, Volume 45, No. 3. March 2000, page 477.&lt;/p&gt;</content><author><name></name></author><summary type="html">multi-modal distributions, sigma point transform, matrix square roots ...</summary></entry><entry><title type="html">Robot Localization</title><link href="http://localhost:4000/robot-localization/" rel="alternate" type="text/html" title="Robot Localization" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/localization</id><content type="html" xml:base="http://localhost:4000/robot-localization/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;localization&quot;&gt;Localization&lt;/h2&gt;

&lt;p&gt;Navigation in urban environments requires clever solutions because sensors like GPS can’t provide centimeter-level accuracy.  Precise localization requires fusing signals from sensors like GPS, IMU, wheel odometry, and LIDAR data in concert with pre-built maps [1].&lt;/p&gt;

&lt;p&gt;In order for an autonomous robot to stay in a specific lane, it needs to know where the lane is. For an autonomous robot to stay in a lane, the localization requirements are in the order of decimeters [1].&lt;/p&gt;

&lt;p&gt;I’ll review some methods from 2007-2018.&lt;/p&gt;

&lt;h2 id=&quot;building-a-map&quot;&gt;Building a Map&lt;/h2&gt;

&lt;p&gt;The dominant method is to learn a detailed map of the environment, and then to use a vehicle’s LIDAR sensor to localize relative to this map [1]. In [1], a “map” was a 2-D overhead view of the road surface, taken in the infrared spectrum with 5-cm resolution. This 2-D grid assigns to each x-y location in the environment an infrared reflectivity value. Thus, their ground map is a orthographic infrared photograph of the ground. To acquire such a map, multiple laser range finders are mounted on a vehicle, pointing downwards at the road surface. Obtain range + infrared reflectivity. GraphSLAM is employed to map roads.&lt;/p&gt;

&lt;p&gt;To make a map, one has to differentiate between static and dynamic objects. There are two good ways to do this, as described in [1]&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;track or remove objects that move at the time of mapping.&lt;/li&gt;
  &lt;li&gt;reducing the map to features are very likely to be static.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A 2D-world assumption is often not enough. In [2], localization is desired within a multi-level parking garage. They utilize multi-level surface maps to compactly represent such buildings, with a graph-based optimization procedure to establish the consistency of the map. Store surface height and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma_{ij}^{l}&lt;/script&gt; to represent the uncertainty in the height of the surface.&lt;/p&gt;

&lt;p&gt;build an accurate map of the environment offline through aligning multiple
sensor passes over the same area. [9]&lt;/p&gt;

&lt;h2 id=&quot;online-localization-feature-matching-in-the-map&quot;&gt;Online Localization: Feature Matching in the Map&lt;/h2&gt;

&lt;p&gt;In the online stage, sensory input must be matched against the HD-map.&lt;/p&gt;

&lt;p&gt;In [1], the authors correlate via the Pearson product-moment correlation the measured infrared reflectivity with the map. The importance weight of each particle is a function of the correlation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_t^{[k]} = \mbox{exp}\{ -\frac{1}{2} (x_t^{[k]} − y_t)^T \Gamma_t^{−1} (x_t^{[k]} − y_t)\} \cdot \Bigg( \mbox{corr} \Bigg[ \begin{pmatrix} h_1(m,x_t^{[k]}) \\ \vdots \\ h_{180}(m,x_t^{[k]}) \end{pmatrix}, \begin{pmatrix} z_t^1 \\ \vdots \\ z_t^{180} \end{pmatrix} \Bigg] + 1 \Bigg)&lt;/script&gt;

&lt;p&gt;In [2], online localization is performed via the iterative closest points (ICP) algorithm to obtain a maximum likelihood estimate of the robot motion between subsequent observations. Instead of performing ICP on raw 3D point clouds, the ICP is performed on local MLS-maps.&lt;/p&gt;

&lt;h2 id=&quot;points-planes-poles-gaussian-bars-over-2d-grids-feature-representation-method&quot;&gt;Points Planes Poles Gaussian Bars over 2D Grids (Feature representation method)&lt;/h2&gt;

&lt;p&gt;point-to-plane ICP between the raw 3D LiDAR points against the 3D pre-scanned localization prior at 10Hz
particle filter method for correlating LIDAR measurements [1]&lt;/p&gt;

&lt;h2 id=&quot;2d-grids&quot;&gt;2D grids&lt;/h2&gt;

&lt;h2 id=&quot;using-deep-learning&quot;&gt;Using Deep Learning&lt;/h2&gt;

&lt;p&gt;Barsan &lt;em&gt;et al.&lt;/em&gt; are the first to use CNNs on LiDAR orthographic (bird’s-eye-view) images of the ground for the online localization, feature matching step [9].  embeds both LiDAR intensity maps
and online LiDAR sweeps in a common space where calibration is not required. In this scenario, online localization can be performed by searching exhaustively over 3-DoF poses – 2D position on the map manifold plus rotation. The authors score the quality of pose matches by the cross-correlation between the embeddings.&lt;/p&gt;

&lt;p&gt;They use a histogram filter, the discretized Bayes’ filter, to perform the search over 3D space. The three grid dimensions are &lt;script type=&quot;math/tex&quot;&gt;x,y,\theta&lt;/script&gt;, and they compute the belief likelihood for every cell in the search space.&lt;/p&gt;

&lt;p&gt;Barsan &lt;em&gt;et al.&lt;/em&gt; generate ground-truth poses 
Our ground-truth poses are acquired through an expensive high
precision offline matching procedure with up to several centimeter uncertainty. We rasterize the
aggregated LiDAR points to create a LiDAR intensity image. Both the online intensity image and
the intensity map are discretized at a spatial resolution of 5cm covering a 30m×24m region.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] J. Levinson, M. Montemerlo, and S. Thrun. &lt;em&gt;Map-based precision vehicle localization in urban environments&lt;/em&gt;. In Robotics: Science and Systems, volume 4, page 1. Citeseer, 2007.&lt;/p&gt;

&lt;p&gt;[2] R. Kummerle, D. Hahnel, D. Dolgov, S. Thrun, and W. Burgard. &lt;em&gt;Autonomous driving in a multi-level parking structure&lt;/em&gt;. In IEEE International Conference on Robotics and Automation (ICRA), pages 3395–3400, May 2009.&lt;/p&gt;

&lt;p&gt;[3] J. Levinson and S. Thrun. &lt;em&gt;Robust vehicle localization in urban environments using probabilistic maps&lt;/em&gt;. In IEEE International Conference on Robotics and Automation (ICRA), 934 pages 4372–4378, May 2010.&lt;/p&gt;

&lt;p&gt;[4] R. W. Wolcott and R. M. Eustice. &lt;em&gt;Fast LIDAR localization using multiresolution gaussian mixture maps&lt;/em&gt;. In IEEE International Conference on Robotics and Automation (ICRA), pages 2814–2821, May 2015.&lt;/p&gt;

&lt;p&gt;[5] R. W. Wolcott and R. M. Eustice. &lt;em&gt;Robust LIDAR localization using multiresolution gaussian mixture maps for autonomous driving&lt;/em&gt;. The International Journal of Robotics Research, 36(3):292–319, 2017.&lt;/p&gt;

&lt;p&gt;[6] H. Kim, B. Liu, C. Y. Goh, S. Lee, and H. Myung. &lt;em&gt;Robust vehicle localization using entropy-weighted particle filter-based data fusion of vertical and road intensity information for a large scale urban area&lt;/em&gt;. IEEE Robotics and Automation 923 Letters, 2(3):1518–1524, July 2017.&lt;/p&gt;

&lt;p&gt;[7] R. Dub, M. G. Gollub, H. Sommer, I. Gilitschenski, R. Siegwart, C. Cadena, and J. Nieto. &lt;em&gt;Incremental-segment-based localization in 3-D point clouds&lt;/em&gt;. IEEE Robotics and Automation Letters, 3(3):1832–1839, July 2018. 1&lt;/p&gt;

&lt;p&gt;[8] G. Wan, X. Yang, R. Cai, H. Li, Y. Zhou, H. Wang, and S. Song. &lt;em&gt;Robust and precise vehicle localization based on multi-sensor fusion in diverse city scenes&lt;/em&gt;. In IEEE International Conference on Robotics and Automation (ICRA), pages 4670–4677, May 2018.&lt;/p&gt;

&lt;p&gt;[9] Ioan Andrei Barsan, Shenlong Wang, Andrei Pokrovsky, Raquel Urtasun. &lt;em&gt;Learning to Localize Using a LiDAR Intensity Map&lt;/em&gt;. Proceedings of The 2nd Conference on Robot Learning, PMLR 87:605-616, 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html">ICP, grid histograms, ...</summary></entry><entry><title type="html">The Kalman Filter</title><link href="http://localhost:4000/kalman-filter/" rel="alternate" type="text/html" title="The Kalman Filter" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/kalman-filter</id><content type="html" xml:base="http://localhost:4000/kalman-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sfmpipeline&quot;&gt;A Basic SfM Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#costfunctions&quot;&gt;Cost Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bundleadjustment&quot;&gt;Bundle Adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;sfmpipeline&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-kalman-filter&quot;&gt;What is the Kalman Filter?&lt;/h2&gt;

&lt;p&gt;The Kalman Filter is nothing more than the Bayes’ Filter for Gaussian distributed random variables. The Bayes’ Filter is described in a &lt;a href=&quot;/bayes-filter/&quot;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is a very surprising result that we can write out the integrals analytically for the Bayes’ Filter when working with a special family of distributions: Gaussian distributed random variables (r.v.’s).  As we recall from the Bayes’ Filter, we have three quantities that we’ll need to be able to evaluate:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) &lt;script type=&quot;math/tex&quot;&gt;\int_{x_{t-1}} p(x_t \mid x_{t-1}) p(x_{t-1} \mid y_{1:t-1}) dx_{t-1}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(2) &lt;script type=&quot;math/tex&quot;&gt;p(x_t \mid x_{t-1})  = f(x_t, x_{t-1} )&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(3) &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t) = g(y_t, x_t)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Expressions (2) and (3) must be finitely parameterizable. It will take a fair amount of work to derive the analytical formulas of the Bayes’ Filter for Gaussian r.v.’s (the Kalman Filter).  We’ll first review properties of multivariate Gaussians, then the Woodbury matrix inversion lemmas, intuition behind covariance matrices, and then derive the Kalman Filter.&lt;/p&gt;

&lt;h2 id=&quot;woodbury-matrix-inversion-lemmas&quot;&gt;Woodbury Matrix Inversion Lemmas&lt;/h2&gt;

&lt;h2 id=&quot;kalman-filter-derivation&quot;&gt;Kalman Filter Derivation&lt;/h2&gt;

&lt;h2 id=&quot;gaussian-white-noise&quot;&gt;Gaussian White Noise&lt;/h2&gt;

&lt;p&gt;In practice, noise is usually not Gaussian and is not white (usually colored).&lt;/p&gt;

&lt;p&gt;\item Fact: For a Linear-Gaussian dynamical system (one who’s dynamics are expressed with Gaussian White Noise
\begin{equation}
\begin{array}{ll}
x_{t+1} = Ax_t + B u_t + w_t, &amp;amp; w_t \sim \mathcal{N}(0,Q) &lt;br /&gt;
\end{array}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{array}{ll}
y_t = Cx_t + Du_t + v_t, &amp;amp; v_t \sim \mathcal{N}(O, R)
\end{array}
\end{equation}
where $w_t, v_t$ are zero-mean white noise&lt;/p&gt;</content><author><name></name></author><summary type="html">Multivariate Gaussians, ...</summary></entry><entry><title type="html">What is State Estimation? and the Bayes Filter</title><link href="http://localhost:4000/bayes-filter/" rel="alternate" type="text/html" title="What is State Estimation? and the Bayes Filter" /><published>2018-12-27T06:00:00-05:00</published><updated>2018-12-27T06:00:00-05:00</updated><id>http://localhost:4000/bayes-filter</id><content type="html" xml:base="http://localhost:4000/bayes-filter/">&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#state-estimation&quot;&gt;What is State Estimation?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dt-lds&quot;&gt;Discrete-time Linear Dynamical Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#probability-review&quot;&gt;Probability Review: The Chain Rule, Marginalization, &amp;amp; Bayes Rule&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-estimation&quot;&gt;Recursive Bayesian Estimation + Conditional Independence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pgm-bayes-filter&quot;&gt;Graphical Model: The Structure of Variables in the Bayes Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-filter-deriv-predict&quot;&gt;Derivation of the Bayes Filter: Predict Step&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-filter-deriv-update&quot;&gt;Derivation of the Bayes Filter: Update Step&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;state-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-state-estimation&quot;&gt;What is State Estimation?&lt;/h2&gt;

&lt;p&gt;State estimation is the study of reproducing the state of a robot (e.g. its orientation, location) from noisy measurements. Unfortunately, we can’t obtain the state &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; directly. Instead, we can obtain get a measurement that is all tangled up with noise.&lt;/p&gt;

&lt;p&gt;A more formal definition: Given a history of measurements &lt;script type=&quot;math/tex&quot;&gt;y_1, \dots, y_t)&lt;/script&gt;, and system inputs &lt;script type=&quot;math/tex&quot;&gt;(u_1, \dots, u_t)&lt;/script&gt;, find an estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{x}_t&lt;/script&gt; of the state &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with small error &lt;script type=&quot;math/tex&quot;&gt;\|\hat{x}_t - x_t \|&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;What do we have to work with?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) We are given the measurements &lt;script type=&quot;math/tex&quot;&gt;y_t&lt;/script&gt;, whose distribution is modeled  as &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t, u_t)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;(2) We assume that we know the state transition distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+1} \mid x_t, u_t)&lt;/script&gt;, i.e. the robot dynamics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal of state estimation is to find the posterior distribution of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; given all of the prior measurements and inputs:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t}, u_{1:t})&lt;/script&gt;

&lt;h2 id=&quot;why-do-we-need-a-distribution-instead-of-a-single-state-estimate&quot;&gt;Why do we need a distribution instead of a single state estimate?&lt;/h2&gt;

&lt;p&gt;The goal of state estimation is not to obtain a single &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt;. Rather, we desire a distribution over &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;’s. You might ask, &lt;em&gt;why?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The mean of a distribution may not be representative of the distribution whatsoever. For example, consider a bimodal distribution with a mean around 0, but consisting of two camel humps. There is more information in the Bayesian estimate that we can use for control. In the case of a Kalman Filter, we will express the state distribution as a Gaussian, which is parameterized compactly by a mean and covariance.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;dt-lds&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;discrete-time-linear-dynamical-systems-dt-lds&quot;&gt;Discrete-time Linear Dynamical Systems (DT LDS)&lt;/h2&gt;

&lt;p&gt;Filtering and estimation is much more easily described in discrete time than in continuous time. We use Linear Dynamical Systems as a key tool in state estimation.&lt;/p&gt;

&lt;p&gt;Suppose we have a system with state &lt;script type=&quot;math/tex&quot;&gt;x \in R^n&lt;/script&gt;, which changes over timesteps &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; [1,2]. The matrix &lt;script type=&quot;math/tex&quot;&gt;A(t) \in R^{n \times n}&lt;/script&gt; is called the dynamics matrix. Suppose we provide an input &lt;script type=&quot;math/tex&quot;&gt;u \in R^m&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  &lt;script type=&quot;math/tex&quot;&gt;B(t)&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;R^{n \times m}&lt;/script&gt; input matrix. The vector &lt;script type=&quot;math/tex&quot;&gt;c(t) \in R^n&lt;/script&gt; is called the offset. We can express the system as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(t + 1) = A(t)x(t) + B(t)u(t) + c(t)&lt;/script&gt;

&lt;p&gt;We will use the shorthand notation for simplicity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t + 1} = A_t x_t + B_t u_t + c_t&lt;/script&gt;

&lt;p&gt;As Boyd and Vandenberghe point out [2], the LDS is a “special case of a Markov system where the next state is a linear function of the current state.” Suppose we also have an output of the system &lt;script type=&quot;math/tex&quot;&gt;y(t) \in R^{p}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;C(t) \in R^{p \times n}&lt;/script&gt; is the output or sensor matrix. This equation can be modeled as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t) = C(t)x(t) + D(t)u(t)&lt;/script&gt;

&lt;p&gt;or in shorthand, as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t = C_t x_t + D_t u_t&lt;/script&gt;

&lt;p&gt;where  &lt;script type=&quot;math/tex&quot;&gt;t \in Z = \{0, \pm 1, \pm 2, \dots \}&lt;/script&gt; and vector signals &lt;script type=&quot;math/tex&quot;&gt;x,u,y&lt;/script&gt; are sequences. It is not hard to see that the DT LDS is a first-order vector recursion [1].&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;probability-review&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;probability-review-the-chain-rule-marginalization--bayes-rule&quot;&gt;Probability Review: The Chain Rule, Marginalization, &amp;amp; Bayes Rule&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Chain Rule&lt;/strong&gt; states that if &lt;script type=&quot;math/tex&quot;&gt;y_1,...y_t&lt;/script&gt; are events, and &lt;script type=&quot;math/tex&quot;&gt;p(y_i) &gt; 0&lt;/script&gt;, then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_1 \cap y_2 \cap \dots \cap y_t) = p(y_1) p(y_2 \mid y_1)···p(y_t \mid y_1 \cap \dots \cap y_{t-1})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Marginalization&lt;/strong&gt; is a method of variable elimination. If &lt;script type=&quot;math/tex&quot;&gt;X_1,X_2&lt;/script&gt; are independent:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_{x_2} p(x_1,x_2) dx_2 = \int\limits_{x_2} p(x_1)p(x_2)dx = p(x_1) \int\limits_{x_2} p(x_2)dx_2 = p(x_1)&lt;/script&gt;

&lt;p&gt;We recall &lt;strong&gt;Bayes’ Rule&lt;/strong&gt; states that if &lt;script type=&quot;math/tex&quot;&gt;x,y&lt;/script&gt; are events, and &lt;script type=&quot;math/tex&quot;&gt;p(x) &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(y) &gt; 0&lt;/script&gt;, then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y) = \frac{p(y \mid x)p(x)}{p(y)} = \frac{p(y,x)}{p(y)}&lt;/script&gt;

&lt;p&gt;In the domain of state estimation, we can assign the following meaning to these distributions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; is our prior.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y \mid x)&lt;/script&gt; is our measurement likelihood.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; is the normalization term.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y)&lt;/script&gt; is our posterior (what we can’t see, given what we can see).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One problem is that we may not know &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; directly. Fortunately, it turns out &lt;em&gt;we don’t need to&lt;/em&gt;.
By marginalization of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from the joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(x,y)&lt;/script&gt;, we can write the normalization constant &lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt; in the denominator differently:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y)  = \frac{p(y \mid x) p(x)}{\int\limits_x p(y\mid x)p(x) dx}&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;bayes-estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;recursive-bayesian-estimation--conditional-independence&quot;&gt;Recursive Bayesian Estimation + Conditional Independence&lt;/h2&gt;

&lt;p&gt;In estimation, the state &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is static. In filtering, the state &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is dynamic. We will address the &lt;strong&gt;Bayesian estimation&lt;/strong&gt; case first, which can be modeled graphically as Naive Bayes, and &lt;strong&gt;later we’ll address the Bayesian filtering&lt;/strong&gt; case.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/naive_bayes.png&quot; width=&quot;45%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
Suppose we have sequence of measurements (Y_1,Y_2,..., Y_t), revealed. X is a static state. Assume Y_1,..., Y_t are conditionally independent given X. This is the Naive Bayes' structure of Bayesian estimation.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;We’ll now derive the recursion, which iterates upon the previous timesteps. The key insight is that &lt;strong&gt;we can factor Bayes Rule via conditional independence.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have observed a measurement &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt; timesteps. In the &lt;strong&gt;previous time step&lt;/strong&gt; we had posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t-1})  = \frac{p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx}&lt;/script&gt;

&lt;p&gt;We want to compute:
&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y_{1:t})  = \frac{p(y_{1:t} \mid x) p(x)}{\int\limits_x p(y_{1:t}\mid x)p(x) dx}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;By the Chain Rule:
&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t, x, y_{1:t-1}) }{\int\limits_x p(y_{1:t}, x) dx} =  \frac{ p(y_t \mid x, y_{1:t-1}) p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t}\mid x)p(x) dx}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We assume that conditioned upon &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, all &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; are conditionally independent. Thus, we can discard evidence: &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x, y_{1:t-1}) = p(y_t \mid x)&lt;/script&gt;. Simplifying, we see:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t \mid x) p(y_{1:t-1} \mid x) p(x)}{\int\limits_x  p(y_t \mid x) p(y_{1:t-1} \mid x) p(x) dx}&lt;/script&gt;

&lt;p&gt;Rewrite it in terms of the posterior from the previous time step. We see the previous time step’s posterior also included &lt;script type=&quot;math/tex&quot;&gt;p(y_{1:t-1} \mid x) p(x)&lt;/script&gt; (the unnormalized previous time step posterior)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t})  = \frac{ p(y_t \mid x) \frac{p(y_{1:t-1} \mid x) p(x)}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx} }{ \frac{\int\limits_x p(y_t \mid x) p(y_{1:t-1} \mid x) p(x) dx}{\int\limits_x p(y_{1:t-1}\mid x)p(x) dx} }&lt;/script&gt;

&lt;p&gt;Since constant w.r.t integral, can combine integrals, and see previous posterior in the denominator. Get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid y_{1:t}) = \frac{p(y_t \mid x) p(x \mid y_{1:t-1})}{ \int\limits_x p(y_t \mid x) p(x \mid y_{1:t-1})dx} = f\Bigg(y_t, p(x \mid y_{1:t-1}) \Bigg)&lt;/script&gt;

&lt;p&gt;In the recursive Bayes Filter, the prior is just the posterior from the previous time step. Thus, we have the following loop: &lt;strong&gt;Measure-&amp;gt;Estimate-&amp;gt;Measure-&amp;gt;Estimate…&lt;/strong&gt;. We only got this from conditional independence of measurements, given the state. The graphical model is Naive Bayes.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;pgm-bayes-filter&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;graphical-model-the-structure-of-variables-in-the-bayes-filter&quot;&gt;Graphical Model: The Structure of Variables in the Bayes Filter&lt;/h2&gt;

&lt;p&gt;Now consider a dynamic state &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;, instead of a static &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. This system can be modeled as a Hidden Markov Model.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/bayesian_filter_hmm.png&quot; width=&quot;65%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    As we move forward in time, the state evolves from $$X_0 \rightarrow X_1 \rightarrow \dots \rightarrow X_t$$
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The HMM structure gives us two gifts: (1) the Markov Property, and (2) Conditional Independence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Markov Property (Markovianity)&lt;/strong&gt;: discard information regarding the state&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid x_{1:t-1}, y_{1:t-1}) = p(x_t \mid x_{t-1})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Conditional Independence of Measurements&lt;/strong&gt;: discard information regarding the measurement&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t \mid x_{1:t}, y_{1:t-1}) = p(y_t \mid x_t)&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;bayes-filter-deriv-predict&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;derivation-of-the-bayes-filter-predict-step&quot;&gt;Derivation of the Bayes Filter: Predict Step&lt;/h2&gt;

&lt;p&gt;Suppose we start with our posterior from the previous timestep, which incorporates all measurements &lt;script type=&quot;math/tex&quot;&gt;y_1,\dots, y_{t-1}&lt;/script&gt;: Via marginalization, we can rewrite the expression as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_{t-1}} p(x_{t}, x_{t-1}  \mid y_{1:t-1})dx_{t-1}&lt;/script&gt;

&lt;p&gt;By the chain rule, we can factor:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_{t-1}} p(x_{t},   \mid x_{t-1}, y_{1:t-1})  p( x_{t-1}  \mid y_{1:t-1})   dx_{t-1}&lt;/script&gt;

&lt;p&gt;By Markovianity, we can simplify the expression above, giving us &lt;strong&gt;the Predict Step&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t-1}) = \int_{x_t-1} p(x_{t} \mid x_{t-1}) p(x_{t-1} \mid y_{1:t-1})dx_{t-1}&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;bayes-filter-deriv-update&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;derivation-of-the-bayes-filter-update-step&quot;&gt;Derivation of the Bayes Filter: Update Step&lt;/h2&gt;

&lt;p&gt;In what we call the update step, we simply express the posterior using Bayes’ Rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{p(y_{1:t} \mid x_{t}) p(x_{t})}{p(y_{1:t})} =  \frac{p(y_{1:t} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_{1:t} \mid x_{t}) p(x_{t}) dx_{t}}&lt;/script&gt;

&lt;p&gt;We can now factor the numerator with the chain rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t, y_{1:t-1}) p(y_{1:t-1} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_t \mid x_t, y_{1:t-1}) p(y_{1:t-1} \mid x_{t}) p(x_{t})dx_{t}}&lt;/script&gt;

&lt;p&gt;By the conditional independence of measurements &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, we know &lt;script type=&quot;math/tex&quot;&gt;p(y_t \mid x_t, y_{1:t-1}) = p(y_t \mid x_t)&lt;/script&gt;, so we can simplify:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(y_{1:t-1} \mid x_{t}) p(x_{t})}{\int\limits_{x_{t}} p(y_t \mid x_t) p(y_{1:t-1} \mid x_{t}) p(x_{t})dx_{t}}&lt;/script&gt;

&lt;p&gt;Interestingly enough, we can see above in the right hand side two terms from Bayes’ Rule. We’ll be able to collapse them into a single term. Using these two terms, the left side of Bayes’ Rule would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) = \frac{p(y_{1:t-1} \mid x_{t}) p(x_{t})}{ \int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t }&lt;/script&gt;

&lt;p&gt;Multiplying by the denominator, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) \int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t = p(y_{1:t-1} \mid x_{t}) p(x_{t})&lt;/script&gt;

&lt;p&gt;Since by marginalization &lt;script type=&quot;math/tex&quot;&gt;\int_{x_t}p(y_{1:t-1} \mid x_{t}) p(x_{t})  dx_t = p(y_{1:t-1})&lt;/script&gt;, we can simplify the line above to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t \mid y_{1:t-1}) p(y_{1:t-1}) = p(y_{1:t-1} \mid x_{t}) p(x_{t})&lt;/script&gt;

&lt;p&gt;We now plug this substitution into the numerator and in the denominator of the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1})   }{\int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;p(y_{1:t-1})&lt;/script&gt; does not depend upon &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we can pull it out of the integral, and the term cancels in the top and bottom:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) p(y_{1:t-1})   }{ p(y_{1:t-1})\int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1})  dx_{t}} = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int\limits_{x_{t}} p(y_t \mid x_t) p(x_t \mid y_{1:t-1})  dx_{t}}&lt;/script&gt;

&lt;p&gt;This is the closed form expression for the &lt;strong&gt;Update Step&lt;/strong&gt; of the Bayes’ Filter.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t} \mid y_{1:t}) = \frac{p(y_{t} \mid x_{t})p(x_{t} \mid y_{1:t-1})}{\int\limits_{x_{t}} p(y_{t} \mid x_{t}) p(x_{t} \mid y_{1:t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;The algorithm consists of repeatedly applying two steps: (1) the &lt;strong&gt;predict step&lt;/strong&gt;, where we move forward the time step, and (2) the &lt;strong&gt;update step&lt;/strong&gt;, where we incorporate the measurement.  They appear in an arbitrary order and constitute a cycle, but you have to start somewhere. At the end of the update step, we have the best estimate.&lt;/p&gt;

&lt;p&gt;It turns out that we can write out these integrals analytically for a very special family of distributions: Gaussian distributed random variables. This will be the Kalman Filter, which is covered in the next blog post in the &lt;em&gt;State Estimation&lt;/em&gt; module.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stephen Boyd and Reza Mahalati. Lecture 1,  Introduction to Linear Dynamical Systems (EE 263). &lt;a href=&quot;http://ee263.stanford.edu/lectures/overview.pdf&quot;&gt;http://ee263.stanford.edu/lectures/overview.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Stephen Boyd and Lieven Vandenberghe. &lt;em&gt;Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares&lt;/em&gt;. Cambridge University Press, 2018. &lt;a href=&quot;https://web.stanford.edu/~boyd/vmls/vmls.pdf&quot;&gt;https://web.stanford.edu/~boyd/vmls/vmls.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.&lt;/p&gt;</content><author><name></name></author><summary type="html">linear dynamical systems, bayes' rule, bayesian filtering &amp; estimation ...</summary></entry><entry><title type="html">Iterative Closest Point</title><link href="http://localhost:4000/icp/" rel="alternate" type="text/html" title="Iterative Closest Point" /><published>2018-11-29T06:00:00-05:00</published><updated>2018-11-29T06:00:00-05:00</updated><id>http://localhost:4000/icp</id><content type="html" xml:base="http://localhost:4000/icp/">&lt;h2 id=&quot;the-3d-registration-problem&quot;&gt;The 3D Registration Problem&lt;/h2&gt;
&lt;p&gt;Given two shapes A and B which partially overlap&lt;/p&gt;

&lt;p&gt;Goal: using only rigid transforms, register B against A by minimizing a measure of distance between A and B&lt;/p&gt;

&lt;p&gt;Assume A and B are positioned close to each other&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min\limits_T \delta(A, T(B))&lt;/script&gt;

&lt;h2 id=&quot;degrees-of-freedom-transform-estimation&quot;&gt;Degrees of Freedom: Transform estimation&lt;/h2&gt;

&lt;p&gt;A rigid motion has 6 degrees of freedom (3 for translation and 3 for rotation). We typically estimate the motion using many more pairs of corresponding points, so the problem is overdetermined (which is good, given noise, outliers, etc – use least squares approaches).&lt;/p&gt;

&lt;h2 id=&quot;key-challenges-of-the-hard-optimization-problem&quot;&gt;Key Challenges of the Hard Optimization Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(1) We must estimate correspondences. This gives rise to combinatorial searches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(2) We must estimate the aligning transform. Transforms may be non-linear.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fortunately, the low-dimensionality of some transforms helps.&lt;/p&gt;

&lt;h2 id=&quot;optimal-transformation-for-point-clouds&quot;&gt;Optimal Transformation for Point Clouds&lt;/h2&gt;

&lt;p&gt;When given correspondences, the problem is formulated as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Given two sets points:&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\{a_i \}_{i=1}^n, \{ b_i \}_{i=1}^n&lt;/script&gt; &lt;em&gt;in&lt;/em&gt;   . &lt;em&gt;Find the rigid transform&lt;/em&gt; \(\mathbf{R}, t\) &lt;em&gt;that minimizes&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{\mathbf{R}, t}{\mbox{minimize }} \sum\limits_{i=1}^N \| \mathbf{R}x_i + t - y_i \|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n \|Ra_i − t − b_i \|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{a} = \frac{1}{|A|} \sum\limits_i a_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{b} = \frac{1}{|B|} \sum\limits_i b_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_i^{\prime} = a_i − \bar{a}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_i^{\prime} = b_i − \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_i = a_i^{\prime} + \bar{a},  b_i = b_i^{\prime} + \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum\limits_{i=1}^n \|R(a_i^{\prime} + \bar{a}) − t − (b_i^{\prime} + \bar{b})\|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum\limits_{i=1}^n \|Ra_i^{\prime} − b_i^{\prime} + (R\bar{a} − \bar{b} − t)\|_2^2&lt;/script&gt;

&lt;p&gt;Let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = R\bar{a} − \bar{b}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^n \|Ra_i − t − b_i \|_2^2 = \sum\limits_{i=1}^n \|Ra_i^{\prime} − b_i^{\prime}\|_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;tr(RN)&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def find_rigid_alignment(A, B):
	&quot;&quot;&quot;
	2-D or 3-D registration with known correspondences.
	Registration occurs in the zero centered coordinate system, and then
	must be transported back.

		Args:
		-	A: Numpy array of shape (N,D) -- Reference Point Cloud (target)
		-	B: Numpy array of shape (N,D) -- Point Cloud to Align (source)

		Returns:
		-	R: optimal rotation
		-	t: optimal translation
	&quot;&quot;&quot;
	num_pts = A.shape[0]
	dim = A.shape[1]

	a_mean = np.mean(A, axis=0)
	b_mean = np.mean(B, axis=0)

	# Zero-center the point clouds
	A -= a_mean
	B -= b_mean

	N = np.zeros((dim, dim))
	for i in range(num_pts):
		N += A[i].reshape(dim,1).dot( B[i].reshape(1,dim) )
	N = A.T.dot(B)

	U, D, V_T = np.linalg.svd(N)
	S = np.eye(dim)
	det = np.linalg.det(U) * np.linalg.det(V_T.T)
	
	# Check for reflection case
	if not np.isclose(det,1.):
		S[dim-1,dim-1] = -1

	R = U.dot(S).dot(V_T)
	t = R.dot( b_mean.reshape(dim,1) ) - a_mean.reshape(dim,1)
	return R, -t.squeeze()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similar code named the &lt;em&gt;Umeyama Transform&lt;/em&gt; after[4] ships with &lt;a href=&quot;https://eigen.tuxfamily.org/dox/group__Geometry__Module.html#gab3f5a82a24490b936f8694cf8fef8e60&quot;&gt;Eigen&lt;/a&gt; as &lt;a href=&quot;https://eigen.tuxfamily.org/dox/Umeyama_8h_source.html&quot;&gt;Umeyama&lt;/a&gt;. The Open3D library utilizes the Umeyama method also (source code &lt;a href=&quot;https://github.com/IntelVCL/Open3D/blob/master/src/Core/Registration/TransformationEstimation.cpp#L47&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;local-methods&quot;&gt;Local Methods&lt;/h2&gt;

&lt;h3 id=&quot;iterated-closest-pair-icp-2&quot;&gt;Iterated Closest Pair (ICP) [2]&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Align the A points to their closest B neighbors, then repeat.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Converges, if starting positions are “close enough”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variants&quot;&gt;Variants&lt;/h2&gt;

&lt;h3 id=&quot;exhaustive-search&quot;&gt;Exhaustive Search&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Compare (ideally) all alignments&lt;/li&gt;
  &lt;li&gt;Sample the space of possible initial alignments.&lt;/li&gt;
  &lt;li&gt;Correspondence is determined by the alignment at which models are closest.&lt;/li&gt;
  &lt;li&gt;Provides optimal result&lt;/li&gt;
  &lt;li&gt;Can be unnecessarily slow&lt;/li&gt;
  &lt;li&gt;Does not generalize well to non-rigid deformations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Leonidas Guibas. Alignments and Correspondences, Geometric Features. CS 233: Geometric and Topological Data Analysis, 9 May 2018.&lt;/p&gt;

&lt;p&gt;[2] Besl, McKay 1992.&lt;/p&gt;

&lt;p&gt;[3] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns”, PAMI 1991. &lt;a href=&quot;http://web.stanford.edu/class/cs273/refs/umeyama.pdf&quot;&gt;http://web.stanford.edu/class/cs273/refs/umeyama.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Jeff Phillips. Lecture 24: Iterative Closest Point and Earth Mover’s Distance. CPS296.2 Geometric Optimization. 10 April 2007. &lt;a href=&quot;https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture24.pdf&quot;&gt;https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture24.pdf&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Constrained Optimization, Lagrangians, Duality, and Interior Point Methods</summary></entry></feed>