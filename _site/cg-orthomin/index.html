<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Modern Iterative Methods for Linear System Solving</title>
  <meta name="description" content="Krylov subspaces, Conjugate Gradients (CG), large systems of equations, Orthomin, Cayley-Hamilton Theorem">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/cg-orthomin/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>Modern Iterative Methods for Linear System Solving</h1>
    <p class="meta">Dec 27, 2018</p>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>
<ul>
  <li><a href="#conjugate-gradients">Conjugate Gradients</a></li>
  <li><a href="#krylov-subspaces">Krylov Subspaces</a></li>
  <li><a href="#cayley-hamilton-thm">Cayley-Hamilton Theorem</a></li>
  <li><a href="#cg-algo">The CG Algorithm</a></li>
  <li><a href="#precondition">Preconditioning</a></li>
</ul>

<h2 id="modern-iterative-solvers-for-systems-of-linear-equations">Modern Iterative Solvers for Systems of Linear Equations</h2>

<p>Two key ideas serve as the foundation of modern iterative solvers: Krylov subspaces and pre-conditioning. In this tutorial, we’ll discuss Krylov subspaces for the problem <script type="math/tex">Ax=b</script>.</p>

<p><a name="krylov-subspaces"></a></p>
<h2 id="the-krylov-subspace">The Krylov Subspace</h2>

<p>CG relies upon an idea named the <em>Krylov subspace</em>. The Krylov subspace is defined as the span of the vectors generated from successively higher powers of <script type="math/tex">A</script>:</p>

<script type="math/tex; mode=display">\mathcal{K}_k = \mbox{span} \{b, Ab, \dots, A^{k-1}b \}</script>

<p>Krylov subspace approximation is a hierarchical (nested) structure of subspaces. We will search for an approximation of <script type="math/tex">x</script> in this space. A first approximation is <script type="math/tex">x_0 \in \mbox{span} \{ \vec{b} \}</script>.  If <script type="math/tex">A</script> is the identity matrix or a scalar, then this subspace is already sufficient to find <script type="math/tex">x</script> in <script type="math/tex">Ax=b</script>.</p>

<p>We can proceed with higher powers of <script type="math/tex">k</script>, e.g. <script type="math/tex">x_1 \in \mbox{span} \{b, Ab \}</script>, <script type="math/tex">x_k \in \mbox{span} \{ b, Ab, \cdots, A^{k-1}b \}</script>. Our approximation for <script type="math/tex">x^{\star}</script> will become increasingly refined as we perform enlargement of Krylov subspaces.</p>

<p>The Krylov sequence is a sequence of solutions to our convex objective <script type="math/tex">f(x)</script>. Each successive solution has to come from a subspace <script type="math/tex">\mathcal{K}_k</script> of progressively higher power <script type="math/tex">k</script>. Formally, the Krylov sequence <script type="math/tex">x^{(1)},x^{(2)},\dots</script> is defined as:</p>

<script type="math/tex; mode=display">x^{(k)} = \underset{x \in \mathcal{K}_k}{\mbox{argmin }} f(x)</script>

<p>The CG algorithm generates the Krylov sequence. The beauty of the Krylov subspaces are that any element (a vector) can be constructed only with (1) matrix vector multiplication, (2) vector addition, and (3) vector-scalar multiplication. We never have to compute expensive things like <script type="math/tex">A^{-1}</script>, or determinant. This is because <script type="math/tex">Ab</script> is a vector, so <script type="math/tex">A^2b = A(Ab)</script> is also just matrix-vector multiplication of <script type="math/tex">A</script> with the vector <script type="math/tex">Ab</script>.</p>

<h2 id="theorem-justification-of-krylov-subspaces">Theorem: Justification of Krylov Subspaces</h2>

<p>Theorem: Given any nonsingular <script type="math/tex">A</script> of shape <script type="math/tex">{n \times n}</script>, then</p>

<script type="math/tex; mode=display">A^{-1}b \in \mbox{span} \Bigg\{ b, Ab, A^b, \cdots, A^{n-1}b \Bigg\} = \mathcal{K}_n</script>

<p>This theorem has two implications: (1) If you search inside <script type="math/tex">\mathcal{K}_n</script>, whether or not <script type="math/tex">K</script> spans the whole space <script type="math/tex">\mathbf{R}^{n}</script>, you will get the solution. An extreme case when <script type="math/tex">K \neq \mathbf{R}^n</script> is that <script type="math/tex">b</script> is an eigenvector of <script type="math/tex">A</script> (then the space is 1-dimensional). If you want to solve a linear system, this is why it is sufficient to look inside the Krylov subspaces.  Implication 2: If you search orthogonally inside <script type="math/tex">K</script>, then you can get the solution in <script type="math/tex">\leq n</script> steps. The Krylov subspace is at most $n$-dimensional since it is composed of <script type="math/tex">n</script>-dimensional vectors. CG, Orthomin(2), and GMRES all do this. GMRES is in a more general setup. Notice that the theorem itself is independent of the search algorithm – it will always be true. To prove the theorem, we need a lemma:</p>

<p><a name="cayley-hamilton-thm"></a></p>

<h2 id="lemma-the-cayley-hamilton-theorem">Lemma: The Cayley-Hamilton Theorem</h2>

<p>Let <script type="math/tex">\chi(s)</script> be a polynomial function of <script type="math/tex">s</script> defined as <script type="math/tex">\chi(s) = \mbox{det}(sI-A)</script>
which is the characteristic polynomial of <script type="math/tex">A</script>. Then <script type="math/tex">\chi(A)=0_{n \times n}</script> (a zero matrix). The Cayley-Hamilton Theorem states that if <script type="math/tex">A \in \mathbf{R}^{n \times n}</script>:</p>

<script type="math/tex; mode=display">A^n + \alpha_1 A^{n-1} + \cdots + \alpha_n I = 0_{n \times n}</script>

<p>We can solve for <script type="math/tex">I</script> by rearranging terms:</p>

<script type="math/tex; mode=display">\alpha_n I = - A^n + - \alpha_1 A^{n-1} + \cdots + \alpha_{n-1} A^{1}</script>

<p>We now divide by <script type="math/tex">\alpha_n</script>:</p>

<script type="math/tex; mode=display">I = - \frac{1}{\alpha_n} A^n + -  \frac{\alpha_1}{\alpha_n} A^{n-1} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} A^{1}</script>

<p>We now left-multiply all terms by <script type="math/tex">A^{-1}</script> and simplify:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
A^{-1}I & = - \frac{1}{\alpha_n} A^{-1} A^n + -  \frac{\alpha_1}{\alpha_n} A^{-1} A^{n-1} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} A^{-1} A^{1} \\
A^{-1} & = - \frac{1}{\alpha_n} A^{n-1} + -  \frac{\alpha_1}{\alpha_n}  A^{n-2} + \cdots +  \frac{\alpha_{n-1}}{\alpha_n} I \\
\end{aligned} %]]></script>

<p>We can now see that <script type="math/tex">x^{\star}</script> is a linear combination of the vectors that span the Krylov subspace. Thus, <script type="math/tex">x^{\star} = A^{-1}b \in \mathcal{K}_n</script>.</p>

<p>Thus, we need to search in the Krylov subspace of maximum size! If you have a smaller subapce, you may or may not have the exact solution inside.</p>

<h2 id="krylov-subspace-methods">Krylov Subspace Methods</h2>

<p>We will consider two types of error. One:</p>

<p><script type="math/tex">r_k = b-Ax_k</script> residue/residual: what your <script type="math/tex">x_k</script> cannot satisfy
smaller <script type="math/tex">\|r_k\| \implies</script> better satisfaction of the equation $\approx$ better accuracy</p>

<p>Two, your error: <script type="math/tex">e_k = x^{\star} - x_k</script>. These are not the same thing.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Orthomin(1)</th>
      <th style="text-align: right">Orthomin(2)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">r_{k+1} = r_k - a_k A p_k</script></td>
      <td style="text-align: center"><script type="math/tex">r_{k+1} \perp A p_k</script></td>
      <td style="text-align: right"><script type="math/tex">r_{k+1}</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">e_{k+1} = e_k - a_k p_k</script></td>
      <td style="text-align: center"><script type="math/tex">e_{k+1} \perp_A p_k</script></td>
      <td style="text-align: right"><script type="math/tex">e_{k+1}</script></td>
    </tr>
  </tbody>
</table>

<p>Suppose we define some kind of iterative method via a recurrence relation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
x_{k+1} &= x_k + a_k (b- Ax_k) \\
 x_{k+1} &= x_k + a_kr_k
 \end{aligned} %]]></script>

<p>We can then also obtain a recurrence between the residuals, with <script type="math/tex">a_k</script> as our scalar knob:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
        r_{k+1} &= b - Ax_{k+1} \\
        &= b - A \Big(x_k + a_k(b-Ax_k) \Big) \\
        &= (b - Ax_k) - a_kA(b-Ax_k) \\
        &= r_k - a_kAr_k
\end{aligned} %]]></script>

<h3 id="orthomin1">Orthomin(1)</h3>

<p>If you want to make the residue as small as possible, given some initial residue from some initial guess, then you want to minimize <script type="math/tex">\|r_{k+1}\|</script>. Since <script type="math/tex">aAr_k</script> and <script type="math/tex">r_k</script> are simple vectors, we know how to minimize their difference: one should project <script type="math/tex">r_k</script> onto <script type="math/tex">a_kAr_k</script>:</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/orthomin_1_projection.jpg" width="50%" />
  <div class="figcaption">
    Projection minimizes the difference of the vectors. Find a scaling a_k so that if you subtract the result from r_k, the result is minimized.
  </div>
</div>

<p>As shown <a href="/linear-algebra/">in the tutorial on projection</a>, projection of a vector <script type="math/tex">x</script> onto a vector <script type="math/tex">y</script> is given by <script type="math/tex">P_y(x)= \frac{x^Ty}{y^Ty}y</script>. We wish to solve for the scalar knob <script type="math/tex">a_k</script>, with <script type="math/tex">x=r_k, y= Ar_k</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
a_k Ar_k &= \frac{ r_k^T (Ar_k) }{ (Ar_k)^T (Ar_k)} (Ar_k) \\
a_k &= \frac{ r_k^T (Ar_k) }{ (Ar_k)^T (Ar_k)} & \mbox{ remove multiplication by vector on both sides} \\
 a_k &= \frac{\langle r_k, Ar_k \rangle}{\langle Ar_k, Ar_k \rangle} & \mbox{write as standard 2-inner products}
\end{aligned} %]]></script>

<p>To minimize the 2-norm, we use standard 2-inner-product. This gives the Orthomin(1) method – from orthogonal projections. The purpose is to minimize the residual and the method seems to work for a general <script type="math/tex">A</script> matrix.</p>

<h3 id="steepest-descent">Steepest Descent</h3>

<p>Now contrast Orthomin(1) with a second method, <em>Steepest Descent</em>. In Steepest Descent, you don’t have to choose the 2-norm to minimize the residual.</p>

<p>If <script type="math/tex">A</script> is Hermitian (can always diagonalize it, eigenvalues are real), and if it is positive definite (all eigenvalues are non-zero positive), then you can define another norm. This norm comes from the 2-norm, induced by <script type="math/tex">A</script>:</p>

<script type="math/tex; mode=display">\|x\|_A = \langle x, Ax \rangle^{1/2}</script>

<p>When <script type="math/tex">A</script> is positive definite, it is truly a norm. We can use this norm instead.</p>

<p>In Steepest Descent, we will minimize the error <script type="math/tex">e_k = x^{\star} - x_k</script> instead of the residual <script type="math/tex">r_k</script>. You might ask, how could you possibly compute the error if we don’t have access to <script type="math/tex">x^{\star}</script>, which requires solving the problem? It turns out that terms cancel, so we don’t actually need to know <script type="math/tex">x^{\star}</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
	e_k &= x^{\star} - x_k \\
		&= A^{-1}b - x_k \\
		&= A^{-1}b + A^{-1}(r_k - b) & \mbox{since } r_k = b - Ax_k \implies A^{-1}(r_k -b) = - x_k \\
		&= A^{-1}b - A^{-1}b + A^{-1}r_k \\
		&= A^{-1}r_k \\
\end{aligned} %]]></script>

<p>We can now form a recurrence between <script type="math/tex">e_{k+1}</script> and <script type="math/tex">e_{k}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
e_{k} &= A^{-1}r_k & \mbox{as shown above} \\
e_{k+1} &= A^{-1} r_{k+1} & \mbox{consider the next time step} \\
e_{k+1} &= A^{-1} \Big( r_k - a_k A r_k \Big) \\
e_{k+1} &= A^{-1}r_k - a_k A^{-1}Ar_k \\
e_{k+1}	&= A^{-1}r_k - a_k r_k & \mbox{because } A^{-1}A = I \\
e_{k+1} &= e_k - a_k r_k & \mbox{because } e_k = A^{-1}r_k
\end{aligned} %]]></script>

<p>Our recurrence is between two vectors, <script type="math/tex">e_k</script> and a scaled <script type="math/tex">r_k</script>.  In order to minimize <script type="math/tex">\|e_{k+1}\|_A</script> given <script type="math/tex">e_k</script>, we can simply project <script type="math/tex">e_k</script> onto <script type="math/tex">r_k</script>.</p>

<p>We recall that the projection of a vector <script type="math/tex">x</script> onto a vector <script type="math/tex">y</script> is given by <script type="math/tex">P_y(x) = \frac{x^Ty}{y^Ty}y</script>. Thus, we can compute the scaling <script type="math/tex">a</script> in <script type="math/tex">(ar_k)</script> as:</p>

<script type="math/tex; mode=display">a_k = \frac{\langle e_k, r_k \rangle_A}{\langle r_k, r_k \rangle_A} = \frac{\langle e_k, Ar_k\rangle }{\langle r_k, Ar_k\rangle } = \frac{\langle A^He_k, r_k\rangle }{\langle r_k, Ar_k\rangle } = \frac{\langle A e_k, r_k\rangle }{\langle r_k, Ar_k\rangle } = \frac{\langle r_k, r_k\rangle }{\langle r_k, Ar_k\rangle }</script>

<p>We used the fact that <script type="math/tex">A=A^H</script> (since <script type="math/tex">A</script> is Hermitian) and the fact that <script type="math/tex">e_k = A^{-1}r_k</script>, so <script type="math/tex">Ae_k = r_k</script>. Note that now everything is expressed in terms of <script type="math/tex">r_k</script>, not <script type="math/tex">e_k</script>, since <script type="math/tex">r_k</script> is computable even when <script type="math/tex">e_k</script> isn’t. This method is called steepest descent and as we will see, it corresponds to a Krylov subspace method. Steepest descent requires that <script type="math/tex">A=A^H</script>, and that <script type="math/tex">A</script> is positive definite for the norm definition.</p>

<h3 id="another-perspective-on-steepest-descent">Another Perspective on Steepest Descent</h3>

<p>Another way to consider Steepest Descent is to consider that the algorithm at each step performs a gradient descent step with optimal step size. Consider the following objective function:</p>

<script type="math/tex; mode=display">V(x) = x^HAx - 2b^Hx</script>

<p>Then we can find the stationarity condition by finding the derivative and set to zero:</p>

<script type="math/tex; mode=display">\begin{aligned}
\nabla V = 2Ax - 2b = 0 \\
Ax = b
\end{aligned}</script>

<p>Our step is
<script type="math/tex">x_{k+1} = x_k - a_k \frac{1}{2}\nabla V(x_k)</script></p>

<p>At each step, we go as deep as you can, then we change to another direction.</p>

<h3 id="steepest-descent-is-a-krylov-subspace-method">Steepest Descent is a Krylov Subspace Method</h3>

<p>Recall that a Krylov Subspace method builds solutions in the subpace</p>

<script type="math/tex; mode=display">\mathcal{K}_k = \mbox{span} \{ \vec{b}, A \vec{b}, A^2 \vec{b}, \cdots \}</script>

<p>In Steepest Descent, our iterative solutions are formed from the recurrence relation:</p>

<script type="math/tex; mode=display">\vec{x}_{k+1} = \vec{x}_k + a_k( \vec{b} - A\vec{x}_k)</script>

<p>One can prove via induction that Steepest Descent is a Krylov Subspace method. To show a brief example, suppose <script type="math/tex">\vec{x}_0 = 0</script>. This implies</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\vec{x}_1 &= x_0 + a_k (b-Ax_0) \\
\vec{x}_1 &= 0 + a_k (b-A \cdot 0) \\
\vec{x}_1 &= a_k b \\
\vec{x}_1 & \in \mbox{span} \{ \vec{b} \} \\
\end{aligned} %]]></script>

<p>Consider <script type="math/tex">\vec{x}_2</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\vec{x}_2 &= x_1 + a_k (b-Ax_1) \\
\vec{x}_2 &= (a_k b) + a_k \Big(b-A(a_k b)\Big) & \mbox{since } \vec{x}_1 &= a_k b \\
\vec{x}_2 &= a_kb + a_kb - a_k Aa_k b\\
\vec{x}_2 &= (2 a_k) b - (a_k^2) Ab \\
\vec{x_2} & \in \mbox{span} \Bigg\{ \vec{b}, A \vec{b} \Bigg\}
\end{aligned} %]]></script>

<p>If you consider <script type="math/tex">x_{k+1}</script>, it is not hard to show that if <script type="math/tex">x_k</script> was in a Krylov subspace, then so is <script type="math/tex">x_{k+1}</script>. If <script type="math/tex">x_0</script> is non-zero, we simply start with a shifted <script type="math/tex">\vec{b}</script>. As we recall, with an <script type="math/tex">n</script>-dimensional problem, we only need to do <script type="math/tex">n</script>-steps, then we span the whole space and are guaranteed to find the solution.</p>

<h3 id="orthomin2">Orthomin(2)</h3>

<p>We now seek an improvement on Orthomin(1). Orthomin(1) fails to converge if <script type="math/tex">0</script> is in the numerical range of <script type="math/tex">A</script>. In Orthomin(1), we had a single orthogonality condition: <script type="math/tex">r_{k+1} \perp A r_k</script>. Our step was in the direction of the residual <script type="math/tex">r_k</script>. For the sake of generality, so we will call our descent direction <script type="math/tex">p_k</script>, and for Orthomin(1), it just so happens that <script type="math/tex">p_k = r_k</script>.</p>

<p>In Orthomin(2), we will now require two orthogonality conditions. We must be orthogonal to the <em>previous two</em> descent directions.</p>

<script type="math/tex; mode=display">r_{k+1} \perp A p_k, Ap_{k-1}</script>

<p>The question is, can we find some descent direction <script type="math/tex">p_k</script> such that we can accomplish this double orthogonality condition? As we showed previously, one of the orthogonality conditions is easy. If we want <script type="math/tex">r_{k+1} \perp A p_k</script>, this is doable if we just pick a good scalar value <script type="math/tex">a_k</script>. A slightly less trivial requirement is to have orthogonality to the previous descent direction.</p>

<p><a name="conjugate-gradients"></a></p>
<h2 id="conjugate-gradients-cg">Conjugate Gradients (CG)</h2>

<p>Conjugate Gradients (CG) is a well-studied method introduced in 1952 by Hestenes and Stiefel [4] for solving a system of <script type="math/tex">n</script> equations, <script type="math/tex">Ax=b</script>, where <script type="math/tex">A</script> is positive definite. Solving such a system becomes quite challenging when <script type="math/tex">n</script> is large. CG continues to be used today, especially in state-of-the-art reinforcement learning algorithms like <a href="/policy-gradients/trunc-natural-grad">TRPO</a>.</p>

<p>The CG method is interesting because we never give a set of numbers for <script type="math/tex">A</script>. In fact, we never form or even store the matrix <script type="math/tex">A</script>. This could be desirable when <script type="math/tex">A</script> is huge. Instead, CG is simply a method for calculating <script type="math/tex">A</script> times a vector [1] that relies upon a deep result, the <em>Cayley-Hamilton Theorem</em>.</p>

<p>A common example where CG proves useful is minimization of the following quadratic form, where <script type="math/tex">A \in \mathbf{R}^{n \times n}</script>:</p>

<script type="math/tex; mode=display">f(x) = \frac{1}{2} x^TAx - b^Tx</script>

<p><script type="math/tex">f(x)</script> is a convex function when <script type="math/tex">A</script> is positive definite. The gradient of <script type="math/tex">f</script> is:</p>

<script type="math/tex; mode=display">\nabla f(x) = Ax - b</script>

<p>Setting the gradient equal to <script type="math/tex">0</script> allows us to find the solution to this system of equations, <script type="math/tex">x^{\star} = A^{-1}b</script>. CG is an appropriate method to do so when inverting <script type="math/tex">A</script> directly is not feasible.</p>

<p><a name="cg-algo"></a></p>

<h2 id="the-cg-algorithm">The CG Algorithm</h2>

<p>We will maintain the square of the residual <script type="math/tex">r</script> at each step, which we call <script type="math/tex">r_k</script>. If the square root of your residual is small enough, <script type="math/tex">\sqrt{\rho_{k-1}}</script>, then you can quit. Your search direction is <script type="math/tex">p</script>: the combination of your current r esidual and the previous search direction [2,3].</p>

<ul>
  <li>\(x:=0\),  \(r:=b\),  \( \rho_0 := | r |^2 \)</li>
  <li>for <script type="math/tex">k=1,\dots,N_{max}</script>
    <ul>
      <li>quit if <script type="math/tex">\sqrt{\rho_{k-1}} \leq \epsilon \|b\|</script></li>
      <li>if \(k=1\)
        <ul>
          <li>\( p:= r \)</li>
        </ul>
      </li>
      <li>else
        <ul>
          <li>\( p:= r + \frac{\rho_{k-1}}{\rho_{k-2}} \)</li>
        </ul>
      </li>
      <li>\( w:=Ap \)</li>
      <li>\( \alpha := \frac{ \rho_{k-1} }{ p^Tw } \)</li>
      <li>\( x := x + \alpha p \)</li>
      <li>\( r := r - \alpha w \)</li>
      <li>\( \rho_k := |r|^2 \)</li>
    </ul>
  </li>
</ul>

<p>Along the way, we’ve created the Krylov sequence <script type="math/tex">x</script>. Operations like <script type="math/tex">x + \alpha p</script> or <script type="math/tex">r - \alpha w</script> are <script type="math/tex">\mathcal{O}(n)</script> (BLAS level-1).</p>

<p><strong>Properties of the Krylov Sequence</strong>
It should be clear that <script type="math/tex">x^{(k)}=p_k(A)b</script>, where <script type="math/tex">p_k</script> is a polynomial with degree <script type="math/tex">% <![CDATA[
p_k < k %]]></script>, because <script type="math/tex">x^{(k)} \in \mathcal{K}_k</script>. Surprisingly enough, the Krylov sequence is a two-term recurrence:</p>

<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} + \alpha_k r^{(k)} + \beta_k (x^{(k)} - x^{(k+1)})</script>

<p>for some values <script type="math/tex">\alpha_k, \beta_k</script>.This means the current iterate is a linear combination of the previous two iterates.  This is the basis of the CG algorithm.</p>

<p><a name="precondition"></a></p>

<h2 id="the-art-of-preconditioning">The Art of Preconditioning</h2>

<p>It turns out that CG will often just fail. The trick in CG is to change coordinates first (precondition) and then run CG on the system in the changed coordinates. This is because of round-off errors that accumulate, leading to unstability and divergence. For example, we may want to make the spectrum of <script type="math/tex">A</script> clustered.</p>

<p>Instead of solving a linear system <script type="math/tex">Ax=b</script>, one can solve <script type="math/tex">M^{-1}Ax = M^{-1}b</script> for some carefully-chosen matrix <script type="math/tex">M</script>. Theoretically, the problems are the same. Numerically, however, pre-conditioning is possibly advantageous. <em>Preconditioning a matrix is an art.</em> Here are two good criteria for which preconditioner should you use:</p>
<ul>
  <li>Need <script type="math/tex">M^{-1}y</script> cheap to compute.</li>
  <li>We actually want <script type="math/tex">M \approx A</script>. Then <script type="math/tex">M^{-1}A \approx I</script>, and <script type="math/tex">M^{-1}Ax = M^{-1}b</script> is easy to solve.</li>
</ul>

<p>Krylov subspaces and preconditioning can be combined. We basically need</p>

<script type="math/tex; mode=display">x_k \in \mbox{span} \Bigg\{M^{-1}b, (M^{-1}A)M^{-1}b, \cdots, (M^{-1}A)^{k-1} M^{-1}b \Bigg\}</script>

<p>That is our preconditioned Krylov subspace!</p>

<p>A good, generic preconditioner is a diagonal matrix, e.g.</p>

<script type="math/tex; mode=display">M = \mbox{diag}(\frac{1}{A_{11}}, \dots, \frac{1}{A_{nn}})</script>

<p>We will discuss pre-conditioning in more detail in an upcoming tutorial.</p>

<h2 id="references">References</h2>

<p>[1] Stephen Boyd. <em>Conjugate Gradient Method</em>. EE364b Lectures Slides, Stanford University. <a href="https://web.stanford.edu/class/ee364b/lectures/conj_grad_slides.pdf">https://web.stanford.edu/class/ee364b/lectures/conj_grad_slides.pdf</a>. <a href="https://www.youtube.com/watch?v=E4gl91l0l40">Video</a>.</p>

<p>[2] C.T. Kelley. <em>Iterative Methods for Optimization</em>. SIAM, 2000.</p>

<p>[3] C. Kelley. <em>Iterative Methods for Linear and Nonlinear Equations</em>. Frontiers in Applied Mathematics SIAM, (1995). <a href="https://archive.siam.org/books/textbooks/fr16_book.pdf">https://archive.siam.org/books/textbooks/fr16_book.pdf</a>.</p>

<p>[4] Magnus Hestenes and Eduard Stiefel. <em>Method of Conjugate Gradients for Solving Linear Systems</em>. Journal of Research of the National Bureau of Standards, Vol. 49, No. 6, December 1952. <a href="https://web.njit.edu/~jiang/math614/hestenes-stiefel.pdf">https://web.njit.edu/~jiang/math614/hestenes-stiefel.pdf</a>.</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
