<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>PyTorch Tutorial</title>
  <meta name="description" content="PyTorch Tutorial">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://johnwlambert.github.io/pytorch-tutorial/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>PyTorch Tutorial</h1>
    <p class="meta">Sep 13, 2019</p>
  </header>

  <article class="post-content">
  <p>This tutorial was contributed by John Lambert.</p>

<p>This tutorial will serve as a crash course for those of you not familiar with PyTorch. It is written in the spirit of <a href="http://cs231n.github.io/python-numpy-tutorial/">this Python/Numpy tutorial</a>.</p>

<p>We will be focusing on CPU functionality in PyTorch, not GPU functionality, in this tutorial. We’ll be working with PyTorch 1.1.0, in these examples.</p>

<h3 id="creating-a-tensor">Creating a tensor</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># Prints "tensor([1., 2., 3.])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints "torch.Size([3])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c"># Prints "tensor([[1.],[1.]])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([0., 0., 0.])"</span>

<span class="c"># Alternatively, create a tensor by bringing it in from Numpy</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c"># Prints "[0 1 2 3]"</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># Prints "tensor([0, 1, 2, 3])"</span>
</code></pre></div></div>
<h3 id="data-types-in-pytorch-and-casting">Data types in Pytorch and Casting</h3>
<p>You’ll have a wide range of data types at your disposal, including:</p>

<table>
  <thead>
    <tr>
      <th>Data Type Name</th>
      <th>Code keywords</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>32-bit floating point</td>
      <td>torch.float32 or torch.float), torch.FloatTensor</td>
    </tr>
    <tr>
      <td>64-bit floating point</td>
      <td>torch.float64 or torch.double), torch.DoubleTensor</td>
    </tr>
    <tr>
      <td>8-bit integer (unsigned)</td>
      <td>torch.uint8, torch.ByteTensor</td>
    </tr>
    <tr>
      <td>8-bit integer (signed)</td>
      <td>torch.int8, torch.CharTensor</td>
    </tr>
    <tr>
      <td>32-bit integer (signed)</td>
      <td>torch.int32 or torch.int, torch.IntTensor</td>
    </tr>
    <tr>
      <td>64-bit integer (signed)</td>
      <td>torch.int64 or torch.long, torch.LongTensor</td>
    </tr>
    <tr>
      <td>Boolean</td>
      <td>torch.bool, torch.BoolTensor</td>
    </tr>
  </tbody>
</table>

<p>A tensor can be cast to any data type, with possible loss of precision:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c"># Prints "torch.int64", currently 64-bit integer type</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c"># Prints "torch.float32", now 32-bit float</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span> <span class="c"># Still "torch.float32"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">))</span> <span class="c"># Prints "tensor([0., 1., 2., 3.], dtype=torch.float64)"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">))</span> <span class="c"># Cast back to int-64, prints "tensor([0, 1, 2, 3])"</span>
</code></pre></div></div>

<h3 id="tensor-indexing">Tensor Indexing</h3>
<p>Tensors can be indexed using MATLAB/Numpy-style n-dimensional array indexing.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span> <span class="c"># Create an array of numbers [0,1,2,...,11]</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># Prints "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"</span>
<span class="c"># Form 3 grayscale images of shape (2,2), with CHW ordering, </span>
<span class="c"># C=3 for channels, H=2 for height, W=2 for width</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c"># Reshape tensor from (12,) to (3,2,2)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[0, 1], [2, 3]])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[4, 5], [6, 7]])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[8, 9],[10, 11]])"</span>

<span class="c"># Index instead to get all channels at (0,0) pixel for 0th row, 0th col.</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c"># Prints "tensor([0, 4, 8])"</span>
</code></pre></div></div>

<h3 id="reshaping-tensors">Reshaping tensors</h3>
<p>Above, we used <code class="highlighter-rouge">reshape()</code> to modify the shape of a tensor (but not changing the total number of elements in the tensor). Here are a few other useful tensor-shaping operations:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints "torch.Size([3, 2, 2])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Add batch dimension for NCHW, prints "torch.Size([1, 3, 2, 2])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints "torch.Size([6, 2])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints "torch.Size([6, 2])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Reshape back to flat vector, prints "torch.Size([12])"</span>
</code></pre></div></div>

<h3 id="tensor-arithmetic-matrix-multiplication">Tensor Arithmetic, Matrix Multiplication</h3>
<p>Normal Python or Numpy operators can be used for arithmetic, or explicit PyTorch operators:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span> <span class="c"># Elementwise multiplication of arrays, prints "tensor([2, 4, 6])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span> <span class="c"># Prints "tensor([3, 4, 5])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="c">#  Above is identical to using "+" op, prints "tensor([3, 4, 5])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="c"># Prints "tensor([3, 4, 5])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="c"># Prints "tensor([-1,  0,  1])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="c"># Prints "tensor([-1,  0,  1])"</span>
</code></pre></div></div>

<h3 id="other-helpful-transcendental-functions">Other helpful transcendental functions:</h3>
<p>PyTorch supports cosine, sine, and exponential operations with <code class="highlighter-rouge">cos()</code>, <code class="highlighter-rouge">sin()</code>, <code class="highlighter-rouge">exp()</code>, just like Numpy:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="c"># Prints float as "3.141592653589793"</span>
<span class="c"># print(torch.cos(np.pi)) # Will crash with TypeError, since np.pi is a float, not tensor</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)))</span> <span class="c"># Prints "tensor(-1.)"</span>
<span class="c"># print(torch.cos(torch.tensor(0))) # Will crash, Prints "RuntimeError: cos_vml_cpu not implemented for 'Long'""</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">)))</span> <span class="c"># Must use float as argument, prints "tensor(1.)"</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)))</span> <span class="c"># Prints Euler's number e as "tensor(2.7183)"</span>
</code></pre></div></div>

<h3 id="combining-tensors">Combining Tensors</h3>
<p>Tensors can be combined along any dimension, as long as the dimensions align properly. <em>Concatenating</em> (<code class="highlighter-rouge">torch.cat()</code>) or <em>stacking</em> (<code class="highlighter-rouge">torch.stack()</code>) tensors are considered different operations in PyTorch:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints torch.Size([1, 3]), x is a row vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Prints torch.Size([1, 3]), y is a row vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Concatenates sequence of tensors along a new dimension, prints torch.Size([2, 1, 3])</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Concatenates tensor along default dim=0, prints "torch.Size([2, 3])"</span>
</code></pre></div></div>

<h3 id="logical-operations">Logical Operations</h3>
<p>Logical operations like AND, OR, etc. can be computed on PyTorch tensors:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">])</span> <span class="c"># Create uint8/Byte tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span> <span class="c"># Create uint8/Byte tensor</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c"># Prints "torch.uint8 torch.uint8"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">&amp;</span> <span class="n">y</span><span class="p">)</span> <span class="c"># Logical and op., prints "tensor([1, 0, 0], dtype=torch.uint8)"</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">|</span> <span class="n">y</span><span class="p">)</span> <span class="c"># Logical or op., prints "tensor([1, 1, 1], dtype=torch.uint8)""</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">cond</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># Create a condition with logical `AND`</span>
<span class="k">print</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="more-logical-operations">More logical operations.</h3>
<p><code class="highlighter-rouge">torch.where(condition, input, other)</code> looks at each index <script type="math/tex">i</script> of 3 input tensors. It will return <script type="math/tex">input_i</script> if <script type="math/tex">condition_i</script> true, else will return <script type="math/tex">other_i</script>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span> <span class="c"># Only at 1st index is element &gt; 0 and &lt; 2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c"># Prints "tensor([6, 1, 4])"</span>
</code></pre></div></div>

<p>Logical operations can be combined with <code class="highlighter-rouge">tensor.nonzero()</code> to retrieve relevant
array indices. <code class="highlighter-rouge">.nonzero()</code> will return the indices of all non-zero elements of the input. It can be used much like <code class="highlighter-rouge">np.argwhere()</code>, in the following manner:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="c"># At index i=2 and i=3, x[i]=3</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">())</span> <span class="c"># Prints "tensor([[2],[3]])"</span>
</code></pre></div></div>

<h3 id="sorting-operations">Sorting Operations</h3>
<p>Indices to sort an array can be computed:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([1, 2, 3, 0])", representing indices to sort x</span>
</code></pre></div></div>

<h3 id="weights-for-convolutional-layers">Weights for Convolutional layers</h3>
<p>PyTorch convolutional layers require 4-dimensional inputs, in <em>NCHW</em> order. <em>N</em> represents the batch dimension, <em>C</em> represents the channel dimension, <em>H</em> represents the image height (number of rows), and <em>W</em> represents the image width (number of columns).</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c"># Represents 3-channel image, each image has dims (2,2)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span> <span class="c"># Convert to float32 since Conv2d cannot accept `Long` type</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[[0, 1],[2, 3]]])" as channel 0</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[[4, 5],[6, 7]]])" as channel 1</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">2</span><span class="p">,:,:])</span> <span class="c"># Prints "tensor([[[8, 9],[10, 11]]])"" as channel 2</span>

<span class="n">conv_1group</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_1group</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># 3 filters, each of size (3 x 1 x 1), prints "torch.Size([3, 3, 1, 1])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_1group</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span> <span class="c"># Prints "None", since no bias here</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c"># 0th filter, prints "tensor([1., 1., 1.])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c"># 1st filter, prints "tensor([2., 2., 2.])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c"># 2nd filter, prints "tensor([3., 3., 3.])"</span>
<span class="n">conv_1group</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="c"># Initialize the layer weight</span>

<span class="c"># Perform 1x1 convolution, i.e. the dot product</span>
<span class="c">#  of [1,1,1] w/ [1,1,1] = 3, and [1,1,1] w/ [2,2,2] = 6, etc.</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_1group</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([[[[3.,3.],[3.,3.]],[[6.,6.],[6.,6.]],[[9.,9.],[9.,9.]]]],grad_fn=&lt;MkldnnConvolutionBackward&gt;)"</span>
</code></pre></div></div>

<h3 id="groups-in-conv-layers">Groups in Conv Layers</h3>
<p>Convolutional filters can be applied along a single channel, instead of over all channels, when groups is set to the number of channels.</p>

<p>The <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d">official documentation states</a> that when <em>groups= in_channels, each input channel is convolved with its own set of filters</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv_3groups</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># 3 filters, each of size (1 x 1 x 1) now, prints "torch.Size([3, 1, 1, 1])"</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c"># Create new weight, scalar 1 is for channel 0, etc.</span>
<span class="n">conv_3groups</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="c"># Initialize CONV layer weight w/ Parameter</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="c"># Prints "Parameter containing: tensor([[[[1.]]],[[[2.]]],[[[3.]]]], requires_grad=True)"</span>

<span class="c"># Input split into 3 groups, and conv layer split into 3 groups (along channel dim.)</span>
<span class="c"># Each pixel in channel 0 multiplied with 1, each pixel in channel 1 multiplied with 2, etc.</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([[[[1.,1.],[1.,1.]],[[2.,2.],[2.,2.]],[[3.,3.],[3.,3.]]]], grad_fn=&lt;MkldnnConvolutionBackward&gt;)</span>
</code></pre></div></div>

<h3 id="bias-in-convolutional-layers-initialized-randomly">Bias in Convolutional Layers (initialized randomly)</h3>
<p>Above, we set the bias to false in our conv layers. A bias will add an offset to the dot product result – below is an example when <em>bias=5</em> for each of 3 filters.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv_3groups</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span> <span class="c"># Prints "Parameter containing:tensor([0.0452, 0.9189, 0.7354], requires_grad=True)"</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># One bias term to add for each of 3 filters, prints "torch.Size([3])"</span>
<span class="n">conv_3groups</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span> <span class="c"># Fill w/ 5's -- will add 5 to result of each 1x1 convolution</span>
<span class="n">conv_3groups</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_3groups</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([[[[6.,6.],[6.,6.]],[[7.,7.],[7.,7.]],[[8.,8.],[8.,8.]]]],grad_fn=&lt;MkldnnConvolutionBackward&gt;)"</span>
</code></pre></div></div>

<h3 id="max-pooling-layers">Max-Pooling layers</h3>
<p>A 2d max-pooling layer will slide a small window over the 2d feature map slices (each channel viewed independently) and will output the largest value in the window:</p>

<p>For example, a max-pooling layer with kernel_size=2 will slide a 2x2 window over the 2d feature maps. With stride=2, this window will be shifted over by 2 pixels along any axis before the subsequent computation. With stride=1, this window will be placed at every possible position (shifted over by 1 pixel at a time).</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c"># 1-channel, 4x4 image</span>
<span class="n">maxpool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c"># Slide `kernel` every 2 pixels</span>
<span class="k">print</span><span class="p">(</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Take max in each 2x2 quadrant of 4x4 image, prints "tensor([[[[2., 4.],[2., 4.]]]])"</span>

<span class="c"># With kernel size 4, stride 1, and padding</span>
<span class="n">maxpool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c"># Step one pixel at a time, taking max in 2x2 cell</span>
<span class="k">print</span><span class="p">(</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Get back 3x3 image since no padding, prints "torch.Size([1, 1, 3, 3])"</span>
<span class="k">print</span><span class="p">(</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Now prints "tensor([[[[2., 3., 4.],[2., 3., 4.],[2., 3., 4.]]]])"</span>

<span class="n">maxpool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c"># Pad to preserve the input size</span>
<span class="k">print</span><span class="p">(</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># Even kernel w/ padding kernel_size//2 will make size increase by 1, prints "torch.Size([1, 1, 5, 5])"</span>
</code></pre></div></div>

<h3 id="creating-a-pytorch-module-weight-initialization">Creating a Pytorch Module, Weight Initialization</h3>
<p>To define a custom layer, you’ll define a class that inherits from <code class="highlighter-rouge">torch.nn.Module</code>. The class will require a constructor, which should be implemented with <code class="highlighter-rouge">__init__()</code> in Python.</p>

<p>Consider a simple layer that applies a single convolutional filter to a 3-channel input. For <em>kernel_size=2</em>, a filter (a cube of shape 3x2x2) will be slided over the input at default <em>stride=1</em>.</p>

<p>Conv layer weights are randomly initialized by default, but can be explicitly specified in a number of ways. In order to initialize all weight values to a constant value, or to draw them from a specific type of distribution, <code class="highlighter-rouge">torch.nn.init</code> weights may be used.</p>

<p>To initialize weight values to a specific tensor, the tensor must be wrapped inside a PyTorch <code class="highlighter-rouge">Parameter</code>, meaning <em>a kind of Tensor that is to be considered a module parameter</em> (a special subclass of Tensor that will make the tensor appear in the module’s <code class="highlighter-rouge">parameters()</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MyNewModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">MyNewModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="c"># Starts with random weights</span>
		<span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c">#  prints "torch.Size([1, 3, 2, 2])"</span>
		<span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="c"># Prints tensor([[[[ 0.2,0.2],[ 0.0,0.1]],...,[[0.0,-0.1],[-0.0,0.2]]]], requires_grad=True)</span>
		<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span> <span class="c"># Fill weight with all ones</span>
		<span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="c"># Prints "Parameter containing:tensor([[[[1.,1.],[1.,1.]],...[[1.,1.],[1.,1.]]]], requires_grad=True)"</span>
		<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="c"># Insert entirely new parameter</span>
		<span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="c"># Prints "Parameter containing:tensor([[[[ 0.,1.],[ 2.,3.]],...[10.,11.]]]], requires_grad=True)"</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c"># Define the behavior for the "forward" pass</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="instantiate-models-and-iterating-over-their-modules">Instantiate Models and iterating over their modules</h3>
<p>The modules and parameters of a model can be inspected by iterating over the relevant iterators:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyNewModule</span><span class="p">()</span> 

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
	<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="c"># Prints "MyNewModule( (conv): Conv2d(3, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)), ..."</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span> <span class="c"># Iterate over parameters</span>
	<span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="c"># Prints "conv.weight Parameter containing: tensor([[[[ 0.,  1.],[ 2.,  3.]],...[10., 11.]]]], requires_grad=True)"</span>
</code></pre></div></div>

<h3 id="sequential-networks">Sequential Networks</h3>
<p>A number of different operations can be stacked into a single, sequential network with <code class="highlighter-rouge">nn.Sequential()</code>. For example, to define a network that applied a convolution and then a max-pooling operation, we could pass these layers to <code class="highlighter-rouge">nn.Sequential()</code>.</p>

<p>For a single-channel input, with 1x1 convolution with filter weights all equal to 2, the operation will double every pixel’s value. In this case, the dot product is over a 1-dimensional input, so the dot product involves only multiplication, not sum.), After subsequent max-pooling of <em>kernel_size</em> 2x2 at <em>stride=2</em>, a 1x1x2x2 tensor will be reduced to a single number, 1x1x1x1, as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">2.</span><span class="p">)</span>
<span class="n">maxpool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span><span class="n">maxpool</span><span class="p">)</span> <span class="c"># </span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c"># Prints "tensor([[[[6.]]]], grad_fn=&lt;MaxPool2DWithIndicesBackward&gt;)"</span>
</code></pre></div></div>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
