<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Particle Filter</title>
  <meta name="description" content="multi-modal distributions, sigma point transform, matrix square roots ...">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://johnwlambert.github.io/particle-filter/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>Particle Filter</h1>
    <p class="meta">Dec 27, 2018</p>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>
<ul>
  <li><a href="#sfmpipeline">A Basic SfM Pipeline</a></li>
  <li><a href="#costfunctions">Cost Functions</a></li>
  <li><a href="#bundleadjustment">Bundle Adjustment</a></li>
</ul>

<p><a name="sfmpipeline"></a></p>

<h2 id="particle-filter">Particle Filter</h2>

<p>The <strong>Particle Filter</strong> is a filtering algorithm that, unlike the Kalman Filter or EKF, can represent multi-modal distributions. This is because it contains no assumptions about the form of the state distribution. It was published in 1995 [2,3] by Simon Julier, Jeffrey Uhlmann, and Hugh Durrant-Whyte at Oxford. It is often called the <strong>“Unscented Kalman Filter” (UKF)</strong> because the inventors thought “it didn’t stink” like the EKF.</p>

<p>The main idea is to Represent a distribution <script type="math/tex">p(x)</script> with a collection of samples (particles) from <script type="math/tex">p(x)</script>: <script type="math/tex">x^i \sim p(x), i=1,\dots, N</script> i.i.d. We know that empirical moments are related to true distribution by the Law of Large Numbers. The algorithm is simple, but expensive to compute with a for loop.</p>

<h2 id="whats-wrong-with-the-ekf">What’s Wrong With the EKF?</h2>

<p>The Particle Filter addresses a number of problems with the EKF:</p>

<p><strong>Problem No. 1</strong> The initial conditions (I.C.s)!
<em>If your initial guess is wrong, the Kalman filter will tell you exactly the wrong thing. The linearization can be vastly different at different parts of the state space</em>. For example, the EKF could diverge if the residual $| \mu_{0 \mid 0} - x |$ on the initial condition is large. In fact, if $ | \mu_{t \mid t} - x_t |$ large at any time, then the EKF could diverge. This has to do with severity of non-linearity. This is the most commonly found problem.</p>

<p><strong>Problem No. 2: Covariance</strong>
<em>In the EKF, the “covariance matrix” does not literally represent the covariance.</em> Unfortunately, the covariance matrix only literally captures the covariance in the Kalman Filter. In the EKF, it is just a matrix! We don’t know what it means! If we treat it as confidence, then it is reasonable enough. And commonly true, as long as $\mu$ is tracking $x$ pretty well. However, this estimate tends to be overconfident since we are not incorporating linearization errors! Instead, <script type="math/tex">\Sigma_{t \mid t}</script> incorporates only the noise errors <script type="math/tex">Q_t, R_t</script>. Thus, <script type="math/tex">\Sigma_{t \mid t}</script>  tends to be smaller than the true covariance matrix.</p>

<p><strong>Problem No. 3: No Canary in the Goldmine</strong>
An additional problem with the EKF is that we have no signal to know if we’re going awry.</p>

<h2 id="a-different-parameterization-particles">A Different Parameterization: Particles</h2>

<p>The UKF represents a different type of compromise than the EKF. In the Kalman Filter and its Extended variant, <script type="math/tex">\mu, \Sigma</script> are the parameters that define the distribution <script type="math/tex">\mathcal{N}(\mu, \Sigma)</script>.</p>

<p>The UKF parameterizes the state distribution in a different way by using “Sigma points.” Consequently, the parameterization is called the (<script type="math/tex">\sigma</script>-points) parameterization. Thus, we move from <script type="math/tex">(\mu, \Sigma)</script> to a set of points with a weight associated with each, e.g. <script type="math/tex">\{ (x^0,w^0), \cdots, (x^{2n}, w^{2n}) \}</script>.</p>

<p>The “Unscented Transform” is a curve-fitting exercise that converts individual points to a mean and covariance, i.e. <script type="math/tex">UT(\mu, \Sigma) = \{ (x^i, w^i) \}_i</script>. An advantage of the UKF is that it is very easy to propagate these individual points through nonlinearities like non-linear dynamics, whereas it is harder to push <script type="math/tex">\mu, \Sigma</script> through the nonlinearities.</p>

<p><em>Properties that we want the unscented transform to have</em>
\item UT(<script type="math/tex">\cdot</script>):<br />
<script type="math/tex">(\mu, \Sigma) = \{ (x^i, w^i) \}_i</script></p>

<p><script type="math/tex">UT^{-1}(\cdot)</script> <br />
<script type="math/tex">\{ (x^i, w^i) \}_i = (\mu, \Sigma)</script>
We want the sample sigma points to share the same mean, covariance</p>

<script type="math/tex; mode=display">\mu = \sum\limits_{i=0}^{2n} w^ix^i</script>

<script type="math/tex; mode=display">\Sigma = \sum\limits_{i=0}^{2n} w^i (x^i - \mu)(x^i - \mu)^T</script>

<p>They are redundant (an overparameterization of the Gaussian)</p>

<p>We have redundancy for a smoothing effect, since won’t be for a perfect Gaussian</p>

<p>Here is the transform <script type="math/tex">UT(\cdot)</script>:<br />
<script type="math/tex">x^0 = \mu</script> <br />
<script type="math/tex">x^i = \mu + ( \sqrt{(n+\lambda) \Sigma } )_i, i = 1, \dots, n</script>\</p>

<p>This is a matrix square root</p>

<p>The index $i$ is the <script type="math/tex">i</script>‘th column in the matrix square root</p>

<p>We also have a mirror image set:<br />
$x^i = \mu - ( \sqrt{(n+\lambda) \Sigma } )_{i-n}, i =n+ 1, \dots, 2n$\</p>

<p>Those were the points. The weights themselves:</p>

<script type="math/tex; mode=display">w^0 = \frac{\lambda}{n+\lambda}</script>

<script type="math/tex; mode=display">w^i = \frac{1}{2(n+\lambda)}, i \geq 1</script>

<p>Each of these points plot points around the circle/ellipse
Break ellipse into major and minor axes. $x_1, x_2, x_3, x_4$ at the corners of the principal axes and the parameters.</p>

<p><script type="math/tex">n \times n</script> matrix is <script type="math/tex">\sqrt{(n+\lambda) \Sigma}</script></p>

<h2 id="verify-inverse-unscented-transform">Verify Inverse Unscented Transform**</h2>

<p>We verify <script type="math/tex">UT^{-1}(\cdot)</script> as claimed</p>

<script type="math/tex; mode=display">\sum\limits_{i=0}^{2n} w^ix^i = \frac{\lambda}{n+\lambda}(\mu) + \sum\limits_{i=1}^n  \frac{1}{2(n+\lambda)} \Bigg(\mu + ( \sqrt{(n+\lambda) \Sigma } )_i \Bigg) + \sum\limits_{i=n+1}^{2n} \frac{1}{2(n+\lambda)} \Bigg(\mu +  - ( \sqrt{(n+\lambda) \Sigma })_{i-n} \Bigg)</script>

<script type="math/tex; mode=display">= \frac{\lambda}{n+\lambda}(\mu) +  \frac{n}{2(n+\lambda)} \Bigg(\mu  \Bigg) +  \frac{n}{2(n+\lambda)} \Bigg(\mu \Bigg) - \mu</script>

<p>Everything also cancels in the $\Sigma$ calculation</p>

<script type="math/tex; mode=display">\sum\limits_{i=1}^n \frac{ (\sqrt{n_\lambda})(\sqrt{n_\lambda})}{ (n+\lambda)} (\sqrt{\Sigma}_i) (\sqrt{\Sigma})_i^T = \Sigma</script>

<p>We notice that if $AA^T = B$, then we can decompose $A$ as</p>

<script type="math/tex; mode=display">% <![CDATA[
A = \begin{bmatrix}
| & \cdots & | \\
a_1 \cdots a_n \\
| & \cdots & | 
\end{bmatrix} %]]></script>

<p>then <script type="math/tex">AA^T</script> is</p>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{bmatrix}
| & \cdots & | \\
a_1 & \cdots  & a_n \\
| & \cdots & | 
\end{bmatrix}
\begin{bmatrix}
-- & a_1^T & -- \\
-- & \cdots & -- \\
-- & a_n^T & -- | 
\end{bmatrix} %]]></script>

<p>Inverting the transform is just as simple
We get <script type="math/tex">a_1 a_1^T + a_2 a_2^T + \cdots + a_n a_n^T</script></p>

<script type="math/tex; mode=display">\sum\limits_{i=1}^n a_i a_i^T = AA^T = B</script>

<p>therefore</p>

<script type="math/tex; mode=display">\sum\limits_{i=1}^n (\sqrt{\Sigma})_i (\sqrt{\Sigma})_i^T = (\sqrt{\Sigma}) (\sqrt{\Sigma})^T = \Sigma</script>

<h2 id="matrix-square-roots">Matrix Square Roots</h2>

<p>You might prefer the Cholesky Factorization for numerical versions
Matrix Square Root! Can use SVD or Cholesky Factorization
There are SVD matrix square roots:
(i) <script type="math/tex">\sqrt{\Sigma} = U \Lambda^{1/2}</script></p>

<script type="math/tex; mode=display">\sqrt{\Sigma} (\sqrt{\Sigma})^T = U \Lambda^{1/2} (U \Lambda^{1/2})^T = \Sigma</script>

<p>Columns of $U$ matrix are principal directions of ellipse. SVD gives you this geometric intution of the points around the semi axes, etc.</p>

<p>(ii)</p>

<script type="math/tex; mode=display">\sqrt{\Sigma} = U \Lambda^{1/2} U^T = \Sigma</script>

<p>SVD computation takes $O(4n^3)$ time</p>

<p><strong>Cholesky Decomposition</strong>
<script type="math/tex">M = LU</script>, where lower triangular times upper triangular
When <script type="math/tex">M=M^T</script>, <script type="math/tex">L = U^T</script>
Let <script type="math/tex">M=LL^T</script>
(iii) <script type="math/tex">L = \sqrt{\Sigma}</script>
People prefer cholesky in UKF because it has complexity <script type="math/tex">O( \frac{1}{6} n^3)</script>, just a constant savings.</p>

<p>We choose <script type="math/tex">\sqrt{\Sigma} \sqrt{\Sigma}^T = LL^T = \Sigma</script></p>

<p>Zero out successive row elements of your vector, it is not along semi axes, but more along different axis-aligned directions as you zero  out rows</p>

<h2 id="ukf-sigma-point-filter">UKF (Sigma Point Filter)</h2>

<script type="math/tex; mode=display">(\mu, \Sigma) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow predict \rightarrow (\bar{x}^i, \bar{w}^i) \rightarrow UT^{-1}(\cdot) \rightarrow  (\bar{\mu}, \bar{\Sigma}) \rightarrow UT(\cdot) \rightarrow (x^i, w^i) \rightarrow update \rightarrow</script>

<p><strong>Predict Step</strong></p>

<script type="math/tex; mode=display">UT(\cdot)</script>

<script type="math/tex; mode=display">\begin{aligned}
x_{t \mid t}^0 = \mu_{t \mid t} \\
x_{t \mid t}^i = \mu_{t \mid t} + (\sqrt{(n+\lambda) \Sigma_{t \mid t}})_i, i = 1, \dots, n \\
x_{t \mid t}^i = \mu_{t \mid t} - (\cdots), i = n+1, \dots, 2n \\
\bar{x}_{t +1 \mid t}^i = f(x_{t \mid t}^i, u_t)
\end{aligned}</script>

<p>We predict through nonlinear dynamics</p>

<p>Now we run <script type="math/tex">UT^{-1}</script></p>

<script type="math/tex; mode=display">\begin{aligned}
\mu_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i \bar{x}_{t+1 \mid t}^i \\
\Sigma_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i (\bar{x}_{t+1 \mid t}^i - \mu_{t+1 \mid t})(\bar{x}_{t+1 \mid t}^i - \mu_{t+1 \mid t})^T
\end{aligned}</script>

<p>We recall the Gaussian estimate!</p>

<script type="math/tex; mode=display">\begin{aligned}
\mu_{t \mid t} = \mu + \Sigma_{XY} \Sigma_{YY}^{-1} (y - \hat{y}) \\
\Sigma_{t \mid t} = \Sigma - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}
\end{aligned}</script>

<p>\item Now the UPDATE step:<br />
We run <script type="math/tex">UT(\cdot)</script><br />
<script type="math/tex">x_{t +1 \mid t}^0</script><br />
<script type="math/tex">x_{t+1 \mid t}^{2n}</script>\</p>

<p>Let’s build $\hat{y}<em>{t+1 \mid t}$ and $\Sigma</em>{t+1 \mid t}^{XY}$, $\Sigma_{t+1 \mid t}^{YY}$
\item Now</p>

<script type="math/tex; mode=display">\hat{y}_{t+1 \mid t} = \sum\limits_{i=0}^{2n} w^i \hat{y}_{t+1 \mid t}^i</script>

<p>which is the expected measurment
Now,</p>

<script type="math/tex; mode=display">\begin{aligned}
\Sigma_{t+1 \mid t}^{YY} = \sum\limits_{i=0}^{2n} w^i (\hat{y}_{t+1 \mid t}^i - \hat{y}_{t+1 \mid t}) (\hat{y}_{t+1 \mid t}^i - \hat{y}_{t+1 \mid t})^T
\end{aligned}</script>

<p>Now</p>

<p>\begin{equation}
\begin{aligned}
\Sigma_{t+1 \mid t}^{XY} = \sum\limits_{i=0}^{2n} w^i (x_{t+1 \mid t}^i - \mu_{t+1 \mid t}) (\hat{y}<em>{t+1 \mid t}^i - \hat{y}</em>{t+1 \mid t})^T
\end{aligned}
\end{equation}</p>

<p>We are doing a fitting operation. That is why we have more sigma points than we need. Smooth out the anomalies due to any one point getting weird.</p>

<p>\begin{equation}
\Sigma_{t+1 \mid t+1} = \Sigma_{t+1 \mid t} - \Sigma_{t+1 \mid t}^{XY} \Sigma_{t+1 \mid t}^{YY}
\end{equation}</p>

<p>\begin{equation}
\mu_{t+1 \mid t+1 } = \mu_{t+1 \mid t } + \Sigma_{t+1 \mid t}^{XY} (\Sigma_{t+1 \mid t}^{YY})^{-1} (y_{t+1} - \hat{y}<em>{t+1 \mid t})
\end{equation}
where $y</em>{t+1}$ is the actual measurement.</p>

<h2 id="choosing-lambda">Choosing Lambda</h2>

<p>What is <script type="math/tex">\lambda</script>? Problem specific.  Consider SVD square root, so that sigma points will be along principal axes.</p>

<p>Suppose we have <script type="math/tex">x^0</script> at the center of the ellipse, and <script type="math/tex">x^1, \dots, x^4</script> lie at each corner of the principal semi-axes</p>

<p>$$\lambda$ is the “confidence-value” of the error ellipse</p>

<p>If <script type="math/tex">n+\lambda = 1</script>, then <script type="math/tex">x^i = \mu \pm \sqrt{\Sigma}_i</script> and each column represents one standard deviation</p>

<p>Standard deviation in each direction. Bigger <script type="math/tex">\lambda</script> is, then the bigger is the ellipse (And vice versa: smaller <script type="math/tex">\lambda</script> gives smaller ellipse)</p>

<p>The size of the ellipse matters because this is what we take as the region about which we create our linearization</p>

<p>UKF is a linearization, takes average slope over a neighborhood. But it is not from the Taylor Series Expansion</p>

<p>Why does size of ellipse matter? Blurring over a bigger neighborhood. (neighborhood about which we fit the Gaussian is determined by  <script type="math/tex">\lambda</script></p>

<p>The smaller <script type="math/tex">\lambda</script> is, the closer it will be to an EKF, which fits about a single-point (linearizing it there)</p>

<p>Interesting value of <script type="math/tex">\lambda</script>: <script type="math/tex">\lambda=2</script>. For a quadratic non-linearity, then the inverse unscented transform fits the mean and the covariance of the Gaussian, and also the Kurtosis (the 4th moment) of the Gaussian (but only for a quadratic nonlinearity)
\item Fitting the Kurtosis is good! We can do it beacause the extra degrees of freedom of the sigma points overparameterize
\end{itemize}
\subsection{PRO version of UKF}
\begin{itemize}
\item Other Form of UKF:</p>

<p>\begin{equation}
\lambda = \alpha^2 ( n + k) - n
\end{equation}
this gives us two parameters to tune</p>

<script type="math/tex; mode=display">x^i = \mu \pm \alpha ( \sqrt{(n+k) \Sigma})_i</script>

<p>where the two parameters are <script type="math/tex">\alpha,k</script></p>

<p>We now have to redefine the weights to be</p>

<script type="math/tex; mode=display">w_c^0 = \frac{\lambda}{n+\lambda} + (1 - \alpha^2 + \beta)</script>

<p>where $\beta$ is another parameter</p>

<p>Hugh Durrant White, the original paper has this original form</p>

<p>How does it work?</p>

<p>Algorithm is simple, but expensive to compute (with for loop)</p>

<p>In UKF, samples deterministally extracted</p>

<p>In Particle filter, no assumption about form of distribution, but need many form of them, and probabilistically extracted</p>

<p><strong>Main Idea:</strong> Represent a distribution $p(x)$ with a collection of samples from <script type="math/tex">p(x)</script>: <script type="math/tex">x^i \sim p(x), i=1,\dots, N</script> i.i.d.</p>

<p>What does it mean to ``represent’’ a distribution <script type="math/tex">p(x)</script> with a bunch of particles <script type="math/tex">\{ x_1, \dots, x_N \}</script>?</p>

<p>We know that empirical moments are related to true distribution by the law of large number</p>

<p>Recall</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{lll}
\mu = \mathbbm{E}[X] = \int_x p(x) dx \approx \frac{1}{N} \sum\limits_{i=1}^N x_i = \bar{\mu}, & x_i \sim p(x), & i=1,\dots, N, i.i.d
\end{array} %]]></script>

<p>Then</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{ll}
\Sigma = \mathbbm{E}[(X-\mu)(X-\mu)^T] = \dots \approx \frac{1}{N} \sum\limits_{i=1}^N (x_i - \bar{\mu})(x_i - \bar{\mu})^T, & x_i \sim p(x)
\end{array} %]]></script>

<p>Also,</p>

<script type="math/tex; mode=display">\mathbbm{E}[f(x)] \approx \frac{1}{N} \sum\limits_{i=1}^N f(x_i)</script>

<p>The Law of Large numbers states that</p>

<script type="math/tex; mode=display">\frac{1}{N} \sum\limits_{i=1}^N f(x_i) \rightarrow \mathbbm{E}[f(X)]</script>

<p>as <script type="math/tex">N \rightarrow \infty</script></p>

<p>Problem: Given a set of particles</p>

<script type="math/tex; mode=display">\{ x_{t+1 \mid t}^1, \dots, x_{t+1  \mid t}^N \} \sim p(x_{t+1} \mid y_{1:t} )</script>

<p>drawn from the above distribution</p>

<p>We recall the update step</p>

<script type="math/tex; mode=display">p(x_t \mid y_{1:t} ) = \frac{ p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) dx_t }</script>

<p>We have particles from the top right expression, <script type="math/tex">x_{t \mid t-1}^i \sim p( x_t \mid y_{1:t-1})</script></p>

<p>We want to use Bayes Rule so that we can transform our particles so that they approximate the posterior (this will be one step of the Bayesian Filter)</p>

<p>We will use :</p>

<p><script type="math/tex">q(x)</script> the proposal distribution, the particles are actually from here</p>

<p>We want them to be from $p(x)$ the target distribution, we wish they were from here</p>

<p>We use Particle Weights</p>

<script type="math/tex; mode=display">\mathbbm{E}_p [ f(X)] = \int_x f(x) p(x) dx = \int_x f(x) p(x) \frac{q(x)}{q(x)} dx</script>

<script type="math/tex; mode=display">\mathbbm{E}_p [ f(X)] = \int_x f(x) w(x) q(x) dx  = \mathbbm{E}_q [ f(X) w(x) ]</script>

<p>Given ${x^1, \dots, x^N }$
New set: ${ (x^1,w^1), \dots, (x^N,w^N) }$ where $w^i = w(x^i)$
Now the expectation is</p>

<script type="math/tex; mode=display">\mathbbm{E}_p[f(X)] \approx \frac{1}{N} \sum\limits_{i=1}^N f(x^i) w^i</script>

<p>and <script type="math/tex">w(x) = \frac{p(x)}{q(x)}</script></p>

<p>If we knew <script type="math/tex">p,q</script>, then we would know the weights
But in the filtering setup, we don’t know the posterior, the distribution we are trying to hit! But in fact, we don’t need the target distribution!</p>

<p>Proposal: What we have</p>

<script type="math/tex; mode=display">q(x) = p(x_t \mid y_{1:t-1})</script>

<p>Target: What we want</p>

<script type="math/tex; mode=display">p(x) = p(x_t \mid y_{1:t})</script>

<p>So we get weights:</p>

<script type="math/tex; mode=display">w^i = w(x^i) = \frac{ p(x^i) }{ q(x^i) } = \frac{p(x_t^i \mid y_{1:t})^i}{p(x_t^i \mid y_{1:t-1}) }</script>

<p>Now, by Bayes Rule, we can say that the PRIOR cancels out, which was the only thing we didn’t know???</p>

<script type="math/tex; mode=display">= \frac{     \frac{p(y_t \mid x_t) p(x_t \mid y_{1:t-1}) }{ \int_{x_t} \cdots dx_t }     }{ p(x_t \mid y_{1:t-1}) }</script>

<p>When the prior cancels, we get</p>

<p>\begin{equation}
w(x_t^i) = \frac{p(y_t \mid x_t^i ) }{ \int_{x_t} \cdots dx_t } 
\end{equation}</p>

<p>We just care about the relative weights, so</p>

<script type="math/tex; mode=display">\bar{w} (x_t^i) = p(y_t \mid x_t^i)</script>

<p>(unnormalized)</p>

<script type="math/tex; mode=display">w(x_t^i) = \frac{ \bar{w}(x_t^i) }{  \sum\limits_{i=1}^N \bar{w}(x_t^i) }</script>

<p>Example: suppose we have measurement noise $v_t \sim \mathcal{N}(0, R_t)$</p>

<script type="math/tex; mode=display">y_t = g(x_t) + v_t</script>

<p>Given $x_{t \mid t-1}^i \sim p(x_t \mid y_{1:t-1})$, then find</p>

<script type="math/tex; mode=display">\{ (x_{t \mid t}^i, w_{t \mid t}^i \}_i</script>

<p>to represent $p(x_t \mid y_{1:t})$. The weights are</p>

<script type="math/tex; mode=display">\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_{t \mid t-1}^i) \sim \mathcal{N}( g(x_{t \mid t-1}^i, R_t)</script>

<p>And now</p>

<script type="math/tex; mode=display">\bar{w}_{t \mid t}^i = \eta \mbox{exp } \{ -\frac{1}{2} (y_t - g(x_{t \mid t-1}^i)^T R_t^{-1} (y_t - g(x_{t \mid t-1}^i) ) \}</script>

<p>Renormalize</p>

<script type="math/tex; mode=display">w(x_t^i) = \frac{ \bar{w}(x_t^i) }{ \sum\limits_{i=1}^N \bar{w}(x_t^i)  }</script>

<p>Then multiply the weights, and renormalize (could have started with weighted particles instead of just particles)</p>

<p>Only the weight changes in the UPDATE step (but we keep two different time indices for weights). Time indices are redundant for the <script type="math/tex">x</script> particles, which are the input</p>

<script type="math/tex; mode=display">\{ (x_t^i, w_{t \mid t-1}^i) \}</script>

<script type="math/tex; mode=display">\bar{w}_{t \mid t}^i = p(y_t \mid x_t = x_t^i) w_{t \mid t-1}^i</script>

<p>and</p>

<script type="math/tex; mode=display">w_{t \mid t}^i  = \frac{ \bar{w}_{t \mid t}^i }{ \sum\limits_{i=1}^N \bar{w}_{t \mid t}^i  }</script>

<h2 id="predict-step">Predict Step</h2>

<script type="math/tex; mode=display">p(x_{t+1} \mid y_{1:t}) = \int_x p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t</script>

<p>the left side is the transition distribution (which we have)</p>

<p>the right side is ${ (x_t^i, w_{t \mid t}^i) }$</p>

<p>Let’s just “simulate” <script type="math/tex">p(x_{t+1} \mid x_t)</script></p>

<p>Draw <script type="math/tex">x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )</script></p>

<p>where <script type="math/tex">x_{t+1} = f(x_t, u_t) + w_t</script>, the process noise <script type="math/tex">w_t \sim \mathcal{N}(0, Q_t)</script></p>

<p>We sample 
<script type="math/tex">% <![CDATA[
\begin{array}{ll}
w_t \sim \mathcal{N}(0, Q_t), & \rightarrow x_{t+1}^i = f(x_{t}^i, u_t) + w_t 
\end{array} %]]></script></p>

<p>1000 particles per dimension <script type="math/tex">(1000^{D}</script> where <script type="math/tex">D</script>=dimension</p>

<p>Exponential explosion of particles required to keep the same resolution</p>

<p>Full Filter: Given</p>

<script type="math/tex; mode=display">\{ (x_{t }^i, w_{t \mid t}^i \}_i</script>

<p>Predict:</p>

<script type="math/tex; mode=display">x_{t+1}^i \sim p(x_{t+1} \mid x_t = x_t^i )</script>

<p>Update:</p>

<script type="math/tex; mode=display">\bar{w}_{t+1 \mid t+1}^i = p(y_{t+1} \mid x_{t+1} = x_{t+1}^i) w_{t \mid t}^i</script>

<script type="math/tex; mode=display">w_{t+1 \mid t+1 }^i  = \frac{ \bar{w}_{t+1 \mid t+1}^i }{ \sum\limits_{i=1}^N \bar{w}_{t+1 \mid t+1}^i  }</script>

<h2 id="sample-degeneracy">Sample Degeneracy</h2>

<p>Unfortunately, it is easy to end up with degenerate samples in the UKF, meaning the samples won’t do anything for you because they are all spread apart. TODO: Only one takes the weight, why?</p>

<p>The <strong>solution</strong>: resample points in a very clever way, via importance (re-)sampling. In a <em>survival of the fittest</em>, those samples with high weight end up lots of children, and those with low weight disappear. I</p>

<p>\item We sample a new set of particles at time $t$, get new $x_t^i$ s.t.
\item This is sequential important re-sampling.</p>

<script type="math/tex; mode=display">Pr( x_t^i = x_t^j) = w_t^j</script>

<p>The noisy prediction will disperse them. The new weights are uniform, i.e. <script type="math/tex">w_t^i = \frac{1}{N}</script>
\item Particle impoverishment; all particles clumped at one point because not enough noise to disperse them
\item If the resampling happens too frequently, in comparison with the dispersal via noise, then we get sample impoverishment!</p>

<p>We will get that all $x_t^i \approx x_t^j$, and $w_t^i \approx \frac{1}{N}, \forall i$</p>

<p>So no diversity in the distribution anymore</p>

<p>To fix it, resample less often</p>

<p>Or you could just throw some random particles into your bag</p>

<p>Check how different weights are at every time. If too different, trigger a resample (variance without subtracting mean is <script type="math/tex">\sum\limits_{i=1}^N (w_t^i)^2 \geq \mbox{thresh}</script></p>

<p>If maximally different, then you would get <script type="math/tex">1</script> and <script type="math/tex">0</script>, maximally different</p>

<p><strong>Low Variance Resampling:</strong> in previous formulation, you could have gotten all copies of the same particle
One number line is <script type="math/tex">r \sim v[0, \frac{1}{N}]</script>
Then map each of these to the right interval above in the weighted bins. Determinsitic resampling
Make <script type="math/tex">n</script> copies of it in every uniform bin</p>

<p><strong>Swept under Rug</strong>
All filters have tried to compute the Bayesian posterior (either exactly or approximately)
The particle filter does not do this (look at prediction step)
PF tracks approximates the posterior of the trajectory instead</p>

<script type="math/tex; mode=display">p(x_{0:t} \mid y_{1:t})</script>

<p>Because of the prediction step, where we say draw sample</p>

<script type="math/tex; mode=display">x_{t+1}^i \sim p(x_{t+1} \mid x_t  = x_t^i )</script>

<p>What we really wanted was</p>

<script type="math/tex; mode=display">x_{t+1}^i \sim p(x_{t+1} \mid y_{1:t} ) = \int_{x_t} p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) dx_t</script>

<p>\item Instead, the Particle Filter finds</p>

<script type="math/tex; mode=display">x_{t+1}^i \sim p(x_{t+1}, x_t \mid y_{1:t}) = p(x_{t+1} \mid x_t = x_t^i) p(x_t = x_t^i \mid y_{1:t})</script>

<p>Track distribution of the whole TRAJECTORY, given all of the measurements.
Important for SLAM, because in SLAM we often want to estimate the history of the trajectory of the robot
\end{itemize}</p>

<h2 id="references">References</h2>

<p>[1] Mac Schwager. Lecture Presentations of AA 273: State Estimation and Filtering for Aerospace Systems, taught at Stanford University in April-June 2018.</p>

<p>[2] SJ Julier, JK Uhlmann, HF Durrant-Whyte. <em>A new approach for filtering nonlinear systems</em>. Proceedings of the American Control Conference, June 1995, Volume 3, pages 1628-1632.</p>

<p>[3] Simon Julier, Jeffrey Uhlmann, and Hugh F. Durrant-Whyte. <em>A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators</em>. IEEE Transactions on Automatic Control, Volume 45, No. 3. March 2000, page 477.</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
