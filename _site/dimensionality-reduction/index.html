<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Dimensionality Reduction</title>
  <meta name="description" content="PCA, geodesic distances, ISOMAP, LLE, SNE, t-SNE">
   <link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://johnwlambert.github.io/dimensionality-reduction/">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.css"/>


  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">John Lambert</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          
          <a class="page-link" href="/collaborators/">Collaborators</a>
          
          
        
          
          
          
        
          
          
          <a class="page-link" href="/publications/">Publications</a>
          
          
        
          
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
          
        
          
          
          
        
          
          
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1>Dimensionality Reduction</h1>
    <p class="meta">Dec 27, 2018</p>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>
<ul>
  <li><a href="#need-for-dr">Need for Dimensionality Reduction</a></li>
  <li><a href="#pca">PCA</a></li>
  <li><a href="#nonlinear-dr-methods">Nonlinear Dimensionality Reduction Methods</a></li>
  <li><a href="#isomap">ISOMAP</a></li>
  <li><a href="#lle">LLE</a></li>
  <li><a href="#sne">SNE</a></li>
  <li><a href="#t-sne">t-SNE</a></li>
</ul>

<p><a name="need-for-dr"></a></p>

<h2 id="the-need-for-dimensionality-reduction">The Need for Dimensionality Reduction</h2>

<p>When the dimensionality of data points is high, interpretation or visualization of the data can be quite challenging.</p>

<p><a name="pca"></a></p>

<h3 id="linear-methods">Linear Methods</h3>

<h2 id="principal-components-analysis-pca">Principal Components Analysis (PCA)</h2>

<p>PCA is an algorithm for linear dimensionality reduction that can be surprisingly confusing. There are many different ways to explain the algorithm, and it is not new – Karl Pearson (University College, London) introduced the ideas in 1901 and Harold Hostelling (Columbia) developed the terminology and more of the math about principal components in 1933.</p>

<p>To understand PCA, we must first consider data as points in a Euclidean space.  Suppose the lies in a high-dimensional subspace, but the real space of the data could be low-dimensional. We would like to identify the real, lower-dim subspace where lower-dim structure corresponds to linear subspaces in the high-dim space.</p>

<p><strong>PCA has one key hyperparameter: the dimensionality of the lower-dim space.</strong> How many dimensions do we need to explain most of the variability in the data?</p>

<p>The key idea is that we will project the points from high-D to low-D, and then when we project them back into high-D, the “reconstruction error” should be minimized.</p>

<p>\item Pearson (1901) – find a single lower dimensional subspace that captures most of the variation in the data
\item What should be the dimensionality of the lower-dim space?</p>

<p>Acts as a proxy for the real space</p>

<p>Minimize errors introudced by projecting the data into this subspace</p>

<p>Assumption: data has zero mean, ``centered data’’</p>

<p>Move the origin to the middle of the data, the data will live in the hyperplane. Becomes subspace in new translated coordinate system</p>

<p>As a consequence, variance becomes just the 2nd moment</p>

<p>Data points in 2D -&gt; project onto the best fit line (projection onto 1D), then assess how well the data looks now when you lay out the projected points onto the best fit line (this is the reconstruction)
Bring it back onto the best fit line (multiply by vector)</p>

<script type="math/tex; mode=display">x_{||} = vv^Tx</script>

<p>Minimize an error function: error equivalent to maximizing the variance of the projected data</p>

<script type="math/tex; mode=display">% <![CDATA[
E = \frac{1}{M} \sum\limits_I \sum\limits_M ... = < \|x^{\prime \mu} - x_{||}^{\prime \mu}\|^2 >_{\mu} %]]></script>

<p>$I$ dimensions
WHY IS THIS?
Trying to find the directions of maximum variance
Knowing the covariance matrix tells us about the 2nd order moments, but not about the highest-order structure of the data (unless data is normal)</p>

<p>If the covariance matrix is diagonal, then finding the direction of maximum variance is easy</p>

<p>Rotate your coordinate system so that the covariance matrix becomes diagonal.</p>

<p>This becomes an eigenvalue problem: the eigenvectors of the covariance matrix point in the directions of maximal variance?</p>

<p>Solve all top-k possible subspaces</p>

<p>Valid for all of them</p>

<p>Check the error depending on many dimensions we used. A tradeoff
$\mathbf{V}^T \mathbf{V} = I$ because $\mathbf{V}$ is an orthonormal matrix
Projection $x_{||} = \mathbf{V}y = \mathbf{V}\mathbf{V}^Tx$</p>

<p>Projection operator = $\mathbf{V}\mathbf{V}^T$</p>

<p>But in general $P$ is of low rank and loses info</p>

<p>Variance and reconstruction error</p>

<script type="math/tex; mode=display">% <![CDATA[
<x^Tx> - <y_{||}^T y_{||}> %]]></script>

<p>Want to be able to maximize the variance in the projected subspace… (FIND OUT WHY THAT IS)</p>

<p>Spectral analysis of the covariance matrix:
if covariance matrix is symmetric, it always has real eigenvalues and orthogonal eigenvectors</p>

<p>The total variance fo the data is just the sum of the eigenvalues of the covariance matrix, all non-negative</p>

<p>Have diagonalized the covariance matrix, aligned, which was what we set out to do</p>

<p>This is an extremal trace problem?</p>

<script type="math/tex; mode=display">\sum\limits_i \lambda_i \sum\limits_p (v_{ip}^{\prime})^2</script>

<script type="math/tex; mode=display">= \mbox{tr }(V^{\prime T} \Lambda V^{\prime})</script>

<p>To maximize the variance, need to put as mucch weight as possible on the large eigenvalues</p>

<p>The reconstruction error is</p>

<script type="math/tex; mode=display">\sum\limits_{i=P+1}^I \lambda_i</script>

<p>Aligns the axis with your data, so that for any $P$, can find $P$-dimensional subspace</p>

<h3 id="eigenvectors-solve-pca">Eigenvectors Solve PCA</h3>

<p><strong>Main Lesson from PCA</strong>: For any P, the optimal $P$-dimensional subspace is the one spanned by the first $P$ eigenvectors of the covariance matrix</p>

<p>The eigenvectors give us the frame that we need – align with it</p>

<h3 id="pca-examples">PCA Examples</h3>

<p>First principal component does not always have semantic meaning (combination of arts, recreation, transportation, health, and housing explain the ratings of places the most)</p>

<p>This is simply what the analysis gives.</p>

<p>The math is beautiful, but semantic meaning is not always clear. Sparse PCA tries to fix this (linear combinations of only a small number of variables)</p>

<p>Geography of where an individual came from is reflected in their genetic material</p>

<p>Machinery gives us a way to understand distortions of changes: a recipe encoded as a matrix</p>

<p>PCA works very well here</p>

<p>View matrices as points in high-dimensional space</p>

<p>Recover grid structure for deformed sphere</p>

<p>Get circular pattern of galloping horse from points</p>

<h3 id="svd-trick">SVD Trick</h3>

<p>What if we have fewer data points than dimensions <script type="math/tex">% <![CDATA[
M<I %]]></script>? Think of images…</p>

<p>By SVD, transpose your data</p>

<p>Think of your data as rows, not columns</p>

<p>Data becomes first pixel across 1000 images</p>

<p>Covariance matrix <script type="math/tex">C_2</script> of transposed problem</p>

<p>Rows of transformed X (PCA’d X) are just eigenvectors of <script type="math/tex">C_1</script></p>

<p>Corresponding eigenvalues are just those of <script type="math/tex">C_2</script>, scaled by <script type="math/tex">I/M</script></p>

<p>ntroduced by Pearson (1901)
and Hotelling (1933) to
describe the variation in a set
of multivariate data in terms of
a set of uncorrelated variables.
PCA looks for a single lower
dimensional subspace that
captures most of the variation
in the data.
Specifically, we aim to
minimize the error introduced
by projecting the data into this
linear subspace.</p>

<p>Use spectral analysis of the covariance matrix C of the
data
For any integer p, the error-minimizing p-dimensional
subspace is the one spanned by the first p eigenvectors
of the covariance matrix</p>

<p>Eigenfaces (PCA on face images)</p>

<p>M. Turk and A. Pentland, Face Recognition using Eigenfaces, CVPR 1991</p>

<p>https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf</p>

<p>http://theory.stanford.edu/~tim/s17/l/l8.pdf</p>

<h2 id="multi-dimensional-scaling-mds">Multi-Dimensional Scaling (MDS)</h2>

<p><a name="nonlinear-dr-methods"></a></p>

<h2 id="non-linear-methods">Non-Linear Methods</h2>

<p>Many data sets contain essential nonlinear structures and unfortunately these structures are invisible to linear methods like PCA [1].</p>

<p>For example, Euclidean distance between points cannot disentangle or capture the structure of manifolds like the “swiss roll.” Instead, we need geodesic distance: distance directly on the manifold.</p>

<p>Furthermore, PCA cares about large distances. The goal is to maximize variance, and variance comes from things that are far apart. If you care about small distances, PCA is the wrong tool to use.</p>

<h2 id="creating-graphs-from-geometric-point-data">Creating Graphs from Geometric Point Data</h2>

<p>Rely upon neighborhood relations. A graph can be constructed via (1) k-nearest neighbors or (2) <script type="math/tex">\epsilon</script>-balls.</p>

<p><a name="isomap"></a></p>

<h3 id="isometric-feature-mapping-isomap">Isometric Feature Mapping (ISOMAP)</h3>

<p>In the case of the swiss roll, Isometric Feature Mapping (ISOMAP) produces an unrolled, planar version of data that respects distances [5]. This is “isometric” unrolling.</p>

<p>The ISOMAP algorithm involves three main steps, as outlined by Guibas [1]:</p>

<ul>
  <li>(1.) Form a nearest-neighbor graph <script type="math/tex">\mathcal{G}</script> on the original data points, weighing the edges by their original distances <script type="math/tex">d_x(i,j)</script>. One can build the NN-graph either (A) with a fixed radius/threshold <script type="math/tex">\epsilon</script>, or by using a (B) fixed # of neighbors.</li>
  <li>(2.) Estimate the geodesic distances <script type="math/tex">d_{\mathcal{G}}(i,j)</script> between all pairs of points on the sampled manifold by computing their shortest path distances in the graph <script type="math/tex">\mathcal{G}</script>. This can be done with classic graph algorithms for all-pairs shortest-path algorithm (APSP)s: Floyd/Warshall’s algorithm or Dijkstra’s algorithm. Initially, all pairs given distance <script type="math/tex">\infty</script>, except for neighbors, which are connected.</li>
  <li>(3.) Construct an embedding of the data in <script type="math/tex">d</script>-dimensional Euclidean space that best preserves the inter-point distances <script type="math/tex">d_{\mathcal{G}}(i,j)</script>. This is performed via Multi-Dimensional Scaling (MDS).</li>
</ul>

<p>ISOMAP actually comes with recovery guarantees that discovered structure equal to actual structure of the manifold, especially as the graph point density increases. MDS and ISOMAP both converge, but ISOMAP gets there more quickly.</p>

<p><a name="lle"></a></p>

<h3 id="locally-linear-embedding-lle">Locally Linear Embedding (LLE)</h3>

<p>LLE is a method that learns linear weights that locally reconstruct points in order to map points to a lower dimension [1,4]. The method requires solving two successive optimization problems and requires a connectivity graph. Almost all methods start with the nearest neihgbor graph, since it’s the only thing that we can trust.</p>

<p>In the <strong>first step of LLE</strong>, we find weights that reconstruct each data point from its neighbors:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{ll}
\underset{w}{\mbox{minimize }} & \| x_i - \sum\limits_{j \in N(i)} w_{ij}x_j \|^2 \\
\mbox{subject to} & \sum\limits_j w_{ij} = 1
\end{array} %]]></script>

<p>We can use linear least squares with Lagrange multipliers to obtain optimal linear combinations.</p>

<p>In the <strong>second step of LLE</strong>, We then <strong>fix these weights</strong> <script type="math/tex">w_{ij}</script> while optimizing for <script type="math/tex">x_i^{\prime}</script>. We try to find low-dimensional coordinates:</p>

<script type="math/tex; mode=display">\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2
\end{array}</script>

<p>This is a sparse eigenvalue problem that requires constraints in order to prevent degenerate solutions:</p>
<ul>
  <li>(1.) The coordinates <script type="math/tex">x_i^{\prime}</script> can be translated by a constant
displacement without affecting the cost. We can remove this degree of freedom by requiring the coordinates to be centered on the origin.</li>
  <li>(2.) We constrain the embedding vectors to have unit covariance.
The optimization problem becomes:</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{ll}
\underset{x_1^{\prime}, \dots, x_n^{\prime}}{\mbox{minimize }} & \sum\limits_i \| x_i^{\prime} - \sum\limits_{j \in N(i)} w_{ij}x_j^{\prime} \|^2 \\
\mbox{subject to} & \sum\limits_i x_i^{\prime} = 0 \\
& \frac{1}{n} \sum\limits_i x_i^{\prime}x_i^{\prime T} = I
\end{array} %]]></script>

<p>These weights <script type="math/tex">w_{ij}</script> capture the local shape. As Roweis and Saul point out, “<em>LLE illustrates a general principle of manifold learning…that overlapping
local neighborhoods – collectively analyzed – can provide information about global
geometry</em>”” [4].</p>

<p><a name="sne"></a></p>

<h3 id="stochastic-neighbor-embedding-sne">Stochastic Neighbor Embedding (SNE)</h3>

<p>The Stochastic Neighbor Embedding (SNE) converts high-dimensional points to low-dimensional points by preserving distances. The method takes a probabilistic point of view: high-dimensional Euclidean point distances are converted into conditional probabilities that represent similarities [2,3].</p>

<p>Similarity of datapoints in <strong>high himension</strong>: The conditional probability is given by</p>

<script type="math/tex; mode=display">p_{j \mid i} = \frac{\mbox{exp }\big( - \frac{\|x_i - x_j\|^2}{2 \sigma_i^2}\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \frac{\|x_i - x_k\|^2}{2 \sigma_i^2} \big)}</script>

<p>Similarity of datapoints in <strong>the low dimension</strong>:</p>

<script type="math/tex; mode=display">q_{j \mid i} = \frac{\mbox{exp }\big( - \|y_i - y_j\|^2\big) }{ \sum\limits_{k \neq i} \mbox{exp }\big( - \|y_i - y_k\|^2 \big)}</script>

<p>If similarities between <script type="math/tex">x_i,x_j</script> are correctly mapped to similarities between <script type="math/tex">y_i,y_j</script> by SNE, then the conditional probabilities should be equal: <script type="math/tex">q_{j \mid i} = p_{j \mid i}</script>.</p>

<p>SNE seeks minimize the following cost function using gradient descent, which measures the dissimilarity between the two distributions (Kullback-Leibler divergence):</p>

<script type="math/tex; mode=display">C = \sum\limits_i KL(P_i || Q_i) = \sum\limits_i \sum\limits_j p_{j \mid i} \mbox{ log } \frac{p_{j \mid i}}{q_{j \mid i}}</script>

<p>This is known as asymetric SNE. The gradient turns out to be analytically simple:</p>

<script type="math/tex; mode=display">\frac{\partial C}{\partial y_i} = 2 \sum\limits_j (p_{j \mid i} - q_{j \mid i} - p_{i \mid j} - q_{i \mid j} )(y_i - y_j)</script>

<p>However, the Kullback-Leibler divergence is not symmetric, so a formulation with a joint distribution can be made.</p>

<p><a name="t-sne"></a></p>

<h3 id="t-sne">t-SNE</h3>

<p>An improvement to SNE is t-Distributed Stochastic Neighbor Embedding (t-SNE). t-SNE employs a Gaussian in the high-dimension, but a t-Student distribution in low-dim. The t-Student distribution has longer tails than a Gaussian and is thus happier to have points far away than a Gaussian. The motivation for doing so is that in low-D, you have have less freedom than you would in the high-dimension to put many things closeby. This is because there is not much space around (crowded easily), so we penalize having points far away less.</p>

<p>The joint distribution in the low-distribution is:</p>

<script type="math/tex; mode=display">q_{ij} = \frac{ (1+ \| y_i −y_j \|^2)^{−1} }{ \sum\limits_{k \neq l} (1+ \|y_k −y_l\|^2)^{−1} }</script>

<h3 id="mnist-examples">MNIST examples</h3>

<h2 id="references">References</h2>

<p>[1] Leonidas Guibas. <em>Multi-Dimensional Scaling, Non-Linear Dimensionality Reduction</em>. Class lectures of CS233: Geometric and Topological Data Analysis, taught at Stanford University in 18 April 2018.</p>

<p>[2] Geoffrey Hinton and Sam Roweis. <em>Stochastic Neighbor Embedding</em>. Advances in Neural Information Processing Systems (NIPS) 2003, pages 857–864. <a href="http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf">http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf</a>.</p>

<p>[3] L.J.P. van der Maaten and G.E. Hinton. <em>Visualizing High-Dimensional Data Using t-SNE</em>. Journal of Machine Learning Research 9 (Nov):2579-2605, 2008.</p>

<p>[4] Sam T. Roweis and Lawrence K. Saul. <em>Nonlinear Dimensionality Reduction by Locally Linear Embedding</em>. Science Magazine, Vol. 290,  22 Dec. 2000.</p>

<p>[5] J. B. Tenenbaum, V. de Silva and J. C. Langford. <em>A Global Geometric Framework for Nonlinear Dimensionality Reduction</em>. Science 290 (5500): 2319-2323, 22 December 2000. <a href="http://web.mit.edu/cocosci/Papers/sci_reprint.pdf">PDF</a>.</p>

<p>[6] Laurenz Wiskott. <em>Principal Component Analysis</em>. 11 March 2004. <a href="https://pdfs.semanticscholar.org/d657/68e1dad46bbdb5cfb17eb19eb07cc0f5947c.pdf">Online PDF</a>.</p>

<p>[7] Karl Pearson. <em>On Lines and Planes of Closest Fit to Systems of Points in Space</em>. 1901. Philosophical Magazine. 2 (11): 559–572.</p>

<p>[8] H Hotelling. <em>Analysis of a complex of statistical variables into principal components</em>. 1933. Journal of Educational Psychology, 24, 417–441, and 498–520.
Hotelling, H (1936). “Relations between two sets of variates”. Biometrika. 28 (3/4): 321–377. doi:10.2307/2333955. JSTOR 2333955.</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>
  
  
  <!-- disqus comments -->
<!--   -->
  
</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">John Lambert</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              John Lambert
            
          </li>
          
          <li><a href="mailto:johnlambert [at] gatech.edu">johnlambert [at] gatech.edu</a></li>
          
          
       </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
         
          <li>
              <a href="https://scholar.google.com/citations?user=6GhZedEAAAAJ">
                <i class="ai ai-google-scholar ai"></i> Google Scholar
              </a>
          </li>
          
          
          
          <li>
              <a href="https://linkedin.com/in/johnwlambert">
                <i class="fa fa-linkedin fa"></i> LinkedIn
              </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
         <ul class="social-media-list">
          <li>
        <a>Ph.D. Candidate in Computer Vision.
</a>
         </li>
          <li>
        Website Design by <a href="http://www.niebles.net/">Juan Carlos Niebles, Ph.D.</a>
        </li>
         </ul>
      </div>
    </div>

  </div>

</footer>

    

  </body>

</html>
